{
  "data": [
    {
      "created": 1732875917,
      "id": "Sao10K/L3-8B-Stheno-v3.2",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 500,
      "output_token_price_per_m": 500,
      "title": "Sao10K/L3-8B-Stheno-v3.2",
      "description": "Sao10K/L3-8B-Stheno-v3.2 is a highly skilled actor that excels at fully immersing itself in any role assigned.",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "L3 8B Stheno V3.2",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": [
        "function-calling"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1755106514,
      "id": "baichuan/baichuan-m2-32b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 700,
      "output_token_price_per_m": 700,
      "title": "baichuan/baichuan-m2-32b",
      "description": "Baichuan-M2 is a medically-enhanced reasoning model specifically designed for real-world medical reasoning tasks. We begin with real-world medical questions and conduct reinforcement learning training based on a large-scale verifier system. While maintaining the model's general capabilities, the medical effectiveness of Baichuan-M2 has achieved breakthrough improvements.\n\nBaichuan-M2 is currently the world's best open-source medical model. On the HealthBench Benchmark, it surpasses all open-source models, including GPT-OSS-120B, as well as many cutting-edge closed-source models. It is the open-source model closest to GPT-5 in terms of medical capabilities.\n\nOur research demonstrates that a robust verifier is crucial for aligning model capabilities with real-world applications, and an end-to-end reinforcement learning approach fundamentally enhances the model's medical reasoning abilities. The release of Baichuan-M2 represents a significant advancement in the field of medical artificial intelligence, pushing t",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "BaiChuan M2 32B",
      "model_type": "chat",
      "max_output_tokens": 131072,
      "features": [
        "structured-outputs",
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1751258572,
      "id": "baidu/ernie-4.5-0.3b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 0,
      "output_token_price_per_m": 0,
      "title": "baidu/ernie-4.5-0.3b",
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "tags": [],
      "context_size": 120000,
      "status": 1,
      "display_name": "ERNIE 4.5 0.3B",
      "model_type": "chat",
      "max_output_tokens": 8000,
      "features": [
        "function-calling"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1751258682,
      "id": "baidu/ernie-4.5-21B-a3b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 700,
      "output_token_price_per_m": 2800,
      "title": "baidu/ernie-4.5-21B-a3b",
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "tags": [],
      "context_size": 120000,
      "status": 1,
      "display_name": "ERNIE 4.5 21B A3B",
      "model_type": "chat",
      "max_output_tokens": 8000,
      "features": [
        "function-calling"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1751243379,
      "id": "baidu/ernie-4.5-300b-a47b-paddle",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2800,
      "output_token_price_per_m": 11000,
      "title": "baidu/ernie-4.5-300b-a47b-paddle",
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "tags": [],
      "context_size": 123000,
      "status": 1,
      "display_name": "ERNIE 4.5 300B A47B",
      "model_type": "chat",
      "max_output_tokens": 12000,
      "features": [
        "function-calling",
        "structured-outputs"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1751258754,
      "id": "baidu/ernie-4.5-vl-28b-a3b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1400,
      "output_token_price_per_m": 5600,
      "title": "baidu/ernie-4.5-vl-28b-a3b",
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "tags": [],
      "context_size": 30000,
      "status": 1,
      "display_name": "ERNIE 4.5 VL 28B A3B",
      "model_type": "chat",
      "max_output_tokens": 8000,
      "features": [
        "function-calling",
        "vision",
        "reasoning"
      ]
    },
    {
      "created": 1751258620,
      "id": "baidu/ernie-4.5-vl-424b-a47b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 4200,
      "output_token_price_per_m": 12500,
      "title": "baidu/ernie-4.5-vl-424b-a47b",
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "tags": [],
      "context_size": 123000,
      "status": 1,
      "display_name": "ERNIE 4.5 VL 424B A47B",
      "model_type": "chat",
      "max_output_tokens": 16000,
      "features": [
        "function-calling",
        "vision",
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1719492271,
      "id": "cognitivecomputations/dolphin-mixtral-8x22b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 9000,
      "output_token_price_per_m": 9000,
      "title": "cognitivecomputations/dolphin-mixtral-8x22b",
      "description": "Dolphin 2.9 is designed for instruction following, conversational, and coding. This model is a finetune of Mixtral 8x22B Instruct. It features a 64k context length and was fine-tuned with a 16k sequence length using ChatML templates.The model is uncensored and is stripped of alignment and bias. It requires an external alignment layer for ethical use.",
      "tags": [],
      "context_size": 16000,
      "status": 1,
      "display_name": "Dolphin Mixtral 8x22B",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": [
        "structured-outputs"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1746010994,
      "id": "deepseek/deepseek-prover-v2-671b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 7000,
      "output_token_price_per_m": 25000,
      "title": "deepseek/deepseek-prover-v2-671b",
      "description": "DeepSeek Launches Open-Source Model DeepSeek-Prover-V2-671B, Specializing in Mathematical Theorem Proving\nThe new model employs a Mixture of Experts (MoE) architecture and is trained using the Lean 4 framework for formal reasoning. With 671 billion parameters, it leverages reinforcement learning and large-scale synthetic data to significantly enhance automated theorem-proving capabilities.",
      "tags": [],
      "context_size": 160000,
      "status": 1,
      "display_name": "Deepseek Prover V2 671B",
      "model_type": "chat",
      "max_output_tokens": 160000,
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1748457624,
      "id": "deepseek/deepseek-r1-0528",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 7000,
      "output_token_price_per_m": 25000,
      "title": "deepseek/deepseek-r1-0528",
      "description": "DeepSeek R1 0528 is the latest open-source model released by the DeepSeek team, featuring impressive reasoning capabilities, particularly achieving performance comparable to OpenAI's o1 model in mathematics, coding, and reasoning tasks.",
      "tags": [],
      "context_size": 163840,
      "status": 1,
      "display_name": "DeepSeek R1 0528",
      "model_type": "chat",
      "max_output_tokens": 163840,
      "features": [
        "function-calling",
        "structured-outputs",
        "reasoning"
      ],
      "endpoints": [
        "completion",
        "batch-api"
      ]
    },
    {
      "created": 1748526709,
      "id": "deepseek/deepseek-r1-0528-qwen3-8b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 600,
      "output_token_price_per_m": 900,
      "title": "deepseek/deepseek-r1-0528-qwen3-8b",
      "description": "DeepSeek-R1-0528-Qwen3-8B is a high-performance reasoning model based on the Qwen3 8B Base model, enhanced through the integration of DeepSeek-R1-0528's Chain-of-Thought (CoT) optimization. In the AIME 2024 evaluation, this open-source model achieved state-of-the-art (SOTA) performance, delivering a 10% improvement over the original Qwen3 8B while matching the reasoning capabilities of the much larger 235-billion-parameter Qwen3-235B-thinking. ",
      "tags": [],
      "context_size": 128000,
      "status": 1,
      "display_name": "DeepSeek R1 0528 Qwen3 8B",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": [
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1737957190,
      "id": "deepseek/deepseek-r1-distill-llama-70b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 8000,
      "output_token_price_per_m": 8000,
      "title": "deepseek/deepseek-r1-distill-llama-70b",
      "description": "DeepSeek R1 Distill LLama 70B",
      "tags": [],
      "context_size": 32000,
      "status": 1,
      "display_name": "DeepSeek R1 Distill LLama 70B",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": [
        "structured-outputs",
        "disable-character",
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1738498617,
      "id": "deepseek/deepseek-r1-distill-llama-8b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 400,
      "output_token_price_per_m": 400,
      "title": "deepseek/deepseek-r1-distill-llama-8b",
      "description": "DeepSeek R1 Distill Llama 8B is a distilled large language model based on Llama-3.1-8B-Instruct, using outputs from DeepSeek R1. ",
      "tags": [],
      "context_size": 32000,
      "status": 1,
      "display_name": "DeepSeek R1 Distill Llama 8B",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": [
        "structured-outputs",
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1738498371,
      "id": "deepseek/deepseek-r1-distill-qwen-14b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1500,
      "output_token_price_per_m": 1500,
      "title": "deepseek/deepseek-r1-distill-qwen-14b",
      "description": "DeepSeek R1 Distill Qwen 14B is a distilled large language model based on Qwen 2.5 14B, using outputs from DeepSeek R1. It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\nOther benchmark results include:\n\nAIME 2024 pass@1: 69.7\nMATH-500 pass@1: 93.9\nCodeForces Rating: 1481\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
      "tags": [],
      "context_size": 64000,
      "status": 1,
      "display_name": "DeepSeek R1 Distill Qwen 14B",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": [
        "structured-outputs",
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1738498293,
      "id": "deepseek/deepseek-r1-distill-qwen-32b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3000,
      "output_token_price_per_m": 3000,
      "title": "deepseek/deepseek-r1-distill-qwen-32b",
      "description": "DeepSeek R1 Distill Qwen 32B is a distilled large language model based on Qwen 2.5 32B, using outputs from DeepSeek R1. It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\nOther benchmark results include:\nAIME 2024 pass@1: 72.6\nMATH-500 pass@1: 94.3\nCodeForces Rating: 1691\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
      "tags": [],
      "context_size": 64000,
      "status": 1,
      "display_name": "DeepSeek R1 Distill Qwen 32B",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": [
        "structured-outputs",
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1741174465,
      "id": "deepseek/deepseek-r1-turbo",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 7000,
      "output_token_price_per_m": 25000,
      "title": "deepseek/deepseek-r1-turbo",
      "description": "DeepSeek R1 is the latest open-source model released by the DeepSeek team, featuring impressive reasoning capabilities, particularly achieving performance comparable to OpenAI's o1 model in mathematics, coding, and reasoning tasks.",
      "tags": [],
      "context_size": 64000,
      "status": 1,
      "display_name": "DeepSeek R1 (Turbo)\t",
      "model_type": "chat",
      "max_output_tokens": 16000,
      "features": [
        "function-calling",
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1742909352,
      "id": "deepseek/deepseek-v3-0324",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2800,
      "output_token_price_per_m": 11400,
      "title": "deepseek/deepseek-v3-0324",
      "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.",
      "tags": [],
      "context_size": 163840,
      "status": 1,
      "display_name": "DeepSeek V3 0324",
      "model_type": "chat",
      "max_output_tokens": 163840,
      "features": [
        "function-calling",
        "structured-outputs"
      ],
      "endpoints": [
        "completion",
        "anthropic"
      ]
    },
    {
      "created": 1741174512,
      "id": "deepseek/deepseek-v3-turbo",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 4000,
      "output_token_price_per_m": 13000,
      "title": "deepseek/deepseek-v3-turbo",
      "description": "DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.",
      "tags": [],
      "context_size": 64000,
      "status": 1,
      "display_name": "DeepSeek V3 (Turbo)\t",
      "model_type": "chat",
      "max_output_tokens": 16000,
      "features": [
        "function-calling"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1755759094,
      "id": "deepseek/deepseek-v3.1",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2700,
      "output_token_price_per_m": 10000,
      "title": "deepseek/deepseek-v3.1",
      "description": "DeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode.DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.",
      "tags": [],
      "context_size": 163840,
      "status": 1,
      "display_name": "DeepSeek V3.1",
      "model_type": "chat",
      "max_output_tokens": 163840,
      "features": [
        "structured-outputs",
        "function-calling",
        "reasoning"
      ],
      "endpoints": [
        "completion",
        "anthropic"
      ]
    },
    {
      "created": 1756965597,
      "id": "google/gemma-3-12b-it",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 500,
      "output_token_price_per_m": 1000,
      "title": "google/gemma-3-12b-it",
      "description": "",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Gemma3 12B",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": [
        "structured-outputs",
        "vision"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1750848096,
      "id": "google/gemma-3-1b-it",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 0,
      "output_token_price_per_m": 0,
      "title": "google/gemma-3-1b-it",
      "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
      "tags": [],
      "context_size": 32768,
      "status": 1,
      "display_name": "Gemma3 1B IT",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "endpoints": [
        "batch-api"
      ]
    },
    {
      "created": 1742889385,
      "id": "google/gemma-3-27b-it",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1190,
      "output_token_price_per_m": 2000,
      "title": "google/gemma-3-27b-it",
      "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 32k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs. Gemma 3 27B is Google's latest open source model, successor to Gemma.",
      "tags": [],
      "context_size": 32768,
      "status": 1,
      "display_name": "Gemma 3 27B",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": [
        "structured-outputs",
        "vision"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1714024873,
      "id": "gryphe/mythomax-l2-13b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 900,
      "output_token_price_per_m": 900,
      "title": "gryphe/mythomax-l2-13b",
      "description": "The idea behind this merge is that each layer is composed of several tensors, which are in turn responsible for specific functions. Using MythoLogic-L2's robust understanding as its input and Huginn's extensive writing capability as its output seems to have resulted in a model that exceeds at both, confirming my theory. (More details to be released at a later time).",
      "tags": [],
      "context_size": 4096,
      "status": 1,
      "display_name": "Mythomax L2 13B",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1714024815,
      "id": "meta-llama/llama-3-70b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 5100,
      "output_token_price_per_m": 7400,
      "title": "meta-llama/llama-3-70b-instruct",
      "description": "Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 70B instruct-tuned version was optimized for high quality dialogue usecases. It has demonstrated strong performance compared to leading closed-source models in human evaluations.",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "Llama3 70B Instruct",
      "model_type": "chat",
      "max_output_tokens": 8000,
      "features": [
        "structured-outputs"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1714024874,
      "id": "meta-llama/llama-3-8b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 400,
      "output_token_price_per_m": 400,
      "title": "meta-llama/llama-3-8b-instruct",
      "description": "Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 8B instruct-tuned version was optimized for high quality dialogue usecases. It has demonstrated strong performance compared to leading closed-source models in human evaluations.",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "Llama 3 8B Instruct",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1721801867,
      "id": "meta-llama/llama-3.1-8b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 200,
      "output_token_price_per_m": 500,
      "title": "meta-llama/llama-3.1-8b-instruct",
      "description": "Meta's latest class of models, Llama 3.1, launched with a variety of sizes and configurations. The 8B instruct-tuned version is particularly fast and efficient. It has demonstrated strong performance in human evaluations, outperforming several leading closed-source models.",
      "tags": [],
      "context_size": 16384,
      "status": 1,
      "display_name": "Llama 3.1 8B Instruct",
      "model_type": "chat",
      "max_output_tokens": 16384,
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1732609540,
      "id": "meta-llama/llama-3.2-1b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 0,
      "output_token_price_per_m": 0,
      "title": "meta-llama/llama-3.2-1b-instruct",
      "description": "The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).",
      "tags": [],
      "context_size": 131000,
      "status": 1,
      "display_name": "Llama 3.2 1B Instruct\t",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "endpoints": [
        "completion",
        "batch-api"
      ]
    },
    {
      "created": 1732607748,
      "id": "meta-llama/llama-3.2-3b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 300,
      "output_token_price_per_m": 500,
      "title": "meta-llama/llama-3.2-3b-instruct",
      "description": "The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out)",
      "tags": [],
      "context_size": 32768,
      "status": 1,
      "display_name": "Llama 3.2 3B Instruct",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": [
        "function-calling"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1733560109,
      "id": "meta-llama/llama-3.3-70b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1300,
      "output_token_price_per_m": 3900,
      "title": "meta-llama/llama-3.3-70b-instruct",
      "description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Llama 3.3 70B Instruct",
      "model_type": "chat",
      "max_output_tokens": 120000,
      "features": [
        "function-calling"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1743906990,
      "id": "meta-llama/llama-4-maverick-17b-128e-instruct-fp8",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1700,
      "output_token_price_per_m": 8500,
      "title": "meta-llama/llama-4-maverick-17b-128e-instruct-fp8",
      "description": "Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction.\n\nMaverick features early fusion for native multimodality and a 1 million token context window. It was trained on a curated mixture of public, licensed, and Meta-platform data, covering ~22 trillion tokens, with a knowledge cutoff in August 2024. Released on April 5, 2025 under the Llama 4 Community License, Maverick is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.",
      "tags": [],
      "context_size": 1048576,
      "status": 1,
      "display_name": "Llama 4 Maverick Instruct",
      "model_type": "chat",
      "max_output_tokens": 1048576,
      "features": [
        "function-calling",
        "vision"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1743906925,
      "id": "meta-llama/llama-4-scout-17b-16e-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1000,
      "output_token_price_per_m": 5000,
      "title": "meta-llama/llama-4-scout-17b-16e-instruct",
      "description": "Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, activating 17 billion parameters out of a total of 109B. It supports native multimodal input (text and image) and multilingual output (text and code) across 12 supported languages. Designed for assistant-style interaction and visual reasoning, Scout uses 16 experts per forward pass and features a context length of 10 million tokens, with a training corpus of ~40 trillion tokens.\n\nBuilt for high efficiency and local or commercial deployment, Llama 4 Scout incorporates early fusion for seamless modality integration. It is instruction-tuned for use in multilingual chat, captioning, and image understanding tasks. Released under the Llama 4 Community License, it was last trained on data up to August 2024 and launched publicly on April 5, 2025.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Llama 4 Scout Instruct",
      "model_type": "chat",
      "max_output_tokens": 131072,
      "features": [
        "function-calling",
        "vision"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1713938472,
      "id": "microsoft/wizardlm-2-8x22b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 6200,
      "output_token_price_per_m": 6200,
      "title": "microsoft/wizardlm-2-8x22b",
      "description": "WizardLM-2 8x22B is Microsoft AI's most advanced Wizard model. It demonstrates highly competitive performance compared to leading proprietary models, and it consistently outperforms all existing state-of-the-art opensource models.",
      "tags": [],
      "context_size": 65535,
      "status": 1,
      "display_name": "Wizardlm 2 8x22B",
      "model_type": "chat",
      "max_output_tokens": 8000,
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1750139830,
      "id": "minimaxai/minimax-m1-80k",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 5500,
      "output_token_price_per_m": 22000,
      "title": "minimaxai/minimax-m1-80k",
      "description": "MiniMax-M1: The World's First Open-Weight, Large-Scale Hybrid Attention Inference Model\n\nMiniMax-M1 adopts a Mixture of Experts (MoE) architecture and integrates the Flash Attention mechanism. The model contains a total of 456 billion parameters, with 45.9 billion parameters activated per token.\n\nNatively, the M1 model supports a context length of 1 million tokens\u20148 times that of DeepSeek R1. Additionally, by combining the CISPO algorithm with an efficient hybrid attention design for reinforcement learning training, MiniMax-M1 achieves industry-leading performance in long-context reasoning and real-world software engineering scenarios.",
      "tags": [],
      "context_size": 1000000,
      "status": 1,
      "display_name": "MiniMax M1",
      "model_type": "chat",
      "max_output_tokens": 40000,
      "features": [
        "function-calling",
        "reasoning"
      ]
    },
    {
      "created": 1719492913,
      "id": "mistralai/mistral-7b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 290,
      "output_token_price_per_m": 590,
      "title": "mistralai/mistral-7b-instruct",
      "description": "A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.",
      "tags": [],
      "context_size": 32768,
      "status": 1,
      "display_name": "Mistral 7B Instruct",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1722337858,
      "id": "mistralai/mistral-nemo",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 400,
      "output_token_price_per_m": 1700,
      "title": "mistralai/mistral-nemo",
      "description": "A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA. The model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi. It supports function calling and is released under the Apache 2.0 license.",
      "tags": [],
      "context_size": 60288,
      "status": 1,
      "display_name": "Mistral Nemo",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": [
        "structured-outputs"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1757052991,
      "id": "moonshotai/kimi-k2-0905",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 6000,
      "output_token_price_per_m": 25000,
      "title": "moonshotai/kimi-k2-0905",
      "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k.\nThis update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. Kimi K2 is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. It excels across coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) benchmarks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.",
      "tags": [],
      "context_size": 262144,
      "status": 1,
      "display_name": "Kimi K2 0905",
      "model_type": "chat",
      "max_output_tokens": 262144,
      "features": [
        "structured-outputs",
        "function-calling"
      ],
      "endpoints": [
        "completion",
        "anthropic"
      ]
    },
    {
      "created": 1752263515,
      "id": "moonshotai/kimi-k2-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 5700,
      "output_token_price_per_m": 23000,
      "title": "moonshotai/kimi-k2-instruct",
      "description": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.Specifically designed for tool use, reasoning, and autonomous problem-solving.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Kimi K2 Instruct",
      "model_type": "chat",
      "max_output_tokens": 131072,
      "features": [
        "function-calling",
        "structured-outputs"
      ],
      "endpoints": [
        "anthropic"
      ]
    },
    {
      "created": 1719493012,
      "id": "nousresearch/hermes-2-pro-llama-3-8b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1400,
      "output_token_price_per_m": 1400,
      "title": "nousresearch/hermes-2-pro-llama-3-8b",
      "description": "Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house.",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "Hermes 2 Pro Llama 3 8B",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": [
        "structured-outputs"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1754438873,
      "id": "openai/gpt-oss-120b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1000,
      "output_token_price_per_m": 5000,
      "title": "openai/gpt-oss-120b",
      "description": "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "OpenAI GPT OSS 120B",
      "model_type": "chat",
      "max_output_tokens": 131072,
      "features": [
        "structured-outputs",
        "function-calling",
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1754438961,
      "id": "openai/gpt-oss-20b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 500,
      "output_token_price_per_m": 2000,
      "title": "openai/gpt-oss-20b",
      "description": "gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI\u2019s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "OpenAI: GPT OSS 20B",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": [
        "structured-outputs",
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1728962258,
      "id": "qwen/qwen-2.5-72b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3800,
      "output_token_price_per_m": 4000,
      "title": "qwen/qwen-2.5-72b-instruct",
      "description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.",
      "tags": [],
      "context_size": 32000,
      "status": 1,
      "display_name": "Qwen 2.5 72B Instruct",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": [
        "structured-outputs",
        "function-calling"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1744797581,
      "id": "qwen/qwen2.5-7b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 700,
      "output_token_price_per_m": 700,
      "title": "qwen/qwen2.5-7b-instruct",
      "description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.",
      "tags": [],
      "context_size": 32000,
      "status": 1,
      "display_name": "Qwen2.5 7B Instruct",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": [
        "function-calling",
        "structured-outputs"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1742888969,
      "id": "qwen/qwen2.5-vl-72b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 8000,
      "output_token_price_per_m": 8000,
      "title": "qwen/qwen2.5-vl-72b-instruct",
      "description": "Qwen2.5-VL, the latest vision-language model in the Qwen2.5 series, delivers enhanced multimodal capabilities including advanced visual comprehension for object/text recognition, chart/layout analysis, and agent-based dynamic tool orchestration. It processes long-form videos (>1 hour) with key event detection while enabling precise spatial annotation through bounding boxes or coordinate points. The model specializes in structured data extraction from scanned documents (invoices, tables, etc.) and achieves state-of-the-art performance across multimodal benchmarks encompassing image understanding, temporal video analysis, and agent task evaluations.",
      "tags": [],
      "context_size": 32768,
      "status": 1,
      "display_name": "Qwen2.5 VL 72B Instruct",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": [
        "vision",
        "video"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1745897024,
      "id": "qwen/qwen3-235b-a22b-fp8",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2000,
      "output_token_price_per_m": 8000,
      "title": "qwen/qwen3-235b-a22b-fp8",
      "description": "Achieves effective integration of inference and non-inference modes, enabling seamless switching between modes during conversations. The model's inference capability significantly surpasses that of QwQ, and its general capabilities exceed those of Qwen2.5-72B-Instruct, reaching the state-of-the-art (SOTA) level among models of the same scale.",
      "tags": [],
      "context_size": 40960,
      "status": 1,
      "display_name": "Qwen3 235B A22B",
      "model_type": "chat",
      "max_output_tokens": 20000,
      "features": [
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1753176794,
      "id": "qwen/qwen3-235b-a22b-instruct-2507",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1500,
      "output_token_price_per_m": 8000,
      "title": "qwen/qwen3-235b-a22b-instruct-2507",
      "description": "Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \"thinking mode\" (<think> blocks).\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.",
      "tags": [],
      "context_size": 262144,
      "status": 1,
      "display_name": "Qwen3 235B A22B Instruct 2507",
      "model_type": "chat",
      "max_output_tokens": 262144,
      "features": [
        "structured-outputs",
        "function-calling"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1753443150,
      "id": "qwen/qwen3-235b-a22b-thinking-2507",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3000,
      "output_token_price_per_m": 30000,
      "title": "qwen/qwen3-235b-a22b-thinking-2507",
      "description": "The Qwen3-235B-A22B-Thinking-2507 represents the newest thinking-enabled model in the Qwen3 series, delivering groundbreaking improvements in reasoning capabilities. This advanced AI demonstrates significantly enhanced performance across logical reasoning, mathematics, scientific analysis, coding tasks, and academic benchmarks - matching or even surpassing human-expert level performance to achieve state-of-the-art results among open-source thinking models. Beyond its exceptional reasoning skills, the model shows markedly better general capabilities including more precise instruction following, sophisticated tool usage, highly natural text generation, and improved alignment with human preferences. It also features enhanced 256K long-context understanding, allowing it to maintain coherence and depth across extended documents and complex discussions.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Qwen3 235B A22b Thinking 2507",
      "model_type": "chat",
      "max_output_tokens": 131072,
      "features": [
        "function-calling",
        "structured-outputs",
        "reasoning"
      ],
      "endpoints": [
        "completion",
        "anthropic"
      ]
    },
    {
      "created": 1745897181,
      "id": "qwen/qwen3-30b-a3b-fp8",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 900,
      "output_token_price_per_m": 4500,
      "title": "qwen/qwen3-30b-a3b-fp8",
      "description": "Achieves effective integration of inference and non-inference modes, allowing seamless switching between modes during conversations. Its inference capability matches that of QwQ-32B with a smaller parameter size, and its general capabilities significantly surpass those of Qwen2.5-14B, reaching the state-of-the-art (SOTA) level among models of the same scale.",
      "tags": [],
      "context_size": 40960,
      "status": 1,
      "display_name": "Qwen3 30B A3B",
      "model_type": "chat",
      "max_output_tokens": 20000,
      "features": [
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1745897287,
      "id": "qwen/qwen3-32b-fp8",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1000,
      "output_token_price_per_m": 4500,
      "title": "qwen/qwen3-32b-fp8",
      "description": "Achieves effective integration of inference and non-inference modes, allowing seamless switching between modes during conversations. Its inference capability matches that of QwQ-32B with a smaller parameter size, and its general capabilities significantly surpass those of Qwen2.5-14B, reaching the state-of-the-art (SOTA) level among models of the same scale.",
      "tags": [],
      "context_size": 40960,
      "status": 1,
      "display_name": "Qwen3 32B",
      "model_type": "chat",
      "max_output_tokens": 20000,
      "features": [
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1745901504,
      "id": "qwen/qwen3-4b-fp8",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 0,
      "output_token_price_per_m": 0,
      "title": "qwen/qwen3-4b-fp8",
      "description": "Achieves effective integration of reasoning and non-reasoning modes, allowing seamless switching during conversations. The model delivers state-of-the-art (SOTA) reasoning performance among models of the same scale, with significantly enhanced human preference alignment. Notable improvements are seen in creative writing, role-playing, multi-turn dialogue, and instruction following, leading to a clearly improved user experience.",
      "tags": [],
      "context_size": 128000,
      "status": 1,
      "display_name": "Qwen3 4B",
      "model_type": "chat",
      "max_output_tokens": 20000,
      "features": [
        "reasoning"
      ],
      "endpoints": [
        "completion",
        "batch-api"
      ]
    },
    {
      "created": 1745901633,
      "id": "qwen/qwen3-8b-fp8",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 350,
      "output_token_price_per_m": 1380,
      "title": "qwen/qwen3-8b-fp8",
      "description": "Achieves effective integration of reasoning and non-reasoning modes, allowing seamless mode switching during conversations. Its reasoning capability reaches state-of-the-art (SOTA) performance among models of the same scale, and its general capabilities significantly outperform those of Qwen2.5-7B.",
      "tags": [],
      "context_size": 128000,
      "status": 1,
      "display_name": "Qwen3 8B",
      "model_type": "chat",
      "max_output_tokens": 20000,
      "features": [
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1753233789,
      "id": "qwen/qwen3-coder-480b-a35b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3500,
      "output_token_price_per_m": 15000,
      "title": "qwen/qwen3-coder-480b-a35b-instruct",
      "description": "Qwen3-Coder-480B-A35B-Instruct is a cutting-edge open coding model from Qwen, matching Claude Sonnet\u2019s performance in agentic programming, browser automation, and core development tasks. With native 256K context (extendable to 1M tokens via YaRN), it excels at repository-scale analysis and features specialized function-call support for platforms like Qwen Code and CLINE\u2014making it ideal for complex, real-world development workflows.",
      "tags": [],
      "context_size": 262144,
      "status": 1,
      "display_name": "Qwen3 Coder 480B A35B Instruct",
      "model_type": "chat",
      "max_output_tokens": 262144,
      "features": [
        "function-calling",
        "structured-outputs"
      ],
      "endpoints": [
        "completion",
        "anthropic"
      ]
    },
    {
      "created": 1718699128,
      "id": "sao10k/l3-70b-euryale-v2.1",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 14800,
      "output_token_price_per_m": 14800,
      "title": "sao10k/l3-70b-euryale-v2.1",
      "description": "The uncensored llama3 model is a powerhouse of creativity, excelling in both roleplay and story writing. It offers a liberating experience during roleplays, free from any restrictions. This model stands out for its immense creativity, boasting a vast array of unique ideas and plots, truly a treasure trove for those seeking originality. Its unrestricted nature during roleplays allows for the full breadth of imagination to unfold, akin to an enhanced, big-brained version of Stheno. Perfect for creative minds seeking a boundless platform for their imaginative expressions, the uncensored llama3 model is an ideal choice",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "L3 70B Euryale V2.1\t",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": [
        "function-calling"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1732791714,
      "id": "sao10k/l3-8b-lunaris",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 500,
      "output_token_price_per_m": 500,
      "title": "sao10k/l3-8b-lunaris",
      "description": "A generalist / roleplaying model merge based on Llama 3.",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "Sao10k L3 8B Lunaris\t",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": [
        "structured-outputs",
        "function-calling"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1726742141,
      "id": "sao10k/l31-70b-euryale-v2.2",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 14800,
      "output_token_price_per_m": 14800,
      "title": "sao10k/l31-70b-euryale-v2.2",
      "description": "Euryale L3.1 70B v2.2 is a model focused on creative roleplay from Sao10k. It is the successor of Euryale L3 70B v2.1.",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "L31 70B Euryale V2.2",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": [
        "function-calling"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1718626452,
      "id": "sophosympatheia/midnight-rose-70b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 8000,
      "output_token_price_per_m": 8000,
      "title": "sophosympatheia/midnight-rose-70b",
      "description": "A merge with a complex family tree, this model was crafted for roleplaying and storytelling. Midnight Rose is a successor to Rogue Rose and Aurora Nights and improves upon them both. It wants to produce lengthy output by default and is the best creative writing merge produced so far by sophosympatheia.",
      "tags": [],
      "context_size": 4096,
      "status": 1,
      "display_name": "Midnight Rose 70B",
      "model_type": "chat",
      "max_output_tokens": 2048,
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1744798076,
      "id": "thudm/glm-4-32b-0414",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 5500,
      "output_token_price_per_m": 16600,
      "title": "thudm/glm-4-32b-0414",
      "description": "GLM-4-32B-0414 is the latest open-source model in the GLM series, featuring 32 billion parameters. Its performance is comparable to OpenAI's GPT series and DeepSeek's V3/R1 series, while also supporting highly user-friendly local deployment capabilities.\nGLM-4-32B-Base-0414 was pre-trained on 15T of high-quality data, including a large amount of reasoning-type synthetic data, which laid a solid foundation for subsequent reinforcement learning extensions. In the post-training stage, in addition to human preference alignment for dialogue scenarios, the research team enhanced the model\u2019s performance in instruction following, engineering code, and function calling using techniques such as rejection sampling and reinforcement learning, thereby strengthening the atomic capabilities required for agent tasks.\nGLM-4-32B-0414 has achieved strong results in engineering code generation, artifact creation, function calling, search-based question answering, and report generation. On several benchmarks, its performance appr",
      "tags": [],
      "context_size": 32000,
      "status": 1,
      "display_name": "GLM-4-32B-0414",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": [
        "function-calling",
        "structured-outputs"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1752056181,
      "id": "thudm/glm-4.1v-9b-thinking",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 350,
      "output_token_price_per_m": 1380,
      "title": "thudm/glm-4.1v-9b-thinking",
      "description": "GLM-4.1V-9B-Thinking is an open-source Vision-Language Model (VLM) jointly released by Zhipu AI and Tsinghua University\u2019s KEG Lab, specifically designed to handle complex multimodal cognitive tasks. Built upon the GLM-4-9B-0414 base model, it integrates Chain-of-Thought (CoT) reasoning and employs reinforcement learning strategies, significantly enhancing its cross-modal reasoning capabilities and stability. As a lightweight model with 9B parameters, it strikes an optimal balance between deployment efficiency and performance. Across 28 authoritative benchmark evaluations, it matches or surpasses the performance of the 72B-parameter Qwen-2.5-VL-72B in 18 metrics. The model excels in tasks such as image-text understanding, mathematical and scientific reasoning, and video comprehension, while also supporting 4K-resolution images and arbitrary aspect ratios.",
      "tags": [],
      "context_size": 65536,
      "status": 1,
      "display_name": "GLM 4.1V 9B Thinking",
      "model_type": "chat",
      "max_output_tokens": 8000,
      "features": [
        "vision",
        "reasoning"
      ]
    },
    {
      "created": 1753709673,
      "id": "zai-org/glm-4.5",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 6000,
      "output_token_price_per_m": 22000,
      "title": "zai-org/glm-4.5",
      "description": "GLM-4.5 Series Models are foundation models specifically engineered for intelligent agents. The flagship GLM-4.5 integrates 355 billion total parameters (32 billion active), unifying reasoning, coding, and agent capabilities to address complex application demands.\nAs a hybrid reasoning system, it offers dual operational modes:\n- Thinking Mode: Enables complex reasoning, tool invocation, and strategic planning\n- Non-Thinking Mode: Delivers low-latency responses for real-time interactions\nThis architecture bridges high-performance AI with adaptive functionality for dynamic agent environments.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "GLM-4.5",
      "model_type": "chat",
      "max_output_tokens": 131072,
      "features": [
        "structured-outputs",
        "function-calling",
        "reasoning"
      ],
      "endpoints": [
        "completion"
      ]
    },
    {
      "created": 1754914926,
      "id": "zai-org/glm-4.5v",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 6000,
      "output_token_price_per_m": 18000,
      "title": "zai-org/glm-4.5v",
      "description": "Z.ai's GLM-4.5V sets a new standard in visual reasoning, achieving SOTA performance across 42 benchmarks among open-source models. Beyond benchmarks, it excels in real-world applications through hybrid training, enabling comprehensive visual understanding\u2014from image/video analysis and GUI interaction to complex document processing and precise visual grounding.\n\nIn China's GeoGuessr challenge, GLM-4.5V surpassed 99% of 21,000 human players within 16 hours, reaching 66th place in a week. Built on the GLM-4.5-Air foundation and inheriting GLM-4.1V-Thinking's approach, it leverages a 106B-parameter MoE architecture for scalable, efficient performance. This model bridges advanced AI research with practical deployment, delivering unmatched visual intelligence",
      "tags": [],
      "context_size": 65536,
      "status": 1,
      "display_name": "GLM 4.5V",
      "model_type": "chat",
      "max_output_tokens": 65536,
      "features": [
        "structured-outputs",
        "function-calling",
        "vision",
        "reasoning",
        "video"
      ],
      "endpoints": [
        "completion"
      ]
    }
  ]
}