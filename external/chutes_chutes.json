{
  "total": 340,
  "page": 0,
  "limit": 25,
  "items": [
    {
      "chute_id": "83594561-0940-5839-ac92-1d94dd280567",
      "name": "unsloth/gemma-3-12b-it",
      "tagline": "Gemma 3 is an open-source multimodal language model from Google DeepMind available in sizes from 1B to 27B parameters that can process both text and images to generate high-quality text responses, making it suitable for tasks like question answering, summarization, and image analysis while being efficient enough to run on consumer hardware.",
      "readme": "\n# Gemma 3 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/core)\n\n**Resources and Technical Documentation**:\n\n* [Gemma 3 Technical Report][g3-tech-report]\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma3]\n\n**Terms of Use**: [Terms][terms]\n\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\n\n### Inputs and outputs\n\n-   **Input:**\n    -  Text string, such as a question, a prompt, or a document to be summarized\n    -  Images, normalized to 896 x 896 resolution and encoded to 256 tokens\n       each\n    -  Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n       32K tokens for the 1B size\n\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output context of 8192 tokens\n\n### Usage\n\nBelow, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0. \n\n```sh\n$ pip install -U transformers\n```\n\nThen, copy the snippet from the section that is relevant for your use case.\n\n#### Running with the `pipeline` API\n\nYou can initialize the model and processor for inference with `pipeline` as follows.\n\n```python\nfrom transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3-12b-it\",\n    device=\"cuda\",\n    torch_dtype=torch.bfloat16\n)\n```\n\nWith instruction-tuned models, you need to use chat templates to process our inputs first. Then, you can pass it to the pipeline.\n\n```python\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n        ]\n    }\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n# Okay, let's take a look! \n# Based on the image, the animal on the candy is a **turtle**. \n# You can see the shell shape and the head and legs.\n```\n\n#### Running the model on a single / multi GPU\n\n```python\n# pip install accelerate\n\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\n\nmodel_id = \"google/gemma-3-12b-it\"\n\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n    model_id, device_map=\"auto\"\n).eval()\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n        ]\n    }\n]\n\ninputs = processor.apply_chat_template(\n    messages, add_generation_prompt=True, tokenize=True,\n    return_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\n\ninput_len = inputs[\"input_ids\"].shape[-1]\n\nwith torch.inference_mode():\n    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n    generation = generation[0][input_len:]\n\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene, \n# focusing on a cluster of pink cosmos flowers and a busy bumblebee. \n# It has a slightly soft, natural feel, likely captured in daylight.\n```\n\n### Citation\n\n```none\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |      77.2     |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot         |      63.2      |      72.3     |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot         |      73.8      |      79.6     |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |      51.9     |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |      65.8     |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot         |      9.48      |      20.0     |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot        |      38.4      |      56.2     |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot         |      73.0      |      82.4     |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot         |      58.2      |      64.7     |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          | few-shot       |      28.4      |      50.9     |      72.6      |      77.7      |\n| [DROP][drop]                   | 1-shot         |      42.4      |      60.1     |      72.2      |      77.2      |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### STEM and code\n\n| Benchmark                      | Metric         | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:-------------:|:--------------:|:--------------:|\n| [MMLU][mmlu]                   | 5-shot         |      59.6     |      74.5      |      78.6      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      29.2     |      45.3      |      52.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      42.1     |      57.4      |      66.2      |\n| [MATH][math]                   | 4-shot         |      24.2     |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 8-shot         |      38.4     |      71.0      |      82.6      |\n| [GPQA][gpqa]                   | 5-shot         |      15.0     |      25.4      |      24.3      |\n| [MBPP][mbpp]                   | 3-shot         |      46.0     |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | 0-shot         |      36.0     |      45.7      |      48.8      |\n\n[mmlu]: https://arxiv.org/abs/2009.03300\n[agieval]: https://arxiv.org/abs/2304.06364\n[math]: https://arxiv.org/abs/2103.03874\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n\n#### Multilingual\n\n| Benchmark                            | Gemma 3 PT 1B | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------------ |:-------------:|:-------------:|:--------------:|:--------------:|\n| [MGSM][mgsm]                         |      2.04     |      34.7     |      64.3     |      74.3     |\n| [Global-MMLU-Lite][global-mmlu-lite] |      24.9     |      57.0     |      69.4     |      75.7     |\n| [WMT24++][wmt24pp] (ChrF)            |      36.7     |      48.4     |      53.9     |      55.7     |\n| [FloRes][flores]                     |      29.5     |      39.2     |      46.0     |      48.8     |\n| [XQuAD][xquad] (all)                 |      43.9     |      68.0     |      74.5     |      76.8     |\n| [ECLeKTic][eclektic]                 |      4.69     |      11.0     |      17.2     |      24.4     |\n| [IndicGenBench][indicgenbench]       |      41.4     |      57.2     |      61.7     |      63.4     |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[flores]: https://arxiv.org/abs/2106.03193\n[xquad]: https://arxiv.org/abs/1910.11856v3\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[eclektic]: https://arxiv.org/abs/2502.21228\n[indicgenbench]: https://arxiv.org/abs/2404.16816\n\n#### Multimodal\n\n| Benchmark                      | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |:-------------:|:--------------:|:--------------:|\n| [COCOcap][coco-cap]            |      102      |      111       |      116       |\n| [DocVQA][docvqa] (val)         |      72.8     |      82.3      |      85.6      |\n| [InfoVQA][info-vqa] (val)      |      44.1     |      54.8      |      59.4      |\n| [MMMU][mmmu] (pt)              |      39.2     |      50.3      |      56.1      |\n| [TextVQA][textvqa] (val)       |      58.9     |      66.5      |      68.6      |\n| [RealWorldQA][realworldqa]     |      45.5     |      52.2      |      53.9      |\n| [ReMI][remi]                   |      27.3     |      38.5      |      44.8      |\n| [AI2D][ai2d]                   |      63.2     |      75.2      |      79.0      |\n| [ChartQA][chartqa]             |      63.6     |      74.7      |      76.3      |\n| [VQAv2][vqav2]                 |      63.9     |      71.2      |      72.9      |\n| [BLINK][blinkvqa]              |      38.0     |      35.9      |      39.6      |\n| [OKVQA][okvqa]                 |      51.0     |      58.7      |      60.2      |\n| [TallyQA][tallyqa]             |      42.5     |      51.8      |      54.3      |\n| [SpatialSense VQA][ss-vqa]     |      50.9     |      60.0      |      59.4      |\n| [CountBenchQA][countbenchqa]   |      26.1     |      17.8      |      68.0      |\n\n[coco-cap]: https://cocodataset.org/#home\n[docvqa]: https://www.docvqa.org/\n[info-vqa]: https://arxiv.org/abs/2104.12756\n[mmmu]: https://arxiv.org/abs/2311.16502\n[textvqa]: https://textvqa.org/\n[realworldqa]: https://paperswithcode.com/dataset/realworldqa\n[remi]: https://arxiv.org/html/2406.09175v1\n[ai2d]: https://allenai.org/data/diagrams\n[chartqa]: https://arxiv.org/abs/2203.10244\n[vqav2]: https://visualqa.org/index.html\n[blinkvqa]: https://arxiv.org/abs/2404.12390\n[okvqa]: https://okvqa.allenai.org/\n[tallyqa]: https://arxiv.org/abs/1810.12440\n[ss-vqa]: https://arxiv.org/abs/1908.02660\n[countbenchqa]: https://github.com/",
      "public": true,
      "version": "c1ec7ac7-36de-5c0f-ac4e-8b3f8bfc75ea",
      "tool_description": null,
      "slug": "chutes-unsloth-gemma-3-12b-it",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-03-12T20:06:59.776770Z",
      "updated_at": "2025-09-06T11:07:33.166455Z",
      "image": {
        "image_id": "468c185f-2e19-56d6-9508-6be534e42145",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "nightly-2025090600",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-09-06T09:40:20.310953Z",
        "build_started_at": "2025-09-06T09:40:22.578714Z",
        "build_completed_at": "2025-09-06T09:56:36.126262Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "5090",
        "a4000_ada",
        "a10",
        "a100_sxm",
        "l40s",
        "l4",
        "a40",
        "h100_nvl",
        "3090",
        "l40",
        "a100_40gb_sxm",
        "a5000",
        "a100_40gb",
        "h100",
        "a4000",
        "h20",
        "a6000",
        "h800",
        "a6000_ada",
        "4090",
        "a100",
        "h100_sxm",
        "pro_6000"
      ],
      "node_selector": {
        "exclude": [
          "mi300x",
          "b200",
          "h200"
        ],
        "include": null,
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 0.4,
        "supported_gpus": [
          "5090",
          "a4000_ada",
          "a10",
          "a100_sxm",
          "l40s",
          "l4",
          "a40",
          "h100_nvl",
          "3090",
          "l40",
          "a100_40gb_sxm",
          "a5000",
          "a100_40gb",
          "h100",
          "a4000",
          "h20",
          "a6000",
          "h800",
          "a6000_ada",
          "4090",
          "a100",
          "h100_sxm",
          "pro_6000"
        ]
      },
      "invocation_count": 5520060,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.04812860000000001,
            "tao": 0.00014175011258166776
          },
          "output": {
            "usd": 0.192608,
            "tao": 0.0005672761244692316
          }
        },
        "usd": {
          "hour": 5.2,
          "second": 0.0014444444444444444
        },
        "tao": {
          "hour": 0.015315230142257873,
          "second": 4.254230595071631e-06
        }
      },
      "instances": [
        {
          "instance_id": "b55570b9-0469-4c86-a7e0-6ba2c833dc47",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "ffbad112-ff38-4be0-9f42-dbc1312c5f4c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "1563b9c2-825c-47a4-8e44-b0fdde601a6d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "8aad6a15-9d95-4dd7-9908-561fe9a8bf2f",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "5e6fe616-d193-4e0b-90ff-3528f7ad6ca6",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "a912060b-7e16-4625-aad8-d1306efe05a8",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "8eea98e2-e729-4e4b-9632-8c441af05e49",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "7739a875-a485-4d3f-b985-14ba335970a9",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "60adbbb5-1eaf-400e-ac87-eb28cbe17e54",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "197506a3-ef08-4ae2-8048-00ffd03a0fc2",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "902374bc-c88c-496f-b569-b3e59890e264",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "88351107-792b-442b-b1aa-afbdd68c4904",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "6527c647-16c3-49a7-8868-d32364a91cc0",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "48672bd6-6144-4133-84bf-cc6d1513ae6a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        },
        {
          "instance_id": "8c97b85b-2407-47e3-99c5-b0a801f8eafc",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:44:21.763072Z"
        }
      ],
      "logo_id": "ae234896-8dab-42b5-a4b7-289b6b46b27d",
      "openrouter": true,
      "chutes_version": "0.3.23.rc1",
      "revision": "1b50bd0b78e6af2c4ea9e9bf220ac2b9a7a22655",
      "concurrency": 40,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/ae234896-8dab-42b5-a4b7-289b6b46b27d.webp",
      "hot": true
    },
    {
      "chute_id": "de510462-c319-543b-9c67-00bcf807d2a7",
      "name": "deepseek-ai/DeepSeek-R1",
      "tagline": "DeepSeek-R1 is a 671B parameter (37B activated) language model trained via reinforcement learning that excels at mathematical reasoning, coding, and complex problem-solving tasks, achieving performance comparable to OpenAI's models.",
      "readme": "# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>\ud83d\udc41\ufe0f</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](https://huggingface.co/deepseek-ai/DeepSeek-R1/raw/main/#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [\ud83e\udd17 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [\ud83e\udd17 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [\ud83e\udd17 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [\ud83e\udd17 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [\ud83e\udd17 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [\ud83e\udd17 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [\ud83e\udd17 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [\ud83e\udd17 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](https://huggingface.co/deepseek-ai/DeepSeek-R1/raw/ma",
      "public": true,
      "version": "2db66031-e0ee-59cf-87f0-bdad2b68490e",
      "tool_description": null,
      "slug": "chutes-deepseek-ai-deepseek-r1",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-02-21T15:33:35.673823Z",
      "updated_at": "2025-09-06T08:25:12.681047Z",
      "image": {
        "image_id": "0411f094-9b3a-5a38-b616-d6e0d3468015",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.4.6.post5b",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-06-19T07:37:36.053996Z",
        "build_started_at": "2025-06-19T07:37:38.058721Z",
        "build_completed_at": "2025-06-19T07:55:39.995629Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "499e4e81d40d",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "h200"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "h200"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 5.4,
        "supported_gpus": [
          "h200"
        ]
      },
      "invocation_count": 4936140,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.1999188,
            "tao": 0.0005888081599546199
          },
          "output": {
            "usd": 0.800064,
            "tao": 0.0023563777477952695
          }
        },
        "usd": {
          "hour": 21.6,
          "second": 0.006
        },
        "tao": {
          "hour": 0.06361710982168654,
          "second": 1.767141939491293e-05
        }
      },
      "instances": [
        {
          "instance_id": "169325fe-3833-460a-bf06-60987e79b62e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:20:23.059690Z"
        },
        {
          "instance_id": "9cfce5d9-f9d1-4335-a407-120b38f9ef5e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:45:03.563882Z"
        },
        {
          "instance_id": "12662f7b-774d-4043-b72d-95b3036a1565",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "417f26ec-cae5-4450-91b1-54b6abc6bf79",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "36835ff5-0ab7-4f47-ac89-22aa3c6a48be",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "a3e0df54-7beb-44b5-aa7a-8e0bb8878fc8",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "91a15fcd-6e81-4014-bc2b-f6dff295c809",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "acdb0abd-1b01-4563-bbba-3c7b3c21991a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "91d96c2d-f120-473d-aff1-9b155bd2af2d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "d128ae41-08a9-42c6-b763-4c683d79afc8",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "18b3c17c-5c27-4114-b910-c7e24f0b8c0e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "0fb9d1cc-cc44-45b7-8381-4dc5f1a0e763",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "a7179bbe-58e5-48c6-a022-a0bb9eb3db3d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "63345edb-9437-408d-8a91-591c549ffe04",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "411d26f0-e41b-44c0-a8f0-67dd3fb987a8",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "447205f2-64af-46c0-b36d-e7e22333a464",
          "region": "n/a",
          "active": false,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:53.928262Z"
        },
        {
          "instance_id": "9f588e2e-17c9-4046-a166-6c86fa530f36",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "b7998215-b8fa-4f73-ae52-3d0cc6cbefa5",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "e22764ed-b6ef-42f4-b784-7f308a953b6e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "14cb7941-fed1-440a-9993-56e3ce33349e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "35383d2d-4e4a-4b77-bea1-933e26b08238",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "f20be3d6-5c56-4dbe-92f6-2499f0da4a1d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "099b93a5-453c-44f4-a7fd-947aeb74c863",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "a34f86bd-72b1-4674-aa93-a4a6e8f1fd85",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "455a5ec3-3021-4454-bf52-35c080d2deca",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "24f2a3a1-6eef-4799-b768-ece69687c0ff",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "4eff8482-9dc2-4520-b698-0a89724832d7",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "1439a96b-de78-4249-b744-7c6bab94af6c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "ca38ca67-ea3b-4452-8b35-c1586ab2e307",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "a31559ef-6a3f-418c-b47a-f0d6cf5b08de",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "a2fd886b-7860-4d5e-a045-3acab811cff1",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "213bae64-61dc-4199-8b3e-6f8f41fc8a1a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "5388aaa7-be1d-47c0-8c50-6bd6b4ad88f6",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "f98b90f3-b90a-48c4-9c90-f707fdcac58d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "5289a846-a820-4f58-8b3f-d6b86dbd0c89",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "1b12e5f1-d528-4163-a4fd-81f515d47629",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "a1f4c9ba-d2b9-4b7f-9444-ace6b54178a4",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "ba386057-a10c-4224-b754-a5d1e2e5fe2e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "9fc9d9cc-ecb7-4e52-bf7b-7b7f36bc20f3",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "df726990-434d-4565-af07-42473a27c20a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "c1394bee-40f8-4ed1-8cd6-da22ac62c2d6",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "bffab184-da19-4868-a447-76411048ecaa",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "479bfa19-8224-46bb-91b2-f21982104dda",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "11502f24-658c-4246-9507-181b31210f5e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "d0c9c0ad-59b2-4616-9412-920becfded97",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:34:36.976596Z"
        },
        {
          "instance_id": "12280f55-668b-4d3d-8c89-dd833b2d11f8",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "dbc787cd-ab22-40a7-b01f-a9b39fb85d78",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "5191c10d-74bc-4725-8926-eefc2614e253",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "be0f42b9-9a18-40d6-a824-86351d14c944",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        },
        {
          "instance_id": "e56a9f89-b98a-4173-8b56-bfc435f0a0a0",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:35:28.903837Z"
        }
      ],
      "logo_id": "10fa8d70-f62b-45f2-90fd-bd311d36c604",
      "openrouter": true,
      "chutes_version": "0.3.18",
      "revision": "f7361cd9ff99396dbf6bd644ad846015e59ed4fc",
      "concurrency": 40,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/10fa8d70-f62b-45f2-90fd-bd311d36c604.webp",
      "hot": true
    },
    {
      "chute_id": "154ad01c-a431-5744-83c8-651215124360",
      "name": "deepseek-ai/DeepSeek-V3-0324",
      "tagline": "DeepSeek-V3-0324 is a powerful open-source language model that shows significant improvements in reasoning, coding, and Chinese language capabilities compared to its predecessor, making it particularly useful for tasks like web development, technical writing, and complex problem-solving.",
      "readme": "# DeepSeek-V3-0324\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n## Features\n\nDeepSeek-V3-0324 demonstrates notable improvements over its predecessor, DeepSeek-V3, in several key aspects.\n\n![Model Performance](figures/0324_comparison.png) https://huggingface.co/deepseek-ai/DeepSeek-V3-0324/raw/main/figures/0324_comparison.png\n\n### Reasoning Capabilities\n\n- Significant improvements in benchmark performance:\n  - MMLU-Pro: 75.9 \u2192 81.2 (+5.3)\n  - GPQA: 59.1 \u2192 68.4 (+9.3)\n  - AIME: 39.6 \u2192 59.4 (+19.8)\n  - LiveCodeBench: 39.2 \u2192 49.2 (+10.0)\n\n### Front-End Web Development\n\n- Improved the executability of the code\n- More aesthetically pleasing web pages and game front-ends\n\n### Chinese Writing Proficiency\n\n- Enhanced style and content quality:\n  - Aligned with the R1 writing style\n  - Better quality in medium-to-long-form writing\n\n- Feature Enhancements\n  - Improved multi-turn interactive rewriting\n  - Optimized translation quality and letter writing\n\n### Chinese Search Capabilities\n\n- Enhanced report analysis requests with more detailed outputs\n\n### Function Calling Improvements\n\n- Increased accuracy in Function Calling, fixing issues from previous V3 versions\n\n---\n\n## Usage Recommendations\n\n### System Prompt\n\nIn the official DeepSeek web/app, we use the same system prompt with a specific date.\n\n```\n\u8be5\u52a9\u624b\u4e3aDeepSeek Chat\uff0c\u7531\u6df1\u5ea6\u6c42\u7d22\u516c\u53f8\u521b\u9020\u3002\n\u4eca\u5929\u662f{current date}\u3002\n```\n\nFor example,\n\n```\n\u8be5\u52a9\u624b\u4e3aDeepSeek Chat\uff0c\u7531\u6df1\u5ea6\u6c42\u7d22\u516c\u53f8\u521b\u9020\u3002\n\u4eca\u5929\u662f3\u670824\u65e5\uff0c\u661f\u671f\u4e00\u3002\n```\n\n### Temperature\n\nIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.3. Because many users use the default temperature 1.0 in API call, we have implemented an API temperature $T_{api}$ mapping mechanism that adjusts the input API temperature value of 1.0 to the most suitable model temperature setting of 0.3.\n\n$$\nT_{model} = T_{api} \\times 0.3 \\quad (0 \\leq T_{api} \\leq 1)\n$$\n\n$$\nT_{model} = T_{api} - 0.7 \\quad (1 < T_{api} \\leq 2)\n$$\n\nThus, if you call V3 via API, temperature 1.0 equals to the model temperature 0.3.\n\n### Prompts for File Uploading and Web Search\n\nFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.\n\n```\nfile_template = \\\n\"\"\"[file name]: {file_name}\n[file content begin]\n{file_content}\n[file content end]\n{question}\"\"\"\n```\n\nFor Web Search, {search_results}, {cur_date}, and {question} are arguments.\n\nFor Chinese query, we use the prompt:\n\n```\nsearch_answer_zh_template = \\\n'''# \u4ee5\u4e0b\u5185\u5bb9\u662f\u57fa\u4e8e\u7528\u6237\u53d1\u9001\u7684\u6d88\u606f\u7684\u641c\u7d22\u7ed3\u679c:\n{search_results}\n\u5728\u6211\u7ed9\u4f60\u7684\u641c\u7d22\u7ed3\u679c\u4e2d\uff0c\u6bcf\u4e2a\u7ed3\u679c\u90fd\u662f[webpage X begin]...[webpage X end]\u683c\u5f0f\u7684\uff0cX\u4ee3\u8868\u6bcf\u7bc7\u6587\u7ae0\u7684\u6570\u5b57\u7d22\u5f15\u3002\u8bf7\u5728\u9002\u5f53\u7684\u60c5\u51b5\u4e0b\u5728\u53e5\u5b50\u672b\u5c3e\u5f15\u7528\u4e0a\u4e0b\u6587\u3002\u8bf7\u6309\u7167\u5f15\u7528\u7f16\u53f7[citation:X]\u7684\u683c\u5f0f\u5728\u7b54\u6848\u4e2d\u5bf9\u5e94\u90e8\u5206\u5f15\u7528\u4e0a\u4e0b\u6587\u3002\u5982\u679c\u4e00\u53e5\u8bdd\u6e90\u81ea\u591a\u4e2a\u4e0a\u4e0b\u6587\uff0c\u8bf7\u5217\u51fa\u6240\u6709\u76f8\u5173\u7684\u5f15\u7528\u7f16\u53f7\uff0c\u4f8b\u5982[citation:3][citation:5]\uff0c\u5207\u8bb0\u4e0d\u8981\u5c06\u5f15\u7528\u96c6\u4e2d\u5728\u6700\u540e\u8fd4\u56de\u5f15\u7528\u7f16\u53f7\uff0c\u800c\u662f\u5728\u7b54\u6848\u5bf9\u5e94\u90e8\u5206\u5217\u51fa\u3002\n\u5728\u56de\u7b54\u65f6\uff0c\u8bf7\u6ce8\u610f\u4ee5\u4e0b\u51e0\u70b9\uff1a\n- \u4eca\u5929\u662f{cur_date}\u3002\n- \u5e76\u975e\u641c\u7d22\u7ed3\u679c\u7684\u6240\u6709\u5185\u5bb9\u90fd\u4e0e\u7528\u6237\u7684\u95ee\u9898\u5bc6\u5207\u76f8\u5173\uff0c\u4f60\u9700\u8981\u7ed3\u5408\u95ee\u9898\uff0c\u5bf9\u641c\u7d22\u7ed3\u679c\u8fdb\u884c\u7504\u522b\u3001\u7b5b\u9009\u3002\n- \u5bf9\u4e8e\u5217\u4e3e\u7c7b\u7684\u95ee\u9898\uff08\u5982\u5217\u4e3e\u6240\u6709\u822a\u73ed\u4fe1\u606f\uff09\uff0c\u5c3d\u91cf\u5c06\u7b54\u6848\u63a7\u5236\u572810\u4e2a\u8981\u70b9\u4ee5\u5185\uff0c\u5e76\u544a\u8bc9\u7528\u6237\u53ef\u4ee5\u67e5\u770b\u641c\u7d22\u6765\u6e90\u3001\u83b7\u5f97\u5b8c\u6574\u4fe1\u606f\u3002\u4f18\u5148\u63d0\u4f9b\u4fe1\u606f\u5b8c\u6574\u3001\u6700\u76f8\u5173\u7684\u5217\u4e3e\u9879\uff1b\u5982\u975e\u5fc5\u8981\uff0c\u4e0d\u8981\u4e3b\u52a8\u544a\u8bc9\u7528\u6237\u641c\u7d22\u7ed3\u679c\u672a\u63d0\u4f9b\u7684\u5185\u5bb9\u3002\n- \u5bf9\u4e8e\u521b\u4f5c\u7c7b\u7684\u95ee\u9898\uff08\u5982\u5199\u8bba\u6587\uff09\uff0c\u8bf7\u52a1\u5fc5\u5728\u6b63\u6587\u7684\u6bb5\u843d\u4e2d\u5f15\u7528\u5bf9\u5e94\u7684\u53c2\u8003\u7f16\u53f7\uff0c\u4f8b\u5982[citation:3][citation:5]\uff0c\u4e0d\u80fd\u53ea\u5728\u6587\u7ae0\u672b\u5c3e\u5f15\u7528\u3002\u4f60\u9700\u8981\u89e3\u8bfb\u5e76\u6982\u62ec\u7528\u6237\u7684\u9898\u76ee\u8981\u6c42\uff0c\u9009\u62e9\u5408\u9002\u7684\u683c\u5f0f\uff0c\u5145\u5206\u5229\u7528\u641c\u7d22\u7ed3\u679c\u5e76\u62bd\u53d6\u91cd\u8981\u4fe1\u606f\uff0c\u751f\u6210\u7b26\u5408\u7528\u6237\u8981\u6c42\u3001\u6781\u5177\u601d\u60f3\u6df1\u5ea6\u3001\u5bcc\u6709\u521b\u9020\u529b\u4e0e\u4e13\u4e1a\u6027\u7684\u7b54\u6848\u3002\u4f60\u7684\u521b\u4f5c\u7bc7\u5e45\u9700\u8981\u5c3d\u53ef\u80fd\u5ef6\u957f\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u8981\u70b9\u7684\u8bba\u8ff0\u8981\u63a8\u6d4b\u7528\u6237\u7684\u610f\u56fe\uff0c\u7ed9\u51fa\u5c3d\u53ef\u80fd\u591a\u89d2\u5ea6\u7684\u56de\u7b54\u8981\u70b9\uff0c\u4e14\u52a1\u5fc5\u4fe1\u606f\u91cf\u5927\u3001\u8bba\u8ff0\u8be6\u5c3d\u3002\n- \u5982\u679c\u56de\u7b54\u5f88\u957f\uff0c\u8bf7\u5c3d\u91cf\u7ed3\u6784\u5316\u3001\u5206\u6bb5\u843d\u603b\u7ed3\u3002\u5982\u679c\u9700\u8981\u5206\u70b9\u4f5c\u7b54\uff0c\u5c3d\u91cf\u63a7\u5236\u57285\u4e2a\u70b9\u4ee5\u5185\uff0c\u5e76\u5408\u5e76\u76f8\u5173\u7684\u5185\u5bb9\u3002\n- \u5bf9\u4e8e\u5ba2\u89c2\u7c7b\u7684\u95ee\u7b54\uff0c\u5982\u679c\u95ee\u9898\u7684\u7b54\u6848\u975e\u5e38\u7b80\u77ed\uff0c\u53ef\u4ee5\u9002\u5f53\u8865\u5145\u4e00\u5230\u4e24\u53e5\u76f8\u5173\u4fe1\u606f\uff0c\u4ee5\u4e30\u5bcc\u5185\u5bb9\u3002\n- \u4f60\u9700\u8981\u6839\u636e\u7528\u6237\u8981\u6c42\u548c\u56de\u7b54\u5185\u5bb9\u9009\u62e9\u5408\u9002\u3001\u7f8e\u89c2\u7684\u56de\u7b54\u683c\u5f0f\uff0c\u786e\u4fdd\u53ef\u8bfb\u6027\u5f3a\u3002\n- \u4f60\u7684\u56de\u7b54\u5e94\u8be5\u7efc\u5408\u591a\u4e2a\u76f8\u5173\u7f51\u9875\u6765\u56de\u7b54\uff0c\u4e0d\u80fd\u91cd\u590d\u5f15\u7528\u4e00\u4e2a\u7f51\u9875\u3002\n- \u9664\u975e\u7528\u6237\u8981\u6c42\uff0c\u5426\u5219\u4f60\u56de\u7b54\u7684\u8bed\u8a00\u9700\u8981\u548c\u7528\u6237\u63d0\u95ee\u7684\u8bed\u8a00\u4fdd\u6301\u4e00\u81f4\u3002\n\n# \u7528\u6237\u6d88\u606f\u4e3a\uff1a\n{question}'''\n```\n\nFor English query, we use the prompt:\n\n```\nsearch_answer_en_template = \\\n'''# The following contents are the search results related to the user's message:\n{search_results}\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\nWhen responding, please keep the following points in mind:\n- Today is {cur_date}.\n- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\n- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\n- Unless the user requests otherwise, your response should be in the same language as the user's question.\n\n# The user's message is:\n{question}'''\n```\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3-0324 is exactly the same as DeepSeek-V3. Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running this model locally.\n\n**This model supports features such as function calling, JSON output, and FIM completion. For instructions on how to construct prompts to use these features, please refer to [DeepSeek-V2.5](https://huggingface.co/deepseek-ai/DeepSeek-V2.5#function-calling) repo.**\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324/raw/main/LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324/raw/main/service@deepseek.com).\n",
      "public": true,
      "version": "9192277f-0652-5282-a881-81309f069657",
      "tool_description": null,
      "slug": "chutes-deepseek-ai-deepseek-v3-0324",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-05-05T09:41:13.378848Z",
      "updated_at": "2025-09-06T10:34:36.810306Z",
      "image": {
        "image_id": "468c185f-2e19-56d6-9508-6be534e42145",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "nightly-2025090600",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-09-06T09:40:20.310953Z",
        "build_started_at": "2025-09-06T09:40:22.578714Z",
        "build_completed_at": "2025-09-06T09:56:36.126262Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "h200"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "h200"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 5.4,
        "supported_gpus": [
          "h200"
        ]
      },
      "invocation_count": 2820579,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.1999188,
            "tao": 0.0005888081599546199
          },
          "output": {
            "usd": 0.800064,
            "tao": 0.0023563777477952695
          }
        },
        "usd": {
          "hour": 21.6,
          "second": 0.006
        },
        "tao": {
          "hour": 0.06361710982168654,
          "second": 1.767141939491293e-05
        }
      },
      "instances": [
        {
          "instance_id": "d036708f-629d-41de-91df-397774c91b11",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "81e6e681-e1ec-4a09-9fb2-f87a848832cd",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "d0240f60-6a5b-4cce-b82c-41116b765eee",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "dbb1de38-2da2-4d83-bca7-2ba427e64cef",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "f3ccb9c6-5395-4cbc-ac63-244515a60b90",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "d7db9c39-aa19-4510-8db8-6fa4b41edb56",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "ec557cea-154d-42f0-a43e-80717610f6ae",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "3ecb05e3-1d53-48ab-9459-d5f1e2cfc1e9",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "13a1d7ff-eb48-4c5a-9d13-f64604d844cc",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "43ca6b75-8f35-4bcb-b560-c39f3d35322b",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "a76ff664-403c-477c-bbac-bef5e58c8b07",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "64572d35-cfa8-4a0d-8cc4-076658ac45d6",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "bb1a0ffa-b0ce-4736-b561-19b97c5b7ee1",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "a856f42e-aff5-447c-b17c-8f6020226685",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "f0837ece-c6a7-4d99-8923-e1428d2083dc",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "9d76f6fc-545e-4e63-aca8-f84ee13e6caf",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "24006839-af65-4d7e-b964-2705eba5f0a5",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "6efc1eba-ce54-4465-bb60-5004b55525c7",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "21d2328f-36e3-4717-9f61-22217bebc882",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "36f22f12-6db6-418f-baf5-795ff0b40c2c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "b15ce63b-56b0-4d01-94a4-c69beaba4d9d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "2fcf5bb2-79cf-4299-aed3-9dc20129b6ea",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "381d34aa-f5cb-42ea-b7c3-ddc49380858a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "4eaf1f11-49bd-4bb5-87d0-d7800a5e31bc",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "7c541151-4a0d-47c5-8bd4-eba449e78650",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "cf50b346-a4d4-43d3-9a66-aad8906abeb0",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "2d9d431e-70fc-473b-9eb5-20fb051966ac",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "7a69f006-db3a-4a12-8309-7fe83a1bfdbe",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "ffd8f05f-a6e3-474f-95ee-d4ff25e149a2",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "42516118-68ce-436e-9969-62937a87ac5c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "8d5c02e0-989d-4b85-af0f-3529c277ea1f",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "480ee625-6247-4bad-a2e4-5f6640cef907",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "6a95be19-1a8a-4842-8f2a-1f9c7564f066",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "7703440b-520a-4ecf-8be8-18c2ad9054a9",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "1cafdb4a-cbfe-4618-b42a-56cb4547172c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        },
        {
          "instance_id": "5f688cc8-6a56-44be-8ad4-f371311d7999",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:03:18.287670Z"
        }
      ],
      "logo_id": "c62a285f-8e3a-46eb-940f-8ec9bf9e81e2",
      "openrouter": true,
      "chutes_version": "0.3.23.rc1",
      "revision": "e9b33add76883f293d6bf61f6bd89b497e80e335",
      "concurrency": 40,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/c62a285f-8e3a-46eb-940f-8ec9bf9e81e2.webp",
      "hot": true
    },
    {
      "chute_id": "20acffc0-0c5f-58e3-97af-21fc0b261ec4",
      "name": "unsloth/Llama-3.2-3B-Instruct",
      "tagline": "Llama 3.2 3B is a 3 billion parameter open-source language model from Meta that excels at multilingual dialogue, retrieval, and summarization tasks in 8 officially supported languages.",
      "readme": "\n## ***See [our collection](https://huggingface.co/collections/unsloth/llama-32-66f46afde4ca573864321a22) for all versions of Llama 3.2 including GGUF, 4-bit and original 16-bit formats.***\n\n# Finetune Llama 3.2, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\n\nWe have a free Google Colab Tesla T4 notebook for Llama 3.2 (3B) here: https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png\" width=\"200\"/>](https://discord.gg/unsloth)\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n\n# unsloth/Llama-3.2-3B-Instruct\nFor more details on the model, please go to Meta's original [model card](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)\n\n## \u2728 Finetune for Free\n\nAll notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\n\n| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\n|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\n| **Llama-3.2 (3B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)               | 2.4x faster | 58% less |\n| **Llama-3.1 (11B vision)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)               | 2.4x faster | 58% less |\n| **Llama-3.1 (8B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)               | 2.4x faster | 58% less |\n| **Phi-3.5 (mini)** | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing)               | 2x faster | 50% less |\n| **Gemma 2 (9B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)               | 2.4x faster | 58% less |\n| **Mistral (7B)**    | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/drive/1Dyauq4kTZoLewQ1cApceUQVNcnnNTzg_?usp=sharing)               | 2.2x faster | 62% less |\n| **DPO - Zephyr**     | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)               | 1.9x faster | 19% less |\n\n- This [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing) is useful for ShareGPT ChatML / Vicuna templates.\n- This [text completion notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.\n- \\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\n\n## Special Thanks\nA huge thank you to the Meta and Llama team for creating and releasing these models.\n\n## Model Information\n\nThe Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model developer**: Meta\n\n**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n**Supported languages:**  English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n\n**Llama 3.2 family of models** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** Sept 25, 2024\n\n**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\n\n**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n",
      "public": true,
      "version": "aafd27f0-831e-5533-9b6e-ce3bfb8c1b66",
      "tool_description": null,
      "slug": "chutes-unsloth-llama-3-2-3b-instruct",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-07-23T17:22:04.445902Z",
      "updated_at": "2025-09-06T11:05:23.851575Z",
      "image": {
        "image_id": "89c8efd3-fff2-530f-a9b2-e344dcaaebe2",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.5.1.post3",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-30T09:40:05.257537Z",
        "build_started_at": "2025-08-30T09:40:08.281915Z",
        "build_completed_at": "2025-08-30T10:01:00.019895Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "5090",
        "a10",
        "a100_sxm",
        "l40s",
        "l4",
        "a40",
        "h100_nvl",
        "3090",
        "l40",
        "a100_40gb_sxm",
        "a5000",
        "a100_40gb",
        "pro_6000",
        "h20",
        "a6000",
        "h800",
        "4090",
        "a100",
        "a6000_ada"
      ],
      "node_selector": {
        "exclude": [
          "mi300x",
          "b200",
          "h200",
          "h100",
          "h100_sxm"
        ],
        "include": null,
        "gpu_count": 1,
        "min_vram_gb_per_gpu": 24,
        "compute_multiplier": 0.0575,
        "supported_gpus": [
          "5090",
          "a10",
          "a100_sxm",
          "l40s",
          "l4",
          "a40",
          "h100_nvl",
          "3090",
          "l40",
          "a100_40gb_sxm",
          "a5000",
          "a100_40gb",
          "pro_6000",
          "h20",
          "a6000",
          "h800",
          "4090",
          "a100",
          "a6000_ada"
        ]
      },
      "invocation_count": 985462,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.01,
            "tao": 2.9452365658188216e-05
          },
          "output": {
            "usd": 0.01,
            "tao": 2.9452365658188216e-05
          }
        },
        "usd": {
          "hour": 0.23,
          "second": 6.38888888888889e-05
        },
        "tao": {
          "hour": 0.000677404410138329,
          "second": 1.8816789170509137e-07
        }
      },
      "instances": [
        {
          "instance_id": "ae499078-9441-4795-8e61-9cec1bfee15b",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:04:51.682834Z"
        },
        {
          "instance_id": "da69cea5-d381-4560-bef6-fe3acb91c3cb",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:04:51.682834Z"
        },
        {
          "instance_id": "7fe4a3b0-a16d-42a3-b95b-852d0452b350",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:04:51.682834Z"
        },
        {
          "instance_id": "b6e8f6aa-b695-4140-8b95-31a4f2fa09a3",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:04:51.682834Z"
        },
        {
          "instance_id": "a292356b-92a5-497c-8caf-4040fb6e6d6e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:04:51.682834Z"
        }
      ],
      "logo_id": "58aea633-e5f9-4f35-adc9-be367865e377",
      "openrouter": false,
      "chutes_version": "0.3.23.rc1",
      "revision": "006f5dcd1393c3add266de40994ba96225e9689d",
      "concurrency": 48,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/58aea633-e5f9-4f35-adc9-be367865e377.webp",
      "hot": true
    },
    {
      "chute_id": "b5326e54-8d9e-590e-bed4-f3db35d9d4cd",
      "name": "zai-org/GLM-4.5-FP8",
      "tagline": "GLM-4.5 is a 355B-parameter Mixture-of-Experts model (with 32B active parameters) designed for reasoning, coding, and agent tasks that features both \"thinking\" and \"direct response\" modes for handling different types of queries.",
      "readme": "\n# GLM-4.5-FP8\n\n[\ud83d\udcda Paper](https://huggingface.co/papers/2508.06471) | [\ud83d\udcbb Code](https://github.com/zai-org/GLM-4.5) | [\ud83c\udf10 Project Page](https://z.ai/blog/glm-4.5)\n\n<div align=\"center\">\n<img src=https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/logo.svg width=\"15%\"/>\n</div>\n<p align=\"center\">\n    \ud83d\udc4b Join our <a href=\"https://discord.gg/QR7SARHRxK\" target=\"_blank\">Discord</a> community.\n    <br>\n    \ud83d\udcd6 Check out the GLM-4.5 <a href=\"https://z.ai/blog/glm-4.5\" target=\"_blank\">technical blog</a>.\n    <br>\n    \ud83d\udccd Use GLM-4.5 API services on <a href=\"https://docs.z.ai/guides/llm/glm-4.5\">Z.ai API Platform (Global)</a> or <br> <a href=\"https://docs.bigmodel.cn/cn/guide/models/text/glm-4.5\">Zhipu AI Open Platform (Mainland China)</a>.\n    <br>\n    \ud83d\udc49 One click to <a href=\"https://chat.z.ai\">GLM-4.5</a>.\n</p>\n  \n## Paper Abstract\n\nWe present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at this https URL .\n\n## Model Introduction\n\nThe **GLM-4.5** series models are foundation models designed for intelligent agents. GLM-4.5 has **355** billion total parameters with **32** billion active parameters, while GLM-4.5-Air adopts a more compact design with **106** billion total parameters and **12** billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.\n\nBoth GLM-4.5 and GLM-4.5-Air are hybrid reasoning models that provide two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses.\n\nWe have open-sourced the base models, hybrid reasoning models, and FP8 versions of the hybrid reasoning models for both GLM-4.5 and GLM-4.5-Air. They are released under the MIT open-source license and can be used commercially and for secondary development.\n\nAs demonstrated in our comprehensive evaluation across 12 industry-standard benchmarks, GLM-4.5 achieves exceptional performance with a score of **63.2**, in the **3rd** place among all the proprietary and open-source models. Notably, GLM-4.5-Air delivers competitive results at **59.8** while maintaining superior efficiency.\n\n![bench](https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/bench.png)\n\nFor more eval results, show cases, and technical details, please visit our [technical blog](https://z.ai/blog/glm-4.5) or refer to the [technical report (paper)](https://huggingface.co/papers/2508.06471).\n\nThe model code, tool parser and reasoning parser can be found in the implementation of [transformers](https://github.com/huggingface/transformers/tree/main/src/transformers/models/glm4_moe), [vLLM](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/glm4_moe_mtp.py) and [SGLang](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/models/glm4_moe.py).\n\n## Model Downloads\n\nYou can directly experience the model on [Hugging Face](https://huggingface.co/spaces/zai-org/GLM-4.5-Space)\nor [ModelScope](https://modelscope.cn/studios/ZhipuAI/GLM-4.5-Demo) or download the model by following the links below.\n\n| Model            | Download Links                                                                                                                                | Model Size | Precision |\n|------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|------------|-----------|\n| GLM-4.5          | [\ud83e\udd17 Hugging Face](https://huggingface.co/zai-org/GLM-4.5)<br> [\ud83e\udd16 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5)                   | 355B-A32B  | BF16      |\n| GLM-4.5-Air      | [\ud83e\udd17 Hugging Face](https://huggingface.co/zai-org/GLM-4.5-Air)<br> [\ud83e\udd16 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5-Air)           | 106B-A12B  | BF16      |\n| GLM-4.5-FP8      | [\ud83e\udd17 Hugging Face](https://huggingface.co/zai-org/GLM-4.5-FP8)<br> [\ud83e\udd16 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5-FP8)           | 355B-A32B  | FP8       |\n| GLM-4.5-Air-FP8  | [\ud83e\udd17 Hugging Face](https://huggingface.co/zai-org/GLM-4.5-Air-FP8)<br> [\ud83e\udd16 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5-Air-FP8)   | 106B-A12B  | FP8       |\n| GLM-4.5-Base     | [\ud83e\udd17 Hugging Face](https://huggingface.co/zai-org/GLM-4.5-Base)<br> [\ud83e\udd16 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5-Base)         | 355B-A32B  | BF16      |\n| GLM-4.5-Air-Base | [\ud83e\udd17 Hugging Face](https://huggingface.co/zai-org/GLM-4.5-Air-Base)<br> [\ud83e\udd16 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5-Air-Base) | 106B-A12B  | BF16      |\n\n## System Requirements\n\n### Inference\n\nWe provide minimum and recommended configurations for \"full-featured\" model inference. The data in the table below is\nbased on the following conditions:\n\n1. All models use MTP layers and specify\n   `--speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4` to ensure competitive\n   inference speed.\n2. The `cpu-offload` parameter is not used.\n3. Inference batch size does not exceed `8`.\n4. All are executed on devices that natively support FP8 inference, ensuring both weights and cache are in FP8 format.\n5. Server memory must exceed `1T` to ensure normal model loading and operation.\n\nThe models can run under the configurations in the table below:\n\n| Model       | Precision | GPU Type and Count   | Test Framework |\n|-------------|-----------|----------------------|----------------|\n| GLM-4.5     | BF16      | H100 x 16 / H200 x 8 | sglang         |\n| GLM-4.5     | FP8       | H100 x 8 / H200 x 4  | sglang         |\n| GLM-4.5-Air | BF16      | H100 x 4 / H200 x 2  | sglang         |\n| GLM-4.5-Air | FP8       | H100 x 2 / H200 x 1  | sglang         |\n\nUnder the configurations in the table below, the models can utilize their full 128K context length:\n\n| Model       | Precision | GPU Type and Count    | Test Framework |\n|-------------|-----------|-----------------------|----------------|\n| GLM-4.5     | BF16      | H100 x 32 / H200 x 16 | sglang         |\n| GLM-4.5     | FP8       | H100 x 16 / H200 x 8  | sglang         |\n| GLM-4.5-Air | BF16      | H100 x 8 / H200 x 4   | sglang         |\n| GLM-4.5-Air | FP8       | H100 x 4 / H200 x 2   | sglang         |\\\n  \n### Fine-tuning\n\nThe code can run under the configurations in the table below\nusing [Llama Factory](https://github.com/hiyouga/LLaMA-Factory):\n\n| Model       | GPU Type and Count | Strategy | Batch Size (per GPU) |\n|-------------|--------------------|----------|----------------------|\n| GLM-4.5     | H100 x 16          | Lora     | 1                    |\n| GLM-4.5-Air | H100 x 4           | Lora     | 1                    |\n\nThe code can run under the configurations in the table below using [Swift](https://github.com/modelscope/ms-swift):\n\n| Model       | GPU Type and Count | Strategy | Batch Size (per GPU) |\n|-------------|--------------------|----------|----------------------|\n| GLM-4.5     | H20 (96GiB) x 16   | Lora     | 1                    |\n| GLM-4.5-Air | H20 (96GiB) x 4    | Lora     | 1                    |\n| GLM-4.5     | H20 (96GiB) x 128  | SFT      | 1                    |\n| GLM-4.5-Air | H20 (96GiB) x 32   | SFT      | 1                    |\n| GLM-4.5     | H20 (96GiB) x 128  | RL       | 1                    |\n| GLM-4.5-Air | H20 (96GiB) x 32   | RL       | 1                    |\n\n## Quick Start\n\nFor more comprehensive details and setup instructions, please refer to our [GitHub page](https://github.com/zai-org/GLM-4.5).\n\n### Transformers Inference\n\nHere is a basic example to run inference with the `transformers` library, demonstrating both thinking and non-thinking modes:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Load model and tokenizer\nmodel_id = \"zai-org/GLM-4.5-FP8\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16, # Adjust as needed (e.g., torch.float8 for FP8 models)\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\nmodel.eval()\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n]\n\n# Example for non-thinking mode (direct response)\n# The `add_nothink_token=True` parameter triggers non-thinking mode.\n# This mode is suitable for straightforward questions not requiring complex reasoning or tool usage.\ninputs_nothink_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False, add_nothink_token=True)\ninput_ids_nothink = tokenizer(inputs_nothink_text, return_tensors=\"pt\").input_ids.to(model.device)\noutputs_nothink = model.generate(input_ids_nothink, max_new_tokens=100)\nprint(\"Non-thinking mode response:\", tokenizer.decode(outputs_nothink[0][len(input_ids_nothink[0]):], skip_special_tokens=True))\n\n# Example for thinking mode (for complex reasoning or tool usage)\n# By default, `add_nothink_token=False` or omitting it triggers thinking mode.\n# This mode allows the model to perform multi-step reasoning, break down tasks, and utilize tools.\ninputs_think_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False, add_nothink_token=False)\ninput_ids_think = tokenizer(inputs_think_text, return_tensors=\"pt\").input_ids.to(model.device)\noutputs_think = model.generate(input_ids_think, max_new_tokens=100)\nprint(\"Thinking mode response:\", tokenizer.decode(outputs_think[0][len(input_ids_think[0]):], skip_special_tokens=True))\n```\n\n### vLLM\n\n+ Both BF16 and FP8 can be started with the following code:\n\n```shell\nvllm serve zai-org/GLM-4.5-Air \\\n    --tensor-parallel-size 8 \\\n    --tool-call-parser glm45 \\\n    --reasoning-parser glm45 \\\n    --enable-auto-tool-choice \\\n    --served-model-name glm-4.5-air\n```\n\nIf you're using 8x H100 GPUs and encounter insufficient memory when running the GLM-4.5 model, you'll need\n`--cpu-offload-gb 16` (only applicable to vLLM).\n\nIf you encounter `flash infer` issues, use `VLLM_ATTENTION_BACKEND=XFORMERS` as a temporary replacement. You can also\nspecify `TORCH_CUDA_ARCH_LIST='9.0+PTX'` to use `flash infer` (different GPUs have different TORCH_CUDA_ARCH_LIST\nvalues, please check accordingly).\n\n### SGLang\n\n+ BF16\n\n```shell\npython3 -m sglang.launch_server \\\n  --model-path zai-org/GLM-4.5-Air \\\n  --tp-size 8 \\\n  --tool-call-parser glm45  \\\n  --reasoning-parser glm45 \\\n  --speculative-algorithm EAGLE \\\n  --speculative-num-steps 3 \\\n  --speculative-eagle-topk 1 \\\n  --speculative-num-draft-tokens 4 \\\n  --mem-fraction-static 0.7 \\\n  --served-model-name glm-4.5-air \\\n  --host 0.0.0.0 \\\n  --port 8000\n```\n\n+ FP8\n\n```shell\npython3 -m sglang.launch_server \\\n  --model-path zai-org/GLM-4.5-Air-FP8 \\\n  --tp-size 4 \\\n  --tool-call-parser glm45  \\\n  --reasoning-parser glm45  \\\n  --speculative-algorithm EAGLE \\\n  --speculative-num-steps 3  \\\n  --speculative-eagle-topk 1  \\\n  --speculative-num-draft-tokens 4 \\\n  --mem-fraction-static 0.7 \\\n  --disable-shared-experts-fusion \\\n  --served-model-name glm-4.5-air-fp8 \\\n  --host 0.0.0.0 \\\n  --port 8000\n```\n\n### Request Parameter Instructions\n\n+ When using `vLLM` and `SGLang`, thinking mode is enabled by default when sending requests. If you want to disable the\n  thinking switch, you need to add the `extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}` parameter.\n+ Both support tool calling. Please use OpenAI-style tool description format for calls.\n+ For specific code, please refer to `api_request.py` in the `inference` folder.\n\n## Citation\nIf you find our work useful or helpful for your R&D works, please feel free to cite our paper as below.\n\n```bibtex\n@article{zhu2025glm45,\n  title={GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models},\n  author={Zhu, Xiaohan and Sun, Tianxiang and Wang, Hao and Xu, Yi and Zhang, Yichen and Wang, Junyi and Huang, Junjie and Zeng, Jiao and Huang, Yangyang and Gu, Ruipeng and Zhang, Xiaodong and Du, Mengying and Han, Hao and Li, Chao and Xiao, Jin and Guo, Weidong and Li, Zhen and Lu, Jingkang and Chen, Shu and Chen, Huadong and Chen, Peng and Liu, Hongguang and Guo, Guang and Liu, Wen and Yang, Tianyu and Hu, Bo and Zhang, Wenmin and Sun, Maosong},\n  journal={arXiv preprint arXiv:2508.06471},\n  year={2025}\n}\n```",
      "public": true,
      "version": "b60b96ec-7106-537c-88ee-87a666cdce8c",
      "tool_description": null,
      "slug": "chutes-zai-org-glm-4-5-fp8",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-07-28T14:34:40.498957Z",
      "updated_at": "2025-09-06T11:23:46.230498Z",
      "image": {
        "image_id": "468c185f-2e19-56d6-9508-6be534e42145",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "nightly-2025090600",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-09-06T09:40:20.310953Z",
        "build_started_at": "2025-09-06T09:40:22.578714Z",
        "build_completed_at": "2025-09-06T09:56:36.126262Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "b200",
        "h200"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "h200",
          "b200"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 5.4,
        "supported_gpus": [
          "b200",
          "h200"
        ]
      },
      "invocation_count": 817154,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.32986602,
            "tao": 0.0009715334639251226
          },
          "output": {
            "usd": 1.3201056,
            "tao": 0.003888023283862195
          }
        },
        "usd": {
          "hour": 35.64,
          "second": 0.0099
        },
        "tao": {
          "hour": 0.1049682312057828,
          "second": 2.9157842001606336e-05
        }
      },
      "instances": [
        {
          "instance_id": "428cbde0-b863-4bb7-93b7-501d19d24fea",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:09:36.774923Z"
        },
        {
          "instance_id": "210c7a01-1c88-4551-827b-97b84d2dfe77",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:09:42.132762Z"
        },
        {
          "instance_id": "af0954af-1f43-4a3c-8968-5ac66a5c3350",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "de9095f5-1b6c-410d-8e83-a31505ecbb66",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "5aacfd91-7b16-454f-b34d-9ad306559bc4",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "b24881e4-5ec5-4a57-8c2b-30ff254ab796",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "1364fb7a-a66f-4bf5-a31c-29f8bffef50e",
          "region": "n/a",
          "active": false,
          "verified": true,
          "last_verified_at": "2025-09-08T10:27:15.266379Z"
        },
        {
          "instance_id": "1129155f-3f0e-4de0-b439-92d2f3e64507",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:09:50.877319Z"
        },
        {
          "instance_id": "cdc69c59-6ae1-45f8-84b9-b64f5810685d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "9e240991-ed97-48f7-9a32-1453cc89f667",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:09:42.973287Z"
        },
        {
          "instance_id": "33b3176a-dc0d-4bb2-ab77-30201d587a71",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:09:37.564328Z"
        },
        {
          "instance_id": "b5608ac0-69b1-4c2c-82d5-4aacb92cf928",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:09:53.427303Z"
        },
        {
          "instance_id": "a2a814f1-cfe5-4d9e-a66e-b90fb7fdd231",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:09:56.361562Z"
        },
        {
          "instance_id": "ef5fcc46-675b-4620-90fe-46e7ad233dcb",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:09:49.791977Z"
        },
        {
          "instance_id": "58a39200-e7c3-42a7-9b85-5fcdeab169e3",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:10:27.347866Z"
        },
        {
          "instance_id": "5071670d-82d0-49fb-b88f-786f71a9befe",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "d68d6872-fb86-4da4-baa9-e8b6349896a6",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "446c9d5d-6869-4ffe-81cb-de7450eab655",
          "region": "n/a",
          "active": false,
          "verified": true,
          "last_verified_at": "2025-09-08T10:00:58.980693Z"
        },
        {
          "instance_id": "7d13889b-ac5d-4032-ae41-88842dd6a6d7",
          "region": "n/a",
          "active": false,
          "verified": true,
          "last_verified_at": "2025-09-08T10:09:12.114618Z"
        },
        {
          "instance_id": "6983b7fe-4130-4e45-9afd-3a2ca698e229",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:09:04.272684Z"
        },
        {
          "instance_id": "66ac9d5d-f98a-41c6-9021-ae3a3d63261d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:26:15.285201Z"
        },
        {
          "instance_id": "eb4c025d-0c98-43d8-b256-41b7b86495db",
          "region": "n/a",
          "active": false,
          "verified": true,
          "last_verified_at": "2025-09-08T10:09:37.006767Z"
        },
        {
          "instance_id": "426f502e-c9d1-47c0-a340-8d3e3be19def",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "1761a512-2993-4fb9-866e-b9b3ed6de810",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:55:21.108092Z"
        },
        {
          "instance_id": "f92cd735-2254-4c2d-b4ac-dae74b03acca",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "80341f04-7dc1-4c40-8722-4c7f012008d1",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "cdd7c3cf-de99-4618-9662-f3d78bd275fd",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:10:29.555307Z"
        },
        {
          "instance_id": "d6eb5499-8ad7-431a-948e-231e15421b54",
          "region": "n/a",
          "active": false,
          "verified": true,
          "last_verified_at": "2025-09-08T09:57:13.308628Z"
        },
        {
          "instance_id": "71f08eca-0cc9-445e-9f94-351ef7c7bbda",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:31:47.531622Z"
        },
        {
          "instance_id": "a9394f54-f93f-452e-98d3-f9fabfff8477",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "2f7e9954-4d07-4e6e-a1d0-fcf0544dc480",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "22e68b2f-10f4-4e35-8c33-8bef7ce1f681",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "fe1ff599-095a-47e9-98dd-4bb0667aad26",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "0456e023-2d92-4d2e-b672-7e2d2ac733bc",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "df3827d3-b74c-4d18-9ff0-27b367635aa9",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "2b9fed6a-7a92-47bf-85f6-b17221dd8e4a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "576f9364-522c-414c-8591-51e555cfa02a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "a7003d41-10a2-408f-b912-f18f824a1057",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "0dd9cde1-d85b-4047-8687-f73021358b80",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "7f2fbf3a-7832-4362-8748-059d07ef17c2",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "c6e2cc4d-23ac-48a5-bfb5-8efa48faf5ff",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "57845a09-432d-459a-9cc8-21ae49ad9205",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "c46ace16-44ab-4a9c-abea-28b0ab83dfa2",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "695e1552-2b2d-4b75-80ac-b08db21ca4ee",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "dc011dd3-c72c-46d8-9a79-e72f4b6008d1",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "cd3af607-e628-4b98-b8ab-35abc1302dd0",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "621133d5-d706-4931-b22f-540b6a7e9e4d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        },
        {
          "instance_id": "891179b0-8923-44f2-a103-77b94865cc31",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:07:13.623568Z"
        }
      ],
      "logo_id": "d9af1158-2d0f-43ec-b2ff-a92088526aac",
      "openrouter": false,
      "chutes_version": "0.3.23.rc1",
      "revision": "a0f46d4df319ef3877e248e617035d4317729846",
      "concurrency": 32,
      "boost": 3.0,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/d9af1158-2d0f-43ec-b2ff-a92088526aac.webp",
      "hot": true
    },
    {
      "chute_id": "14a91d88-d6d6-5046-aaf4-eb3ad96b7247",
      "name": "deepseek-ai/DeepSeek-R1-0528",
      "tagline": "DeepSeek-R1-0528 is a large language model optimized for complex reasoning and mathematical problem-solving, demonstrating strong performance across benchmarks like AIME and programming tasks, though its exact parameter count is not specified in the model card.",
      "readme": "---\r\nlicense: mit\r\nlibrary_name: transformers\r\n---\r\n# DeepSeek-R1-0528\r\n<!-- markdownlint-disable first-line-h1 -->\r\n<!-- markdownlint-disable html -->\r\n<!-- markdownlint-disable no-duplicate-header -->\r\n\r\n<div align=\"center\">\r\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\r\n</div>\r\n<hr>\r\n<div align=\"center\" style=\"line-height: 1;\">\r\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n</div>\r\n\r\n<div align=\"center\" style=\"line-height: 1;\">\r\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n</div>\r\n\r\n<div align=\"center\" style=\"line-height: 1;\">\r\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\r\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n</div>\r\n \r\n\r\n<p align=\"center\">\r\n  <a href=\"https://arxiv.org/pdf/2501.12948\"><b>Paper Link</b>\ud83d\udc41\ufe0f</a>\r\n</p>\r\n\r\n\r\n## 1. Introduction\r\n\r\nThe DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\r\n\r\n<p align=\"center\">\r\n  <img width=\"80%\" src=\"figures/benchmark.png\">\r\n</p>\r\n\r\nCompared to the previous version, the upgraded model shows significant improvements in handling complex reasoning tasks. For instance, in the AIME 2025 test, the model\u2019s accuracy has increased from 70% in the previous version to 87.5% in the current version. This advancement stems from enhanced thinking depth during the reasoning process: in the AIME test set, the previous model used an average of 12K tokens per question, whereas the new version averages 23K tokens per question.\r\n\r\nBeyond its improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding.\r\n\r\n## 2. Evaluation Results\r\n\r\n### DeepSeek-R1-0528\r\n For all our models, the maximum generation length is set to 64K tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 16 responses per query to estimate pass@1.\r\n<div align=\"center\">\r\n\r\n| Category | Benchmark (Metric)               | DeepSeek R1     | DeepSeek R1 0528\r\n|----------|----------------------------------|-----------------|---|\r\n| General  |\r\n|          | MMLU-Redux (EM)                   | 92.9            | 93.4\r\n|          | MMLU-Pro (EM)                     | 84.0            | 85.0\r\n|          | GPQA-Diamond (Pass@1)             | 71.5            | 81.0\r\n|          | SimpleQA (Correct)                | 30.1            | 27.8\r\n|          | FRAMES (Acc.)                     | 82.5            | 83.0\r\n|          | Humanity's Last Exam (Pass@1)                     | 8.5            | 17.7\r\n| Code |\r\n|          | LiveCodeBench (2408-2505) (Pass@1)        | 63.5          | 73.3\r\n|          | Codeforces-Div1 (Rating)          | 1530            | 1930\r\n|          | SWE Verified (Resolved)           | 49.2            | 57.6\r\n|          | Aider-Polyglot (Acc.)             | 53.3            | 71.6\r\n| Math |\r\n|          | AIME 2024 (Pass@1)                | 79.8            | 91.4\r\n|          | AIME 2025 (Pass@1)                     | 70.0           | 87.5\r\n|          | HMMT 2025 (Pass@1)            | 41.7 | 79.4 |\r\n|          | CNMO 2024 (Pass@1)                | 78.8            | 86.9\r\n| Tools |\r\n|          | BFCL_v3_MultiTurn (Acc)     | -            | 37.0 |\r\n|          | Tau-Bench   (Pass@1)       | -            | 53.5(Airline)/63.9(Retail)\r\n\r\n</div>\r\nNote: We use Agentless framework to evaluate model performance on SWE-Verified. We only evaluate text-only prompts in HLE testsets.  GPT-4.1 is employed to act user role in Tau-bench evaluation.\r\n\r\n### DeepSeek-R1-0528-Qwen3-8B\r\nMeanwhile, we distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B. This model achieves state-of-the-art (SOTA) performance among open-source models on the AIME 2024, surpassing Qwen3 8B by +10.0% and matching the performance of Qwen3-235B-thinking. We believe that the chain-of-thought from DeepSeek-R1-0528 will hold significant importance for both academic research on reasoning models and industrial development focused on small-scale models.\r\n\r\n|                                | AIME 24 | AIME 25 | HMMT Feb 25 | GPQA Diamond | LiveCodeBench (2408-2505) |\r\n|--------------------------------|---------|---------|-------------|--------------|---------------------------|\r\n| Qwen3-235B-A22B\t                | 85.7    | 81.5    | 62.5        | 71.1         | 66.5                  |\r\n| Qwen3-32B                      | 81.4    | 72.9    | -           | 68.4         | -                         |\r\n| Qwen3-8B                      | 76.0   | 67.3    | -           | 62.0       | -                         |\r\n| Phi-4-Reasoning-Plus-14B       | 81.3    | 78.0    | 53.6        | 69.3         | -          |\r\n| Gemini-2.5-Flash-Thinking-0520 | 82.3    | 72.0    | 64.2        | 82.8         | 62.3                  |\r\n| o3-mini (medium)               | 79.6    | 76.7    | 53.3        | 76.8         | 65.9                     |\r\n| DeepSeek-R1-0528-Qwen3-8B      | 86.0   | 76.3    | 61.5        | 61.1         | 60.5                      |\r\n\r\n## 3. Chat Website & API Platform\r\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in), and switch on the button \"DeepThink\"\r\n\r\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\r\n\r\n## 4. How to Run Locally\r\n\r\nPlease visit [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1) repository for more information about running DeepSeek-R1-0528 locally.\r\n\r\nCompared to previous versions of DeepSeek-R1, the usage recommendations for DeepSeek-R1-0528 have the following changes:\r\n\r\n1. System prompt is supported now.\r\n2. It is not required to add \"\\<think\\>\\n\" at the beginning of the output to force the model into thinking pattern.\r\n\r\nThe model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528. This model can be run in the same manner as Qwen3-8B.\r\n\r\n### System Prompt\r\nIn the official DeepSeek web/app, we use the same system prompt with a specific date.\r\n```\r\n\u8be5\u52a9\u624b\u4e3aDeepSeek-R1\uff0c\u7531\u6df1\u5ea6\u6c42\u7d22\u516c\u53f8\u521b\u9020\u3002\r\n\u4eca\u5929\u662f{current date}\u3002\r\n```\r\nFor example,\r\n```\r\n\u8be5\u52a9\u624b\u4e3aDeepSeek-R1\uff0c\u7531\u6df1\u5ea6\u6c42\u7d22\u516c\u53f8\u521b\u9020\u3002\r\n\u4eca\u5929\u662f2025\u5e745\u670828\u65e5\uff0c\u661f\u671f\u4e00\u3002\r\n```\r\n### Temperature\r\nIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.6. \r\n### Prompts for File Uploading and Web Search\r\nFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.\r\n```\r\nfile_template = \\\r\n\"\"\"[file name]: {file_name}\r\n[file content begin]\r\n{file_content}\r\n[file content end]\r\n{question}\"\"\"\r\n```\r\nFor Web Search, {search_results}, {cur_date}, and {question} are arguments.\r\nFor Chinese query, we use the prompt:\r\n```\r\nsearch_answer_zh_template = \\\r\n'''# \u4ee5\u4e0b\u5185\u5bb9\u662f\u57fa\u4e8e\u7528\u6237\u53d1\u9001\u7684\u6d88\u606f\u7684\u641c\u7d22\u7ed3\u679c:\r\n{search_results}\r\n\u5728\u6211\u7ed9\u4f60\u7684\u641c\u7d22\u7ed3\u679c\u4e2d\uff0c\u6bcf\u4e2a\u7ed3\u679c\u90fd\u662f[webpage X begin]...[webpage X end]\u683c\u5f0f\u7684\uff0cX\u4ee3\u8868\u6bcf\u7bc7\u6587\u7ae0\u7684\u6570\u5b57\u7d22\u5f15\u3002\u8bf7\u5728\u9002\u5f53\u7684\u60c5\u51b5\u4e0b\u5728\u53e5\u5b50\u672b\u5c3e\u5f15\u7528\u4e0a\u4e0b\u6587\u3002\u8bf7\u6309\u7167\u5f15\u7528\u7f16\u53f7[citation:X]\u7684\u683c\u5f0f\u5728\u7b54\u6848\u4e2d\u5bf9\u5e94\u90e8\u5206\u5f15\u7528\u4e0a\u4e0b\u6587\u3002\u5982\u679c\u4e00\u53e5\u8bdd\u6e90\u81ea\u591a\u4e2a\u4e0a\u4e0b\u6587\uff0c\u8bf7\u5217\u51fa\u6240\u6709\u76f8\u5173\u7684\u5f15\u7528\u7f16\u53f7\uff0c\u4f8b\u5982[citation:3][citation:5]\uff0c\u5207\u8bb0\u4e0d\u8981\u5c06\u5f15\u7528\u96c6\u4e2d\u5728\u6700\u540e\u8fd4\u56de\u5f15\u7528\u7f16\u53f7\uff0c\u800c\u662f\u5728\u7b54\u6848\u5bf9\u5e94\u90e8\u5206\u5217\u51fa\u3002\r\n\u5728\u56de\u7b54\u65f6\uff0c\u8bf7\u6ce8\u610f\u4ee5\u4e0b\u51e0\u70b9\uff1a\r\n- \u4eca\u5929\u662f{cur_date}\u3002\r\n- \u5e76\u975e\u641c\u7d22\u7ed3\u679c\u7684\u6240\u6709\u5185\u5bb9\u90fd\u4e0e\u7528\u6237\u7684\u95ee\u9898\u5bc6\u5207\u76f8\u5173\uff0c\u4f60\u9700\u8981\u7ed3\u5408\u95ee\u9898\uff0c\u5bf9\u641c\u7d22\u7ed3\u679c\u8fdb\u884c\u7504\u522b\u3001\u7b5b\u9009\u3002\r\n- \u5bf9\u4e8e\u5217\u4e3e\u7c7b\u7684\u95ee\u9898\uff08\u5982\u5217\u4e3e\u6240\u6709\u822a\u73ed\u4fe1\u606f\uff09\uff0c\u5c3d\u91cf\u5c06\u7b54\u6848\u63a7\u5236\u572810\u4e2a\u8981\u70b9\u4ee5\u5185\uff0c\u5e76\u544a\u8bc9\u7528\u6237\u53ef\u4ee5\u67e5\u770b\u641c\u7d22\u6765\u6e90\u3001\u83b7\u5f97\u5b8c\u6574\u4fe1\u606f\u3002\u4f18\u5148\u63d0\u4f9b\u4fe1\u606f\u5b8c\u6574\u3001\u6700\u76f8\u5173\u7684\u5217\u4e3e\u9879\uff1b\u5982\u975e\u5fc5\u8981\uff0c\u4e0d\u8981\u4e3b\u52a8\u544a\u8bc9\u7528\u6237\u641c\u7d22\u7ed3\u679c\u672a\u63d0\u4f9b\u7684\u5185\u5bb9\u3002\r\n- \u5bf9\u4e8e\u521b\u4f5c\u7c7b\u7684\u95ee\u9898\uff08\u5982\u5199\u8bba\u6587\uff09\uff0c\u8bf7\u52a1\u5fc5\u5728\u6b63\u6587\u7684\u6bb5\u843d\u4e2d\u5f15\u7528\u5bf9\u5e94\u7684\u53c2\u8003\u7f16\u53f7\uff0c\u4f8b\u5982[citation:3][citation:5]\uff0c\u4e0d\u80fd\u53ea\u5728\u6587\u7ae0\u672b\u5c3e\u5f15\u7528\u3002\u4f60\u9700\u8981\u89e3\u8bfb\u5e76\u6982\u62ec\u7528\u6237\u7684\u9898\u76ee\u8981\u6c42\uff0c\u9009\u62e9\u5408\u9002\u7684\u683c\u5f0f\uff0c\u5145\u5206\u5229\u7528\u641c\u7d22\u7ed3\u679c\u5e76\u62bd\u53d6\u91cd\u8981\u4fe1\u606f\uff0c\u751f\u6210\u7b26\u5408\u7528\u6237\u8981\u6c42\u3001\u6781\u5177\u601d\u60f3\u6df1\u5ea6\u3001\u5bcc\u6709\u521b\u9020\u529b\u4e0e\u4e13\u4e1a\u6027\u7684\u7b54\u6848\u3002\u4f60\u7684\u521b\u4f5c\u7bc7\u5e45\u9700\u8981\u5c3d\u53ef\u80fd\u5ef6\u957f\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u8981\u70b9\u7684\u8bba\u8ff0\u8981\u63a8\u6d4b\u7528\u6237\u7684\u610f\u56fe\uff0c\u7ed9\u51fa\u5c3d\u53ef\u80fd\u591a\u89d2\u5ea6\u7684\u56de\u7b54\u8981\u70b9\uff0c\u4e14\u52a1\u5fc5\u4fe1\u606f\u91cf\u5927\u3001\u8bba\u8ff0\u8be6\u5c3d\u3002\r\n- \u5982\u679c\u56de\u7b54\u5f88\u957f\uff0c\u8bf7\u5c3d\u91cf\u7ed3\u6784\u5316\u3001\u5206\u6bb5\u843d\u603b\u7ed3\u3002\u5982\u679c\u9700\u8981\u5206\u70b9\u4f5c\u7b54\uff0c\u5c3d\u91cf\u63a7\u5236\u57285\u4e2a\u70b9\u4ee5\u5185\uff0c\u5e76\u5408\u5e76\u76f8\u5173\u7684\u5185\u5bb9\u3002\r\n- \u5bf9\u4e8e\u5ba2\u89c2\u7c7b\u7684\u95ee\u7b54\uff0c\u5982\u679c\u95ee\u9898\u7684\u7b54\u6848\u975e\u5e38\u7b80\u77ed\uff0c\u53ef\u4ee5\u9002\u5f53\u8865\u5145\u4e00\u5230\u4e24\u53e5\u76f8\u5173\u4fe1\u606f\uff0c\u4ee5\u4e30\u5bcc\u5185\u5bb9\u3002\r\n- \u4f60\u9700\u8981\u6839\u636e\u7528\u6237\u8981\u6c42\u548c\u56de\u7b54\u5185\u5bb9\u9009\u62e9\u5408\u9002\u3001\u7f8e\u89c2\u7684\u56de\u7b54\u683c\u5f0f\uff0c\u786e\u4fdd\u53ef\u8bfb\u6027\u5f3a\u3002\r\n- \u4f60\u7684\u56de\u7b54\u5e94\u8be5\u7efc\u5408\u591a\u4e2a\u76f8\u5173\u7f51\u9875\u6765\u56de\u7b54\uff0c\u4e0d\u80fd\u91cd\u590d\u5f15\u7528\u4e00\u4e2a\u7f51\u9875\u3002\r\n- \u9664\u975e\u7528\u6237\u8981\u6c42\uff0c\u5426\u5219\u4f60\u56de\u7b54\u7684\u8bed\u8a00\u9700\u8981\u548c\u7528\u6237\u63d0\u95ee\u7684\u8bed\u8a00\u4fdd\u6301\u4e00\u81f4\u3002\r\n# \u7528\u6237\u6d88\u606f\u4e3a\uff1a\r\n{question}'''\r\n```\r\nFor English query, we use the prompt:\r\n```\r\nsearch_answer_en_template = \\\r\n'''# The following contents are the search results related to the user's message:\r\n{search_results}\r\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\r\nWhen responding, please keep the following points in mind:\r\n- Today is {cur_date}.\r\n- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.\r\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\r\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\r\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\r\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\r\n- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.\r\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\r\n- Unless the user requests otherwise, your response should be in the same language as the user's question.\r\n# The user's message is:\r\n{question}'''\r\n```\r\n\r\n## 5. License\r\nThis code repository is licensed under [MIT License](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528/raw/main/LICENSE). The use of DeepSeek-R1 models is also subject to [MIT License](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528/raw/main/LICENSE). DeepSeek-R1 series (including Base and Chat) supports commercial use and distillation.\r\n\r\n## 6. Citation\r\n```\r\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\r\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \r\n      author={DeepSeek-AI},\r\n      year={2025},\r\n      eprint={2501.12948},\r\n      archivePrefix={arXiv},\r\n      primaryClass={cs.CL},\r\n      url={https://arxiv.org/abs/2501.12948}, \r\n}\r\n```\r\n\r\n## 7. Contact\r\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528/raw/main/service@deepseek.com).\r\n",
      "public": true,
      "version": "62fc6bfb-f8c1-525d-9e61-24c1a3b002c1",
      "tool_description": null,
      "slug": "chutes-deepseek-ai-deepseek-r1-0528",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-05-28T18:56:26.606463Z",
      "updated_at": "2025-09-06T10:35:56.751963Z",
      "image": {
        "image_id": "468c185f-2e19-56d6-9508-6be534e42145",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "nightly-2025090600",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-09-06T09:40:20.310953Z",
        "build_started_at": "2025-09-06T09:40:22.578714Z",
        "build_completed_at": "2025-09-06T09:56:36.126262Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "h200"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "h200"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 5.4,
        "supported_gpus": [
          "h200"
        ]
      },
      "invocation_count": 788706,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.1999188,
            "tao": 0.0005888081599546199
          },
          "output": {
            "usd": 0.800064,
            "tao": 0.0023563777477952695
          }
        },
        "usd": {
          "hour": 21.6,
          "second": 0.006
        },
        "tao": {
          "hour": 0.06361710982168654,
          "second": 1.767141939491293e-05
        }
      },
      "instances": [
        {
          "instance_id": "6a3bc5ae-aac1-4a28-acc9-e88a934b593c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "f8153c91-6fdd-4c51-a204-c0444e37a265",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "e122aa06-d7c5-462a-824b-a871ece02864",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "7fcbea85-9e92-4b0d-abfc-f2f8f1b28848",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "c8d5bba4-b57d-4cea-8442-a313995d20bd",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "1672c449-8ff9-46ca-91c1-e995e9ad2017",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:11:22.671621Z"
        },
        {
          "instance_id": "f95dce5e-8a39-4fbb-bd9f-45b050ad0d46",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "a3c8ad7b-ddf8-4d3c-ace1-8667990dd684",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "9bcdc81b-b16e-4532-b377-28047878266a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "123e296d-1db4-4897-a298-bbaf7cac5ed7",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "53d8cbb2-15a5-42ed-8323-5c2b336b6266",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "493e493c-d1cc-46e3-9849-668042701fc9",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "6a8b5d05-96d9-4e86-a079-c9d292fd01cc",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "313390d0-c525-48ce-9bc3-21e20d0f4e81",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "ea535a73-858e-45b4-af72-23bd5f89349a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "3c99f140-dc5c-48ad-9016-914aad2ae765",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "d24553e6-1dea-49e0-9b9d-4529b60e34bd",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "688a442f-8813-4c0b-b290-e970f5019007",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "0618a90c-be34-4e2c-9a18-aef0b6a87219",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "09bc1323-f44c-4e2c-9231-fc320ea72faa",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "efecc97b-92c4-4d19-aa3c-320a113468f3",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "5a8850af-564a-4dde-b35d-fa616ee7f8ff",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "4be97ef0-23af-42cc-837c-0f9bc8d7f08b",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        },
        {
          "instance_id": "f6205c9f-5342-4a02-9482-c37832c6d7c2",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:22.576942Z"
        }
      ],
      "logo_id": "c81dd17a-2d82-4914-8be9-66b41891db44",
      "openrouter": true,
      "chutes_version": "0.3.23.rc1",
      "revision": "4236a6af538feda4548eca9ab308586007567f52",
      "concurrency": 40,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/c81dd17a-2d82-4914-8be9-66b41891db44.webp",
      "hot": true
    },
    {
      "chute_id": "986b874c-bf41-549f-b28f-4322f86fa4ba",
      "name": "unsloth/Mistral-Small-24B-Instruct-2501",
      "tagline": "Mistral Small 3 is a 24B parameter open source model that achieves near GPT-4 level performance on many tasks while being small enough to run on a single RTX 4090 or 32GB MacBook, making it ideal for local deployment of conversational agents and specialized fine-tuning.",
      "readme": "\n# Finetune LLMs 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Mistral (7B) here: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png\" width=\"200\"/>](https://discord.gg/unsloth)\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n\n\n## \u2728 Finetune for Free\n\nAll notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\n\n| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\n|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\n| **Llama-3.2 (3B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2.4x faster | 58% less |\n| **Llama-3.2 (11B vision)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 60% less |\n| **Qwen2 VL (7B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb)               | 1.8x faster | 60% less |\n| **Qwen2.5 (7B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)               | 2x faster | 60% less |\n| **Llama-3.1 (8B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2.4x faster | 58% less |\n| **Phi-3.5 (mini)** | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3.5_Mini-Conversational.ipynb)               | 2x faster | 50% less |\n| **Gemma 2 (9B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb)               | 2.4x faster | 58% less |\n| **Mistral (7B)**    | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 62% less |\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png\" width=\"200\"/>](https://docs.unsloth.ai)\n\n- This [Llama 3.2 conversational notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb) is useful for ShareGPT ChatML / Vicuna templates.\n- This [text completion notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.\n- \\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\n# Model Card for Mistral-Small-24B-Instruct-2501\n\nMistral Small 3 ( 2501 ) sets a new benchmark in the \"small\" Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models!  \nThis model is an instruction-fine-tuned version of the base model: [Mistral-Small-24B-Base-2501](https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501).\n\nMistral Small can be deployed locally and is exceptionally \"knowledge-dense\", fitting in a single RTX 4090 or a 32GB RAM MacBook once quantized.  \nPerfect for:\n- Fast response conversational agents.\n- Low latency function calling.\n- Subject matter experts via fine-tuning.\n- Local inference for hobbyists and organizations handling sensitive data.\n\nFor enterprises that need specialized capabilities (increased context, particular modalities, domain specific knowledge, etc.), we will be releasing commercial models beyond what Mistral AI contributes to the community.\n\nThis release demonstrates our commitment to open source, serving as a strong base model. \n\nLearn more about Mistral Small in our [blog post](https://mistral.ai/news/mistral-small-3/).\n\nModel developper: Mistral AI Team\n\n## Key Features\n- **Multilingual:** Supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish.\n- **Agent-Centric:** Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n- **Advanced Reasoning:** State-of-the-art conversational and reasoning capabilities.\n- **Apache 2.0 License:** Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window:** A 32k context window.\n- **System Prompt:** Maintains strong adherence and support for system prompts.\n- **Tokenizer:** Utilizes a Tekken tokenizer with a 131k vocabulary size.\n\n## Benchmark results\n\n\n### Human evaluated benchmarks\n\n| Category | Gemma-2-27B | Qwen-2.5-32B | Llama-3.3-70B | Gpt4o-mini |\n|----------|-------------|--------------|---------------|------------|\n| Mistral is better | 0.536 | 0.496 | 0.192 | 0.200 |\n| Mistral is slightly better | 0.196 | 0.184 | 0.164 | 0.204 |\n| Ties | 0.052 | 0.060 | 0.236 | 0.160 |\n| Other is slightly better | 0.060 | 0.088 | 0.112 | 0.124 |\n| Other is better | 0.156 | 0.172 | 0.296 | 0.312 |\n\n**Note**:\n\n- We conducted side by side evaluations with an external third-party vendor, on a set of over 1k proprietary coding and generalist prompts.\n- Evaluators were tasked with selecting their preferred model response from anonymized generations produced by Mistral Small 3 vs another model.\n- We are aware that in some cases the benchmarks on human judgement starkly differ from publicly available benchmarks, but have taken extra caution in verifying a fair evaluation. We are confident that the above benchmarks are valid.\n\n### Publicly accesible benchmarks\n\n**Reasoning & Knowledge**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| mmlu_pro_5shot_cot_instruct | 0.663 | 0.536 | 0.666 | 0.683 | 0.617 |\n| gpqa_main_cot_5shot_instruct | 0.453 | 0.344 | 0.531 | 0.404 | 0.377 |\n\n**Math & Coding**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| humaneval_instruct_pass@1 | 0.848 | 0.732 | 0.854 | 0.909 | 0.890 |\n| math_instruct | 0.706 | 0.535 | 0.743 | 0.819 | 0.761 |\n**Instruction following**\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| mtbench_dev | 8.35 | 7.86 | 7.96 | 8.26 | 8.33 |\n| wildbench | 52.27 | 48.21 | 50.04 | 52.73 | 56.13 |\n| arena_hard | 0.873 | 0.788 | 0.840 | 0.860 | 0.897 |\n| ifeval | 0.829 | 0.8065 | 0.8835 | 0.8401 | 0.8499 |\n**Note**:\n- Performance accuracy on all benchmarks were obtained through the same internal evaluation pipeline - as such, numbers may vary slightly from previously reported performance\n([Qwen2.5-32B-Instruct](https://qwenlm.github.io/blog/qwen2.5/), [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct), [Gemma-2-27B-IT](https://huggingface.co/google/gemma-2-27b-it)). \n- Judge based evals such as Wildbench, Arena hard and MTBench were based on gpt-4o-2024-05-13.\n### Basic Instruct Template (V7-Tekken)\n```\n<s>[SYSTEM_PROMPT]<system prompt>[/SYSTEM_PROMPT][INST]<user message>[/INST]<assistant response></s>[INST]<user message>[/INST]\n```\n*`<system_prompt>`, `<user message>` and `<assistant response>` are placeholders.*\n***Please make sure to use [mistral-common](https://github.com/mistralai/mistral-common) as the source of truth***\n## Usage\nThe model can be used with the following frameworks;\n- [`vllm`](https://github.com/vllm-project/vllm): See [here](https://huggingface.co/unsloth/Mistral-Small-24B-Instruct-2501/raw/main/#vLLM)\n- [`transformers`](https://github.com/huggingface/transformers): See [here](https://huggingface.co/unsloth/Mistral-Small-24B-Instruct-2501/raw/main/#Transformers)\n### vLLM\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n**Note 1**: We recommond using a relatively low temperature, such as `temperature=0.15`.\n**Note 2**: Make sure to add a system prompt to the model to best tailer it for your needs. If you want to use the model as a general assistant, we recommend the following \nsystem prompt:\n```\nsystem_prompt = \"\"\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\nYour knowledge base was last updated on 2023-10-01. The current date is 2025-01-30.\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\"What are some good restaurants around me?\\\" => \\\"Where are you?\\\" or \\\"When is the next flight to Tokyo\\\" => \\\"Where do you travel from?\\\")\"\"\"\n```\n**_Installation_**\nMake sure you install [`vLLM >= 0.6.4`](https://github.com/vllm-project/vllm/releases/tag/v0.6.4):\n```\npip install --upgrade vllm\n```\nAlso make sure you have [`mistral_common >= 1.5.2`](https://github.com/mistralai/mistral-common/releases/tag/v1.5.2) installed:\n```\npip install --upgrade mistral_common\n```\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39).\n#### Server\nWe recommand that you use Mistral-Small-24B-Instruct-2501 in a server/client setting. \n1. Spin up a server:\n```\nvllm serve mistralai/Mistral-Small-24B-Instruct-2501 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice\n```\n**Note:** Running Mistral-Small-24B-Instruct-2501 on GPU requires ~55 GB of GPU RAM in bf16 or fp16. \n2. To ping the client you can use a simple Python snippet.\n```py\nimport requests\nimport json\nfrom datetime import datetime, timedelta\n\nurl = \"http://<your-server>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n\nmodel = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Give me 5 non-formal ways to say 'See you later' in French.\"\n    },\n]\ndata = {\"model\": model, \"messages\": messages}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])\n\n# Sure, here are five non-formal ways to say \"See you later\" in French:\n#\n# 1. \u00c0 plus tard\n# 2. \u00c0 plus\n# 3. Salut\n# 4. \u00c0 toute\n# 5. Bisous\n#\n# ```\n#  /\\_/\\\n# ( o.o )\n#  > ^ <\n# ```\n```\n### Function calling\nMistral-Small-24-Instruct-2501 is excellent at function / tool calling tasks via vLLM. *E.g.:*\n<details>\n  <summary>Example</summary>\n```py\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\n\nurl = \"http://<your-url>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n\nmodel = \"mistralai/Mistral-Small-24B-Instruct-2501\"\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime(\"%Y-%m-%d\")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    model_name = repo_id.split(\"/\")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"city\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n                    },\n                    \"state\": {\n                        \"type\": \"string\",\n                        \"description\": \"The state abbreviation, e.g. 'CA' for California\",\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unit for temperature\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                    },\n                },\n                \"required\": [\"city\", \"state\", \"unit\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"rewrite\",\n            \"description\": \"Rewrite a given text for improved clarity\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"text\": {\n                        \"type\": \"string\",\n                        \"description\": \"The input text to rewrite\",\n                    }\n                },\n            },\n        },\n    },\n]\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\n        \"role\": \"user\",\n        \"content\": \"Could you please make the below article more concise?\\n\\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": [\n            {\n                \"id\": \"bbc5b7ede\",\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": \"rewrite\",\n                    \"arguments\": '{\"text\": \"OpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\"}',\n                },\n            }\n        ],\n    },\n    {\n        \"role\": \"tool\",\n        \"content\": '{\"action\":\"rewrite\",\"outcome\":\"OpenAI is a FOR-profit company.\"}',\n        \"tool_call_id\": \"bbc5b7ede\",\n        \"name\": \"rewrite\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"---\\n\\nOpenAI is a FOR-profit company.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Can you tell me what the temperature will be in Dallas, in Fahrenheit?\",\n    },\n]\ndata = {\"model\": model, \"messages\": messages, \"tools\": tools}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nimport ipdb; ipdb.set_trace()\nprint(response.json()[\"choices\"][0][\"message\"][\"tool_calls\"])\n# [{'id': '8PdihwL6d', 'type': 'function', 'function': {'name': 'get_current_weather', 'arguments': '{\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahre",
      "public": true,
      "version": "cdceb2d7-d3fb-5961-bc1f-cf33097707c0",
      "tool_description": null,
      "slug": "chutes-unsloth-mistral-small-24b-instruct-2501",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-03-22T12:45:43.823864Z",
      "updated_at": "2025-09-06T10:37:39.841745Z",
      "image": {
        "image_id": "468c185f-2e19-56d6-9508-6be534e42145",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "nightly-2025090600",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-09-06T09:40:20.310953Z",
        "build_started_at": "2025-09-06T09:40:22.578714Z",
        "build_completed_at": "2025-09-06T09:56:36.126262Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "a6000_ada",
        "h100_nvl",
        "a100",
        "h20",
        "a100_sxm",
        "l40s",
        "pro_6000",
        "h100_sxm",
        "h200",
        "a40",
        "h100",
        "h800"
      ],
      "node_selector": {
        "exclude": [
          "l40",
          "a6000",
          "b200",
          "mi300x"
        ],
        "include": null,
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 48,
        "compute_multiplier": 0.54,
        "supported_gpus": [
          "a6000_ada",
          "h100_nvl",
          "a100",
          "h20",
          "a100_sxm",
          "l40s",
          "pro_6000",
          "h100_sxm",
          "h200",
          "a40",
          "h100",
          "h800"
        ]
      },
      "invocation_count": 622419,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.01999188,
            "tao": 5.888081599546198e-05
          },
          "output": {
            "usd": 0.08000639999999999,
            "tao": 0.00023563777477952693
          }
        },
        "usd": {
          "hour": 2.16,
          "second": 0.0006000000000000001
        },
        "tao": {
          "hour": 0.006361710982168655,
          "second": 1.767141939491293e-06
        }
      },
      "instances": [
        {
          "instance_id": "be7b1b51-382d-4fa4-88b9-f7e4d853887e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:53:18.154674Z"
        },
        {
          "instance_id": "b3b2a79e-4c40-4f99-8108-9b5b068a5413",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:53:18.154674Z"
        },
        {
          "instance_id": "795ec81e-7711-4f35-83bf-8d4800c029f1",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:53:18.154674Z"
        },
        {
          "instance_id": "f4d4a67d-d940-4088-9760-5fc76f07f023",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:53:18.154674Z"
        },
        {
          "instance_id": "c874a9c0-9bcf-4cb2-83d8-5cb098c7cff8",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:53:18.154674Z"
        }
      ],
      "logo_id": "779cffb3-d9e8-456a-8483-07921c478cd4",
      "openrouter": true,
      "chutes_version": "0.3.23.rc1",
      "revision": "2eddef095b2d91c22c59cc3ede00ec595e530d16",
      "concurrency": 40,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/779cffb3-d9e8-456a-8483-07921c478cd4.webp",
      "hot": true
    },
    {
      "chute_id": "07cb1b3a-ec4d-594a-96c2-b547fddcadb0",
      "name": "deepseek-ai/DeepSeek-V3.1",
      "tagline": "DeepSeek-V3.1 is a 671B parameter hybrid language model (with 37B activated parameters) that supports both thinking and non-thinking modes, making it particularly effective for tasks requiring reasoning, tool usage, coding, and search capabilities within a 128K context window.",
      "readme": "# DeepSeek-V3.1\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n## Introduction\n\nDeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects:\n\n- **Hybrid thinking mode**: One model supports both thinking mode and non-thinking mode by changing the chat template. \n\n- **Smarter tool calling**: Through post-training optimization, the model's performance in tool usage and agent tasks has significantly improved.\n\n- **Higher thinking efficiency**: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.\n\nDeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.\n\nAdditionally, DeepSeek-V3.1 is trained using the **UE8M0 FP8 scale data format on both model weights and activations** to ensure compatibility with microscaling data formats. Please refer to [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM) for more details.\n\n## Model Downloads\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3.1-Base | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base) \\| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1-Base) |\n| DeepSeek-V3.1 | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1) \\| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1) |\n\n</div>\n\n## Chat Template\n\nThe details of our chat template is described in `tokenizer_config.json` and `assets/chat_template.jinja`. Here is a brief description.\n\n### Non-Thinking\n\n#### First-Turn\n\nPrefix:\n`<\uff5cbegin\u2581of\u2581sentence\uff5c>{system prompt}<\uff5cUser\uff5c>{query}<\uff5cAssistant\uff5c></think>`\n\nWith the given prefix, DeepSeek V3.1 generates responses to queries in non-thinking mode. Unlike DeepSeek V3,  it introduces an additional token `</think>`.\n\n#### Multi-Turn\nContext:\n`<\uff5cbegin\u2581of\u2581sentence\uff5c>{system prompt}<\uff5cUser\uff5c>{query}<\uff5cAssistant\uff5c></think>{response}<\uff5cend\u2581of\u2581sentence\uff5c>...<\uff5cUser\uff5c>{query}<\uff5cAssistant\uff5c></think>{response}<\uff5cend\u2581of\u2581sentence\uff5c>`\n\nPrefix:\n`<\uff5cUser\uff5c>{query}<\uff5cAssistant\uff5c></think>`\n\nBy concatenating the context and the prefix, we obtain the correct prompt for the query.\n\n### Thinking\n\n#### First-Turn\nPrefix:\n`<\uff5cbegin\u2581of\u2581sentence\uff5c>{system prompt}<\uff5cUser\uff5c>{query}<\uff5cAssistant\uff5c><think>`\n\nThe prefix of thinking mode is similar to DeepSeek-R1. \n\n\n#### Multi-Turn\nContext:\n`<\uff5cbegin\u2581of\u2581sentence\uff5c>{system prompt}<\uff5cUser\uff5c>{query}<\uff5cAssistant\uff5c></think>{response}<\uff5cend\u2581of\u2581sentence\uff5c>...<\uff5cUser\uff5c>{query}<\uff5cAssistant\uff5c></think>{response}<\uff5cend\u2581of\u2581sentence\uff5c>`\n\nPrefix:\n`<\uff5cUser\uff5c>{query}<\uff5cAssistant\uff5c><think>`\n\nThe multi-turn template is the same with non-thinking multi-turn chat template. It means the thinking token in the last turn will be dropped but the `</think>` is retained in every turn of context. \n\n### ToolCall\nToolcall is supported in non-thinking mode. The format is: \n\n`<\uff5cbegin\u2581of\u2581sentence\uff5c>{system prompt}\\n\\n{tool_description}<\uff5cUser\uff5c>{query}<\uff5cAssistant\uff5c></think>` where the tool_description is \n\n```\n## Tools\nYou have access to the following tools:\n\n### {tool_name1}\nDescription: {description}\n\nParameters: {json.dumps(parameters)}\n\nIMPORTANT: ALWAYS adhere to this exact format for tool use:\n<\uff5ctool\u2581calls\u2581begin\uff5c><\uff5ctool\u2581call\u2581begin\uff5c>tool_call_name<\uff5ctool\u2581sep\uff5c>tool_call_arguments<\uff5ctool\u2581call\u2581end\uff5c>{additional_tool_calls}<\uff5ctool\u2581calls\u2581end\uff5c>\n\nWhere:\n- `tool_call_name` must be an exact match to one of the available tools\n- `tool_call_arguments` must be valid JSON that strictly follows the tool's Parameters Schema\n- For multiple tool calls, chain them directly without separators or spaces\n```\n\n### Code-Agent\nWe support various code agent frameworks. Please refer to the above toolcall format to create your own code agents. An example is shown in `assets/code_agent_trajectory.html`.\n\n### Search-Agent\nWe design a specific format for searching toolcall in thinking mode, to support search agent. \n\nFor complex questions that require accessing external or up-to-date information, DeepSeek-V3.1 can leverage a user-provided search tool through a multi-turn tool-calling process.\n\nPlease refer to the `assets/search_tool_trajectory.html` and `assets/search_python_tool_trajectory.html` for the detailed template.\n\n## Evaluation\n| Category | Benchmark (Metric)              | DeepSeek V3.1-NonThinking | DeepSeek V3 0324 | DeepSeek V3.1-Thinking     | DeepSeek R1 0528\n|----------|----------------------------------|-----------------|---|---|---|\n| General  |\n|          | MMLU-Redux (EM)              | 91.8     | 90.5    | 93.7          | 93.4\n|          | MMLU-Pro (EM)                  | 83.7  | 81.2    | 84.8          | 85.0\n|          | GPQA-Diamond (Pass@1)           | 74.9   | 68.4   | 80.1            | 81.0\n|          | Humanity's Last Exam (Pass@1)   | -    |       -            | 15.9         | 17.7\n|Search Agent| \n|          | BrowseComp       | -      | -  | 30.0 | 8.9\n|          | BrowseComp_zh       | -     | -  | 49.2      | 35.7\n|          | Humanity's Last Exam (Python + Search)      |-   | -    | 29.8         | 24.8\n|          | SimpleQA             | -      | -    | 93.4  | 92.3\n| Code |\n|          | LiveCodeBench (2408-2505) (Pass@1)     | 56.4    | 43.0    | 74.8          | 73.3\n|          | Codeforces-Div1 (Rating)        | -   | -    | 2091            | 1930\n|          | Aider-Polyglot (Acc.)           | 68.4    | 55.1   | 76.3           | 71.6\n| Code Agent|\n|          | SWE Verified (Agent mode)           | 66.0       | 45.4  | -    | 44.6\n|          | SWE-bench Multilingual (Agent mode)         | 54.5    | 29.3   | -            | 30.5\n|          | Terminal-bench (Terminus 1 framework)       | 31.3     | 13.3      | -         | 5.7\n| Math |\n|          | AIME 2024 (Pass@1)                | 66.3     | 59.4     | 93.1      | 91.4\n|          | AIME 2025 (Pass@1)                     | 49.8  | 51.3 | 88.4          | 87.5\n|          | HMMT 2025 (Pass@1)        | 33.5    | 29.2   | 84.2 | 79.4 |\n\nNote: \n- Search agents are evaluated with our internal search framework, which uses a commercial search API + webpage filter + 128K context window. Seach agent results of R1-0528 are evaluated with a pre-defined workflow. \n\n- SWE-bench is evaluated with our internal code agent framework.\n\n- HLE is evaluated with the text-only subset.\n\n### Usage Example\n\n```python\nimport transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3.1\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n    {\"role\": \"assistant\", \"content\": \"<think>Hmm</think>I am DeepSeek\"},\n    {\"role\": \"user\", \"content\": \"1+1=?\"}\n]\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=True, add_generation_prompt=True)\n# '<\uff5cbegin\u2581of\u2581sentence\uff5c>You are a helpful assistant<\uff5cUser\uff5c>Who are you?<\uff5cAssistant\uff5c></think>I am DeepSeek<\uff5cend\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>1+1=?<\uff5cAssistant\uff5c><think>'\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=False, add_generation_prompt=True)\n# '<\uff5cbegin\u2581of\u2581sentence\uff5c>You are a helpful assistant<\uff5cUser\uff5c>Who are you?<\uff5cAssistant\uff5c></think>I am DeepSeek<\uff5cend\u2581of\u2581sentence\uff5c><\uff5cUser\uff5c>1+1=?<\uff5cAssistant\uff5c></think>'\n```\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3.1 is the same as DeepSeek-V3. Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running this model locally.\n\n**Usage Recommendations:**\n\n1. **The `mlp.gate.e_score_correction_bias `parameters should be loaded and computed in FP32 precision.**\n2. **Ensure that FP8 model weights and activations are formatted using the UE8M0 scale format.**\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](https://huggingface.co/deepseek-ai/DeepSeek-V3.1/raw/main/LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](https://huggingface.co/deepseek-ai/DeepSeek-V3.1/raw/main/service@deepseek.com).",
      "public": true,
      "version": "67d8a238-7b07-5fd0-a099-8781e432e219",
      "tool_description": null,
      "slug": "chutes-deepseek-ai-deepseek-v3-1",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-08-21T10:21:12.774094Z",
      "updated_at": "2025-09-06T10:50:43.091321Z",
      "image": {
        "image_id": "468c185f-2e19-56d6-9508-6be534e42145",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "nightly-2025090600",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-09-06T09:40:20.310953Z",
        "build_started_at": "2025-09-06T09:40:22.578714Z",
        "build_completed_at": "2025-09-06T09:56:36.126262Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "b200",
        "h200"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "h200",
          "b200"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 5.4,
        "supported_gpus": [
          "b200",
          "h200"
        ]
      },
      "invocation_count": 602592,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.1999188,
            "tao": 0.0005888081599546199
          },
          "output": {
            "usd": 0.800064,
            "tao": 0.0023563777477952695
          }
        },
        "usd": {
          "hour": 21.6,
          "second": 0.006
        },
        "tao": {
          "hour": 0.06361710982168654,
          "second": 1.767141939491293e-05
        }
      },
      "instances": [
        {
          "instance_id": "2a2bbd4c-40d7-40c1-8b68-4928e66e590e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "006f81a0-285b-4cd2-ba73-b886a19dbf3e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "c3d4eac8-0833-4100-8179-d96bb5ab0ac6",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "e4dbd423-ce52-4347-9573-b9604f2f96d0",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "ca0b7a58-9ca6-4817-b0f1-99ed12dbefe6",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "0194d206-7a42-4a3d-bdd9-d87abd28c468",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "600b57e3-52b6-4eb3-8c68-5adf60f7821c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "1beba11d-082d-445f-9f4c-dde90a55e190",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "5c02b931-e058-4d2b-b057-a44049d195f8",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "72c9bb86-6e20-47e3-9476-1ab8524b36b5",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "e0fc669e-e08c-46f9-ac95-ffd794adbad5",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "cfa083df-1c97-484a-b259-b8683b563588",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "aad54a14-8076-45b6-b5aa-462a59fab0f6",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "65240418-a614-49c4-8589-e207aa7e24e1",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "fa54e379-2d19-42e5-9a10-8422b831997d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "171f13e7-8447-4fff-987d-c00bba1026a1",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        },
        {
          "instance_id": "1b26c69e-3099-4684-94f0-f57098b30397",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:50:30.694809Z"
        }
      ],
      "logo_id": "8b9d941a-7477-499e-8c2a-a24b727d430a",
      "openrouter": false,
      "chutes_version": "0.3.23.rc1",
      "revision": "9e6c48c3fa6bb3e1cf684675dc02e813ca45d20f",
      "concurrency": 40,
      "boost": 3.0,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/8b9d941a-7477-499e-8c2a-a24b727d430a.webp",
      "hot": true
    },
    {
      "chute_id": "610528a4-f2db-55c8-a43f-b83f3f215d00",
      "name": "openai/gpt-oss-120b",
      "tagline": "gpt-oss-120b is a 117B parameter open-source language model (with 5.1B active parameters) designed for reasoning, agent-based tasks, and developer use cases that can run on a single 80GB GPU.",
      "readme": "\n<p align=\"center\">\n  <img alt=\"gpt-oss-120b\" src=\"https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-120b.svg\">\n</p>\n\n<p align=\"center\">\n  <a href=\"https://gpt-oss.com\"><strong>Try gpt-oss</strong></a> \u00b7\n  <a href=\"https://cookbook.openai.com/topic/gpt-oss\"><strong>Guides</strong></a> \u00b7\n  <a href=\"https://openai.com/index/gpt-oss-model-card\"><strong>Model card</strong></a> \u00b7\n  <a href=\"https://openai.com/index/introducing-gpt-oss/\"><strong>OpenAI blog</strong></a>\n</p>\n\n<br>\n\nWelcome to the gpt-oss series, [OpenAI\u2019s open-weight models](https://openai.com/open-models) designed for powerful reasoning, agentic tasks, and versatile developer use cases.\n\nWe\u2019re releasing two flavors of these open models:\n- `gpt-oss-120b` \u2014 for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)\n- `gpt-oss-20b` \u2014 for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)\n\nBoth models were trained on our [harmony response format](https://github.com/openai/harmony) and should only be used with the harmony format as it will not work correctly otherwise.\n\n\n> [!NOTE]\n> This model card is dedicated to the larger `gpt-oss-120b` model. Check out [`gpt-oss-20b`](https://huggingface.co/openai/gpt-oss-20b) for the smaller model.\n\n# Highlights\n\n* **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk\u2014ideal for experimentation, customization, and commercial deployment.  \n* **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.  \n* **Full chain-of-thought:** Gain complete access to the model\u2019s reasoning process, facilitating easier debugging and increased trust in outputs. It\u2019s not intended to be shown to end users.  \n* **Fine-tunable:** Fully customize models to your specific use case through parameter fine-tuning.\n* **Agentic capabilities:** Use the models\u2019 native capabilities for function calling, [web browsing](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#browser), [Python code execution](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#python), and Structured Outputs.\n* **MXFP4 quantization:** The models were post-trained with MXFP4 quantization of the MoE weights, making `gpt-oss-120b` run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the `gpt-oss-20b` model run within 16GB of memory. All evals were performed with the same MXFP4 quantization.\n\n---\n\n# Inference examples\n\n## Transformers\n\nYou can use `gpt-oss-120b` and `gpt-oss-20b` with Transformers. If you use the Transformers chat template, it will automatically apply the [harmony response format](https://github.com/openai/harmony). If you use `model.generate` directly, you need to apply the harmony format manually using the chat template or use our [openai-harmony](https://github.com/openai/harmony) package.\n\nTo get started, install the necessary dependencies to setup your environment:\n\n```\npip install -U transformers kernels torch \n```\n\nOnce, setup you can proceed to run the model by running the snippet below:\n\n```py\nfrom transformers import pipeline\nimport torch\n\nmodel_id = \"openai/gpt-oss-120b\"\n\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n]\n\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nAlternatively, you can run the model via [`Transformers Serve`](https://huggingface.co/docs/transformers/main/serving) to spin up a OpenAI-compatible webserver:\n\n```\ntransformers serve\ntransformers chat localhost:8000 --model-name-or-path openai/gpt-oss-120b\n```\n\n[Learn more about how to use gpt-oss with Transformers.](https://cookbook.openai.com/articles/gpt-oss/run-transformers)\n\n## vLLM\n\nvLLM recommends using [uv](https://docs.astral.sh/uv/) for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.\n\n```bash\nuv pip install --pre vllm==0.10.1+gptoss \\\n    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n    --index-strategy unsafe-best-match\n\nvllm serve openai/gpt-oss-120b\n```\n\n[Learn more about how to use gpt-oss with vLLM.](https://cookbook.openai.com/articles/gpt-oss/run-vllm)\n\n## PyTorch / Triton\n\nTo learn about how to use this model with PyTorch and Triton, check out our [reference implementations in the gpt-oss repository](https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation).\n\n## Ollama\n\nIf you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after [installing Ollama](https://ollama.com/download).\n\n```bash\n# gpt-oss-120b\nollama pull gpt-oss:120b\nollama run gpt-oss:120b\n```\n\n[Learn more about how to use gpt-oss with Ollama.](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)\n\n#### LM Studio\n\nIf you are using [LM Studio](https://lmstudio.ai/) you can use the following commands to download.\n\n```bash\n# gpt-oss-120b\nlms get openai/gpt-oss-120b\n```\n\nCheck out our [awesome list](https://github.com/openai/gpt-oss/blob/main/awesome-gpt-oss.md) for a broader collection of gpt-oss resources and inference partners.\n\n---\n\n# Download the model\n\nYou can download the model weights from the [Hugging Face Hub](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4) directly from Hugging Face CLI:\n\n```shell\n# gpt-oss-120b\nhuggingface-cli download openai/gpt-oss-120b --include \"original/*\" --local-dir gpt-oss-120b/\npip install gpt-oss\npython -m gpt_oss.chat model/\n```\n\n# Reasoning levels\n\nYou can adjust the reasoning level that suits your task across three levels:\n\n* **Low:** Fast responses for general dialogue.  \n* **Medium:** Balanced speed and detail.  \n* **High:** Deep and detailed analysis.\n\nThe reasoning level can be set in the system prompts, e.g., \"Reasoning: high\".\n\n# Tool use\n\nThe gpt-oss models are excellent for:\n* Web browsing (using built-in browsing tools)\n* Function calling with defined schemas\n* Agentic operations like browser tasks\n\n# Fine-tuning\n\nBoth gpt-oss models can be fine-tuned for a variety of specialized use cases.\n\nThis larger model `gpt-oss-120b` can be fine-tuned on a single H100 node, whereas the smaller [`gpt-oss-20b`](https://huggingface.co/openai/gpt-oss-20b) can even be fine-tuned on consumer hardware.\n",
      "public": true,
      "version": "229b46c7-1477-5f18-b9fd-bb2f708cf43f",
      "tool_description": null,
      "slug": "chutes-openai-gpt-oss-120b",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-08-05T19:51:23.512412Z",
      "updated_at": "2025-08-19T13:44:02.417127Z",
      "image": {
        "image_id": "d9a02143-06ee-56f4-a52f-e27ad4903e49",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.5.0.rc2",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-18T10:07:42.695059Z",
        "build_started_at": "2025-08-18T10:07:44.616705Z",
        "build_completed_at": "2025-08-18T10:34:17.932804Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "h100_sxm",
        "h200",
        "h100"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "h100",
          "h200",
          "h100_sxm"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 2.8,
        "supported_gpus": [
          "h100_sxm",
          "h200",
          "h100"
        ]
      },
      "invocation_count": 522426,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.07256312,
            "tao": 0.00021371555435389903
          },
          "output": {
            "usd": 0.2903936,
            "tao": 0.0008552778491997644
          }
        },
        "usd": {
          "hour": 7.84,
          "second": 0.0021777777777777776
        },
        "tao": {
          "hour": 0.023090654676019562,
          "second": 6.414070743338767e-06
        }
      },
      "instances": [
        {
          "instance_id": "9b96f53d-2887-46df-934d-977bd3eec862",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:37:12.848707Z"
        },
        {
          "instance_id": "26968cfd-57f9-4dc0-88e9-d91e559b74f4",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:44:03.497522Z"
        },
        {
          "instance_id": "7485553e-2ec4-4f28-b6d3-82bd970d29c9",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:06:13.563094Z"
        },
        {
          "instance_id": "d33befde-4b1c-43d2-918c-714b2a9e80ba",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:11:25.656765Z"
        },
        {
          "instance_id": "7ce649c3-6a27-40e6-b91a-18555bbf07bd",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:57:15.412069Z"
        },
        {
          "instance_id": "43add5f4-4660-48b5-88e0-c2dfcc824773",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:44:15.753073Z"
        },
        {
          "instance_id": "c6b55594-8ca5-4680-baca-ab839d543f0a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:06:58.580239Z"
        },
        {
          "instance_id": "4e9b0158-96c5-4857-a444-02f4a2e65d16",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:59:36.029727Z"
        },
        {
          "instance_id": "c8896f22-9e1a-4498-b40b-a9b87aba82ee",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:53:49.298259Z"
        },
        {
          "instance_id": "0347692e-2779-44fb-996a-c51f556eb007",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:46:28.473632Z"
        },
        {
          "instance_id": "8db85942-e0a5-4901-832e-765fd1e66966",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:58.355593Z"
        },
        {
          "instance_id": "a88d18dc-e379-48fd-b09b-b7f303c0d495",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:19:08.798163Z"
        },
        {
          "instance_id": "f2997f02-7429-412a-8a97-23a6520d0248",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:28:53.325299Z"
        },
        {
          "instance_id": "9630a145-7b52-4fc7-85be-886dcfd83e86",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:02:53.834333Z"
        },
        {
          "instance_id": "1c767a37-a0df-4e28-aca7-c126b25fe519",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:01:27.230484Z"
        },
        {
          "instance_id": "ea89f6fe-0338-4fee-8974-9b88ceb6c992",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:01:49.637994Z"
        },
        {
          "instance_id": "01f546d7-50ef-49cd-8390-ebae6a9e4291",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:42:35.532927Z"
        },
        {
          "instance_id": "ded10ba5-bc30-4cfa-80ea-c395b6ca6211",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:27:39.428188Z"
        },
        {
          "instance_id": "91492f72-d033-435d-b96f-65f41f697205",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:27:53.762348Z"
        },
        {
          "instance_id": "8f3c78f1-0fac-428e-a6ef-0d756bf5dead",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:21:53.758612Z"
        }
      ],
      "logo_id": "2303f59f-b3c6-490a-95cb-c95e298f9e20",
      "openrouter": false,
      "chutes_version": "0.3.19.rc2",
      "revision": "8b193b0ef83bd41b40eb71fee8f1432315e02a3e",
      "concurrency": 32,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/2303f59f-b3c6-490a-95cb-c95e298f9e20.webp",
      "hot": true
    },
    {
      "chute_id": "aa5ff190-f995-5edf-9331-ea8fc5734f3e",
      "name": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
      "tagline": "DeepSeek-R1-0528 is an MIT-licensed large language model optimized for complex reasoning and inference tasks like mathematics and coding, achieving strong performance on benchmarks like AIME and showing capabilities comparable to leading models like O3 and Gemini 2.5 Pro.",
      "readme": "---\r\nlicense: mit\r\nlibrary_name: transformers\r\n---\r\n# DeepSeek-R1-0528\r\n<!-- markdownlint-disable first-line-h1 -->\r\n<!-- markdownlint-disable html -->\r\n<!-- markdownlint-disable no-duplicate-header -->\r\n\r\n<div align=\"center\">\r\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\r\n</div>\r\n<hr>\r\n<div align=\"center\" style=\"line-height: 1;\">\r\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n</div>\r\n\r\n<div align=\"center\" style=\"line-height: 1;\">\r\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\r\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n</div>\r\n\r\n<div align=\"center\" style=\"line-height: 1;\">\r\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\r\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\r\n  </a>\r\n</div>\r\n \r\n\r\n<p align=\"center\">\r\n  <a href=\"https://arxiv.org/pdf/2501.12948\"><b>Paper Link</b>\ud83d\udc41\ufe0f</a>\r\n</p>\r\n\r\n\r\n## 1. Introduction\r\n\r\nThe DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\r\n\r\n<p align=\"center\">\r\n  <img width=\"80%\" src=\"figures/benchmark.png\">\r\n</p>\r\n\r\nCompared to the previous version, the upgraded model shows significant improvements in handling complex reasoning tasks. For instance, in the AIME 2025 test, the model\u2019s accuracy has increased from 70% in the previous version to 87.5% in the current version. This advancement stems from enhanced thinking depth during the reasoning process: in the AIME test set, the previous model used an average of 12K tokens per question, whereas the new version averages 23K tokens per question.\r\n\r\nBeyond its improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding.\r\n\r\n## 2. Evaluation Results\r\n\r\n### DeepSeek-R1-0528\r\n For all our models, the maximum generation length is set to 64K tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 16 responses per query to estimate pass@1.\r\n<div align=\"center\">\r\n\r\n| Category | Benchmark (Metric)               | DeepSeek R1     | DeepSeek R1 0528\r\n|----------|----------------------------------|-----------------|---|\r\n| General  |\r\n|          | MMLU-Redux (EM)                   | 92.9            | 93.4\r\n|          | MMLU-Pro (EM)                     | 84.0            | 85.0\r\n|          | GPQA-Diamond (Pass@1)             | 71.5            | 81.0\r\n|          | SimpleQA (Correct)                | 30.1            | 27.8\r\n|          | FRAMES (Acc.)                     | 82.5            | 83.0\r\n|          | Humanity's Last Exam (Pass@1)                     | 8.5            | 17.7\r\n| Code |\r\n|          | LiveCodeBench (2408-2505) (Pass@1)        | 63.5          | 73.3\r\n|          | Codeforces-Div1 (Rating)          | 1530            | 1930\r\n|          | SWE Verified (Resolved)           | 49.2            | 57.6\r\n|          | Aider-Polyglot (Acc.)             | 53.3            | 71.6\r\n| Math |\r\n|          | AIME 2024 (Pass@1)                | 79.8            | 91.4\r\n|          | AIME 2025 (Pass@1)                     | 70.0           | 87.5\r\n|          | HMMT 2025 (Pass@1)            | 41.7 | 79.4 |\r\n|          | CNMO 2024 (Pass@1)                | 78.8            | 86.9\r\n| Tools |\r\n|          | BFCL_v3_MultiTurn (Acc)     | -            | 37.0 |\r\n|          | Tau-Bench   (Pass@1)       | -            | 53.5(Airline)/63.9(Retail)\r\n\r\n</div>\r\nNote: We use Agentless framework to evaluate model performance on SWE-Verified. We only evaluate text-only prompts in HLE testsets.  GPT-4.1 is employed to act user role in Tau-bench evaluation.\r\n\r\n### DeepSeek-R1-0528-Qwen3-8B\r\nMeanwhile, we distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B. This model achieves state-of-the-art (SOTA) performance among open-source models on the AIME 2024, surpassing Qwen3 8B by +10.0% and matching the performance of Qwen3-235B-thinking. We believe that the chain-of-thought from DeepSeek-R1-0528 will hold significant importance for both academic research on reasoning models and industrial development focused on small-scale models.\r\n\r\n|                                | AIME 24 | AIME 25 | HMMT Feb 25 | GPQA Diamond | LiveCodeBench (2408-2505) |\r\n|--------------------------------|---------|---------|-------------|--------------|---------------------------|\r\n| Qwen3-235B-A22B\t                | 85.7    | 81.5    | 62.5        | 71.1         | 66.5                  |\r\n| Qwen3-32B                      | 81.4    | 72.9    | -           | 68.4         | -                         |\r\n| Qwen3-8B                      | 76.0   | 67.3    | -           | 62.0       | -                         |\r\n| Phi-4-Reasoning-Plus-14B       | 81.3    | 78.0    | 53.6        | 69.3         | -          |\r\n| Gemini-2.5-Flash-Thinking-0520 | 82.3    | 72.0    | 64.2        | 82.8         | 62.3                  |\r\n| o3-mini (medium)               | 79.6    | 76.7    | 53.3        | 76.8         | 65.9                     |\r\n| DeepSeek-R1-0528-Qwen3-8B      | 86.0   | 76.3    | 61.5        | 61.1         | 60.5                      |\r\n\r\n## 3. Chat Website & API Platform\r\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in), and switch on the button \"DeepThink\"\r\n\r\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\r\n\r\n## 4. How to Run Locally\r\n\r\nPlease visit [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1) repository for more information about running DeepSeek-R1-0528 locally.\r\n\r\nCompared to previous versions of DeepSeek-R1, the usage recommendations for DeepSeek-R1-0528 have the following changes:\r\n\r\n1. System prompt is supported now.\r\n2. It is not required to add \"\\<think\\>\\n\" at the beginning of the output to force the model into thinking pattern.\r\n\r\nThe model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528. This model can be run in the same manner as Qwen3-8B, but it is essential to ensure that all configuration files are sourced from our repository rather than the original Qwen3 project.\r\n\r\n### System Prompt\r\nIn the official DeepSeek web/app, we use the same system prompt with a specific date.\r\n```\r\n\u8be5\u52a9\u624b\u4e3aDeepSeek-R1\uff0c\u7531\u6df1\u5ea6\u6c42\u7d22\u516c\u53f8\u521b\u9020\u3002\r\n\u4eca\u5929\u662f{current date}\u3002\r\n```\r\nFor example,\r\n```\r\n\u8be5\u52a9\u624b\u4e3aDeepSeek-R1\uff0c\u7531\u6df1\u5ea6\u6c42\u7d22\u516c\u53f8\u521b\u9020\u3002\r\n\u4eca\u5929\u662f2025\u5e745\u670828\u65e5\uff0c\u661f\u671f\u4e00\u3002\r\n```\r\n### Temperature\r\nIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.6. \r\n### Prompts for File Uploading and Web Search\r\nFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.\r\n```\r\nfile_template = \\\r\n\"\"\"[file name]: {file_name}\r\n[file content begin]\r\n{file_content}\r\n[file content end]\r\n{question}\"\"\"\r\n```\r\nFor Web Search, {search_results}, {cur_date}, and {question} are arguments.\r\nFor Chinese query, we use the prompt:\r\n```\r\nsearch_answer_zh_template = \\\r\n'''# \u4ee5\u4e0b\u5185\u5bb9\u662f\u57fa\u4e8e\u7528\u6237\u53d1\u9001\u7684\u6d88\u606f\u7684\u641c\u7d22\u7ed3\u679c:\r\n{search_results}\r\n\u5728\u6211\u7ed9\u4f60\u7684\u641c\u7d22\u7ed3\u679c\u4e2d\uff0c\u6bcf\u4e2a\u7ed3\u679c\u90fd\u662f[webpage X begin]...[webpage X end]\u683c\u5f0f\u7684\uff0cX\u4ee3\u8868\u6bcf\u7bc7\u6587\u7ae0\u7684\u6570\u5b57\u7d22\u5f15\u3002\u8bf7\u5728\u9002\u5f53\u7684\u60c5\u51b5\u4e0b\u5728\u53e5\u5b50\u672b\u5c3e\u5f15\u7528\u4e0a\u4e0b\u6587\u3002\u8bf7\u6309\u7167\u5f15\u7528\u7f16\u53f7[citation:X]\u7684\u683c\u5f0f\u5728\u7b54\u6848\u4e2d\u5bf9\u5e94\u90e8\u5206\u5f15\u7528\u4e0a\u4e0b\u6587\u3002\u5982\u679c\u4e00\u53e5\u8bdd\u6e90\u81ea\u591a\u4e2a\u4e0a\u4e0b\u6587\uff0c\u8bf7\u5217\u51fa\u6240\u6709\u76f8\u5173\u7684\u5f15\u7528\u7f16\u53f7\uff0c\u4f8b\u5982[citation:3][citation:5]\uff0c\u5207\u8bb0\u4e0d\u8981\u5c06\u5f15\u7528\u96c6\u4e2d\u5728\u6700\u540e\u8fd4\u56de\u5f15\u7528\u7f16\u53f7\uff0c\u800c\u662f\u5728\u7b54\u6848\u5bf9\u5e94\u90e8\u5206\u5217\u51fa\u3002\r\n\u5728\u56de\u7b54\u65f6\uff0c\u8bf7\u6ce8\u610f\u4ee5\u4e0b\u51e0\u70b9\uff1a\r\n- \u4eca\u5929\u662f{cur_date}\u3002\r\n- \u5e76\u975e\u641c\u7d22\u7ed3\u679c\u7684\u6240\u6709\u5185\u5bb9\u90fd\u4e0e\u7528\u6237\u7684\u95ee\u9898\u5bc6\u5207\u76f8\u5173\uff0c\u4f60\u9700\u8981\u7ed3\u5408\u95ee\u9898\uff0c\u5bf9\u641c\u7d22\u7ed3\u679c\u8fdb\u884c\u7504\u522b\u3001\u7b5b\u9009\u3002\r\n- \u5bf9\u4e8e\u5217\u4e3e\u7c7b\u7684\u95ee\u9898\uff08\u5982\u5217\u4e3e\u6240\u6709\u822a\u73ed\u4fe1\u606f\uff09\uff0c\u5c3d\u91cf\u5c06\u7b54\u6848\u63a7\u5236\u572810\u4e2a\u8981\u70b9\u4ee5\u5185\uff0c\u5e76\u544a\u8bc9\u7528\u6237\u53ef\u4ee5\u67e5\u770b\u641c\u7d22\u6765\u6e90\u3001\u83b7\u5f97\u5b8c\u6574\u4fe1\u606f\u3002\u4f18\u5148\u63d0\u4f9b\u4fe1\u606f\u5b8c\u6574\u3001\u6700\u76f8\u5173\u7684\u5217\u4e3e\u9879\uff1b\u5982\u975e\u5fc5\u8981\uff0c\u4e0d\u8981\u4e3b\u52a8\u544a\u8bc9\u7528\u6237\u641c\u7d22\u7ed3\u679c\u672a\u63d0\u4f9b\u7684\u5185\u5bb9\u3002\r\n- \u5bf9\u4e8e\u521b\u4f5c\u7c7b\u7684\u95ee\u9898\uff08\u5982\u5199\u8bba\u6587\uff09\uff0c\u8bf7\u52a1\u5fc5\u5728\u6b63\u6587\u7684\u6bb5\u843d\u4e2d\u5f15\u7528\u5bf9\u5e94\u7684\u53c2\u8003\u7f16\u53f7\uff0c\u4f8b\u5982[citation:3][citation:5]\uff0c\u4e0d\u80fd\u53ea\u5728\u6587\u7ae0\u672b\u5c3e\u5f15\u7528\u3002\u4f60\u9700\u8981\u89e3\u8bfb\u5e76\u6982\u62ec\u7528\u6237\u7684\u9898\u76ee\u8981\u6c42\uff0c\u9009\u62e9\u5408\u9002\u7684\u683c\u5f0f\uff0c\u5145\u5206\u5229\u7528\u641c\u7d22\u7ed3\u679c\u5e76\u62bd\u53d6\u91cd\u8981\u4fe1\u606f\uff0c\u751f\u6210\u7b26\u5408\u7528\u6237\u8981\u6c42\u3001\u6781\u5177\u601d\u60f3\u6df1\u5ea6\u3001\u5bcc\u6709\u521b\u9020\u529b\u4e0e\u4e13\u4e1a\u6027\u7684\u7b54\u6848\u3002\u4f60\u7684\u521b\u4f5c\u7bc7\u5e45\u9700\u8981\u5c3d\u53ef\u80fd\u5ef6\u957f\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u8981\u70b9\u7684\u8bba\u8ff0\u8981\u63a8\u6d4b\u7528\u6237\u7684\u610f\u56fe\uff0c\u7ed9\u51fa\u5c3d\u53ef\u80fd\u591a\u89d2\u5ea6\u7684\u56de\u7b54\u8981\u70b9\uff0c\u4e14\u52a1\u5fc5\u4fe1\u606f\u91cf\u5927\u3001\u8bba\u8ff0\u8be6\u5c3d\u3002\r\n- \u5982\u679c\u56de\u7b54\u5f88\u957f\uff0c\u8bf7\u5c3d\u91cf\u7ed3\u6784\u5316\u3001\u5206\u6bb5\u843d\u603b\u7ed3\u3002\u5982\u679c\u9700\u8981\u5206\u70b9\u4f5c\u7b54\uff0c\u5c3d\u91cf\u63a7\u5236\u57285\u4e2a\u70b9\u4ee5\u5185\uff0c\u5e76\u5408\u5e76\u76f8\u5173\u7684\u5185\u5bb9\u3002\r\n- \u5bf9\u4e8e\u5ba2\u89c2\u7c7b\u7684\u95ee\u7b54\uff0c\u5982\u679c\u95ee\u9898\u7684\u7b54\u6848\u975e\u5e38\u7b80\u77ed\uff0c\u53ef\u4ee5\u9002\u5f53\u8865\u5145\u4e00\u5230\u4e24\u53e5\u76f8\u5173\u4fe1\u606f\uff0c\u4ee5\u4e30\u5bcc\u5185\u5bb9\u3002\r\n- \u4f60\u9700\u8981\u6839\u636e\u7528\u6237\u8981\u6c42\u548c\u56de\u7b54\u5185\u5bb9\u9009\u62e9\u5408\u9002\u3001\u7f8e\u89c2\u7684\u56de\u7b54\u683c\u5f0f\uff0c\u786e\u4fdd\u53ef\u8bfb\u6027\u5f3a\u3002\r\n- \u4f60\u7684\u56de\u7b54\u5e94\u8be5\u7efc\u5408\u591a\u4e2a\u76f8\u5173\u7f51\u9875\u6765\u56de\u7b54\uff0c\u4e0d\u80fd\u91cd\u590d\u5f15\u7528\u4e00\u4e2a\u7f51\u9875\u3002\r\n- \u9664\u975e\u7528\u6237\u8981\u6c42\uff0c\u5426\u5219\u4f60\u56de\u7b54\u7684\u8bed\u8a00\u9700\u8981\u548c\u7528\u6237\u63d0\u95ee\u7684\u8bed\u8a00\u4fdd\u6301\u4e00\u81f4\u3002\r\n# \u7528\u6237\u6d88\u606f\u4e3a\uff1a\r\n{question}'''\r\n```\r\nFor English query, we use the prompt:\r\n```\r\nsearch_answer_en_template = \\\r\n'''# The following contents are the search results related to the user's message:\r\n{search_results}\r\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\r\nWhen responding, please keep the following points in mind:\r\n- Today is {cur_date}.\r\n- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.\r\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\r\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\r\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\r\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\r\n- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.\r\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\r\n- Unless the user requests otherwise, your response should be in the same language as the user's question.\r\n# The user's message is:\r\n{question}'''\r\n```\r\n\r\n## 5. License\r\nThis code repository is licensed under [MIT License](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B/raw/main/LICENSE). The use of DeepSeek-R1 models is also subject to [MIT License](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B/raw/main/LICENSE). DeepSeek-R1 series (including Base and Chat) supports commercial use and distillation.\r\n\r\n## 6. Citation\r\n```\r\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\r\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \r\n      author={DeepSeek-AI},\r\n      year={2025},\r\n      eprint={2501.12948},\r\n      archivePrefix={arXiv},\r\n      primaryClass={cs.CL},\r\n      url={https://arxiv.org/abs/2501.12948}, \r\n}\r\n```\r\n\r\n## 7. Contact\r\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B/raw/main/service@deepseek.com).\r\n",
      "public": true,
      "version": "1ad22ea5-4b6c-5823-b871-ad4f20c0ea09",
      "tool_description": null,
      "slug": "chutes-deepseek-ai-deepseek-r1-0528-qwen3-8b",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-05-29T15:35:34.111992Z",
      "updated_at": "2025-09-06T10:43:03.112452Z",
      "image": {
        "image_id": "468c185f-2e19-56d6-9508-6be534e42145",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "nightly-2025090600",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-09-06T09:40:20.310953Z",
        "build_started_at": "2025-09-06T09:40:22.578714Z",
        "build_completed_at": "2025-09-06T09:56:36.126262Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "5090",
        "a10",
        "a100_sxm",
        "l40s",
        "l4",
        "h200",
        "a40",
        "h100_nvl",
        "3090",
        "l40",
        "a100_40gb_sxm",
        "a5000",
        "a100_40gb",
        "h100",
        "h20",
        "a6000",
        "h800",
        "a6000_ada",
        "4090",
        "a100",
        "h100_sxm",
        "pro_6000"
      ],
      "node_selector": {
        "exclude": [
          "mi300x",
          "b200"
        ],
        "include": null,
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 24,
        "compute_multiplier": 0.46,
        "supported_gpus": [
          "5090",
          "a10",
          "a100_sxm",
          "l40s",
          "l4",
          "h200",
          "a40",
          "h100_nvl",
          "3090",
          "l40",
          "a100_40gb_sxm",
          "a5000",
          "a100_40gb",
          "h100",
          "h20",
          "a6000",
          "h800",
          "a6000_ada",
          "4090",
          "a100",
          "h100_sxm",
          "pro_6000"
        ]
      },
      "invocation_count": 495789,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.01703012,
            "tao": 5.0157732144282423e-05
          },
          "output": {
            "usd": 0.0681536,
            "tao": 0.00020072847481218963
          }
        },
        "usd": {
          "hour": 1.84,
          "second": 0.0005111111111111112
        },
        "tao": {
          "hour": 0.005419235281106632,
          "second": 1.505343133640731e-06
        }
      },
      "instances": [
        {
          "instance_id": "a4f46af2-c658-4340-8479-0cdf9fa623a0",
          "region": "n/a",
          "active": false,
          "verified": true,
          "last_verified_at": "2025-09-08T10:28:55.861246Z"
        },
        {
          "instance_id": "59022028-c39d-4ee6-9bdb-bc12ad54e9d7",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:14:14.853950Z"
        },
        {
          "instance_id": "52643049-a4ce-49cd-b758-eaa09fda6465",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:14:17.112010Z"
        },
        {
          "instance_id": "c4c46c57-34a4-4282-acc4-5eac252e515a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:14:18.727084Z"
        },
        {
          "instance_id": "2c1cd105-08ee-4927-ada3-aa0e0aacba02",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:14:28.738278Z"
        },
        {
          "instance_id": "6693ce1a-5593-41fd-a55c-1113bd89097b",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:14:17.065431Z"
        },
        {
          "instance_id": "ae86f281-53e3-47d0-8a63-1070895ba2dc",
          "region": "n/a",
          "active": false,
          "verified": true,
          "last_verified_at": "2025-09-08T10:27:15.102084Z"
        },
        {
          "instance_id": "457b70f7-d97c-4303-b24f-2a53748d7332",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:59:33.427999Z"
        },
        {
          "instance_id": "f363105e-49d7-4f2f-a04f-830c8ee81237",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:59:33.427999Z"
        },
        {
          "instance_id": "0fed3e39-ecd8-4054-92bf-3dfafe383383",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:59:33.427999Z"
        },
        {
          "instance_id": "dd26c0b6-f965-47d9-a148-e2e037b6ea69",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:59:33.427999Z"
        },
        {
          "instance_id": "4496cf26-7caf-46ca-acf8-2ad18fb21fc4",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:59:33.427999Z"
        },
        {
          "instance_id": "b773d7a5-6257-4407-9c03-51fb250bc216",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:32.682306Z"
        },
        {
          "instance_id": "22ab846c-3395-444f-aee6-5e4cef9d7eb8",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:58.442487Z"
        },
        {
          "instance_id": "941bd435-b303-4351-9a97-f1d46638f0ec",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:24:20.637792Z"
        },
        {
          "instance_id": "b24ce957-b2bd-4c3b-82d8-7d3eb2588460",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:24:49.924911Z"
        }
      ],
      "logo_id": "daf43d25-5713-4318-b763-436c5c321a0a",
      "openrouter": true,
      "chutes_version": "0.3.23.rc1",
      "revision": "6e8885a6ff5c1dc5201574c8fd700323f23c25fa",
      "concurrency": 48,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/daf43d25-5713-4318-b763-436c5c321a0a.webp",
      "hot": true
    },
    {
      "chute_id": "aa9579f7-ac87-53d3-b23c-4fa97a64574d",
      "name": "Qwen/Qwen3-8B",
      "tagline": "Qwen3-8B is an 8.2 billion parameter open source language model that can switch between thinking and non-thinking modes for different tasks, with capabilities in reasoning, math, coding, creative writing, and support for over 100 languages.",
      "readme": "\n# Qwen3-8B\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-8B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 8.2B\n- Number of Paramaters (Non-Embedding): 6.95B\n- Number of Layers: 36\n- Number of Attention Heads (GQA): 32 for Q and 8 for KV\n- Context Length: 32,768 natively and [131,072 tokens with YaRN](https://huggingface.co/Qwen/Qwen3-8B/raw/main/#processing-long-texts). \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: 'qwen3'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-8B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-8B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](https://huggingface.co/Qwen/Qwen3-8B/raw/main/#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](https://huggingface.co/Qwen/Qwen3-8B/raw/main/#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-8B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-8B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Processing Long Texts\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        \"rope_scaling\": {\n            \"rope_type\": \"yarn\",\n            \"factor\": 4.0,\n            \"original_max_position_embeddings\": 32768\n        }\n    }\n    ```\n  For `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072  \n    ```\n\n  For `sglang`, you can use\n    ```shell\n    python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\n    ```\n\n  For `llama-server` from `llama.cpp`, you can use\n    ```shell\n    llama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n    ```\n\n> [!IMPORTANT]\n> If you encounter the following warning\n> ```\n> Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\n> ```\n> please upgrade `transformers>=4.51.0`.\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0. \n\n> [!NOTE]\n> The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n> [!TIP]\n> The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only incl",
      "public": true,
      "version": "c2ebbf76-da66-52d3-a024-5b21a3d60657",
      "tool_description": null,
      "slug": "chutes-qwen-qwen3-8b",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "bcc7eb3b-d9ea-5892-b6ce-97ae9dc8e825",
      "created_at": "2025-04-28T22:13:31.171463Z",
      "updated_at": "2025-08-07T10:43:22.232004Z",
      "image": {
        "image_id": "dd30a5e7-24ae-538e-bee7-ba6a93b3d2f0",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.4.9.post6",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-06T20:56:26.489668Z",
        "build_started_at": "2025-08-06T20:56:28.092938Z",
        "build_completed_at": "2025-08-06T21:28:45.347952Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "a6000_ada",
        "h100_nvl",
        "a100",
        "l40",
        "a100_40gb_sxm",
        "a100_40gb",
        "h20",
        "a100_sxm",
        "l40s",
        "pro_6000",
        "h100_sxm",
        "h200",
        "a40",
        "a6000",
        "h100",
        "h800"
      ],
      "node_selector": {
        "exclude": [
          "mi300x",
          "b200"
        ],
        "include": null,
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 40,
        "compute_multiplier": 0.54,
        "supported_gpus": [
          "a6000_ada",
          "h100_nvl",
          "a100",
          "l40",
          "a100_40gb_sxm",
          "a100_40gb",
          "h20",
          "a100_sxm",
          "l40s",
          "pro_6000",
          "h100_sxm",
          "h200",
          "a40",
          "a6000",
          "h100",
          "h800"
        ]
      },
      "invocation_count": 452684,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.0,
            "tao": 0.0
          },
          "output": {
            "usd": 0.0,
            "tao": 0.0
          }
        },
        "usd": {
          "hour": 0.0,
          "second": 0.0
        },
        "tao": {
          "hour": 0.0,
          "second": 0.0
        }
      },
      "instances": [
        {
          "instance_id": "9ca5d998-16f0-4a19-8926-24f2e6dc98f2",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:58:25.363722Z"
        },
        {
          "instance_id": "faa45e76-1a7e-4810-b2d5-55d15a4df306",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:58:25.363722Z"
        },
        {
          "instance_id": "6e25ae88-fb59-4042-9725-38029827014d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:58:25.363722Z"
        },
        {
          "instance_id": "a8f66e53-eb24-4520-8cec-8fa851911507",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:58:25.363722Z"
        },
        {
          "instance_id": "da4231a5-b819-481a-b9c8-94a1e951640f",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:58:25.363722Z"
        }
      ],
      "logo_id": "6866dead-21bd-481a-87aa-6c723c5fe9cb",
      "openrouter": true,
      "chutes_version": "0.3.19.rc0",
      "revision": "a80f5e57cce20e57b65145f4213844dec1a80834",
      "concurrency": 32,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/6866dead-21bd-481a-87aa-6c723c5fe9cb.webp",
      "hot": true
    },
    {
      "chute_id": "4fa0c7f5-82f7-59d1-8996-661bb778893d",
      "name": "tngtech/DeepSeek-TNG-R1T2-Chimera",
      "tagline": "DeepSeek-TNG-R1T2-Chimera is a 671B parameter model that combines three DeepSeek parent models using Assembly of Experts methodology, offering improved intelligence over DeepSeek-R1 while being 20% faster, making it particularly suitable for reasoning tasks and general-purpose use cases.",
      "readme": "# DeepSeek-TNG-R1T2-Chimera\n\n<div align=\"center\">\n<img src=\"https://354918363417-runtime-assets.s3.eu-central-1.amazonaws.com/company_logo_light.svg\"\n     alt=\"TNG Logo\" \n     width=\"400\"\n     style=\"display: inline-block; vertical-align: middle;\"/>\n</div>\n<br>\n<div align=\"center\">\n  <a href=\"https://huggingface.co/tngtech/DeepSeek-TNG-R1T2-Chimera/blob/main/LICENSE.DeepSeek\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<br>\n<div align=\"center\">\n    <img alt=\"Intelligence Score\" src=\"intelligence_score_vs_output_tokens.png\" style=\"display: inline-block; vertical-align: middle;\" width=\"750\"/>\n    <figcaption><a href=\"https://x.com/tngtech/status/1940531045432283412\">Release Announcement on X</a></figcaption>\n</div>\n\n\n## Assembly of Experts Chimera model constructed with the DeepSeek [R1-0528](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528), [R1](https://huggingface.co/deepseek-ai/DeepSeek-R1) and [V3-0324](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324) parent models\n\nWe present our new **DeepSeek-TNG R1T2 Chimera** 671B model, the first successor to our original [*DeepSeek R1T Chimera*](https://huggingface.co/tngtech/DeepSeek-R1T-Chimera) that was released on April 26th. Unlike the original Chimera, which was based on the *two parent models* V3-0324 and R1, the new Chimera is a **Tri-Mind** *with three parents*, namely additionally R1-0528. It is constructed using the Assembly of Experts-method with relatively fine-granular direct brain edits. This more refined assembly allowed, among other improvements, the fixing of the &lt;think&gt; token consistency issue, which was a weakness of R1T and is now solved for R1T2. \n\n**Sweet spot**\n\nR1T2 operates at a new sweet spot in intelligence vs. output token length. It appears to be...\n\n- about **20% faster than** the regular **R1**, and more than **twice as fast as R1-0528**\n- significantly **more intelligent than** the regular **R1** in benchmarks such as **GPQA**, **AIME-24** and **Aider Polyglot**\n- much **more intelligent** and also **think-token consistent** compared to the first **R1T Chimera** 0426\n- and generally well-behaved and a **nice persona** to talk to, even without any system prompt.\n\n**Recommendations for your model decision**\n\n*R1T2* compared...\n- *vs R1:* We hope that R1T2 is a very desirable, almost universally **better drop-in replacement for R1**\n- *vs R1-0528:* R1T2 is a much **cheaper alternative to the full R1-0528**, if the full 0528-level intelligence is not required\n- *vs R1T:* R1T2 is usually **recommended over R1T**, unless the specific personality of R1T was optimal, the think-token issue not important, or R1T's higher speed crucial\n- *vs V3-0324:* V3 is so much faster that if you can live with the **lower intelligence, take V3**, however, if you **need reasoning, R1T2** is the go-to model\n\n**Limitations**\n\n- **R1-0528** is thinking much longer, but also is achieving **better hard benchmark results** than R1T2\n- As measured by SpeechMap.ai (courtesy of xlr8harder), **R1T2** is significantly **more reserved** than R1T, but not as much as R1-0528\n- When switching from R1T to R1T2 development, we changed from AIME24 and MT-Bench to AIME24, AIME25 and GPQA-Diamond for the intelligence score. With the new benchmark set, there is a larger score difference between R1 and the original R1T Chimera than published earlier.\n- Function calling is supported in general, but both vLLM and SGLang currently require some specific adaptions, see the section below.\n\n**Evaluation results**\n\nEvaluation was performed using the evalchemy framework (pass@1 averaged over 10/5 runs for AIME/GPQAD, at a temperature of 0.6).\nWe report measured benchmark results for our R1T2, R1T models and published benchmark results for V3-0324, R1, R1-0528.\n\n|                                    | R1T2 |  R1T | V3-0324 |   R1 | R1-0528 | Comment |\n|:-----------------------------------|-----:|-----:|--------:|-----:|--------:|:--------|\n| AIME-24                            | 82.3 | 74.7 |    59.4 | 79.8 |    91.4 |         |\n| AIME-25                            | 70.0 | 58.3 |    49.6 | 70.0 |    87.5 | V3-0324 source: AIME-25 measured by us |\n| GPQA-Diamond                       | 77.9 | 72.0 |    68.4 | 71.5 |    81.0 |         |\n| Aider Polyglot                     | 64.4 | 48.4 |    44.9 | 52.0 |    71.6 | R1T2 source: Aider discord, t=0.75 |\n| EQ-Bench Longform Creative Writing | 76.4 |  ./. |    78.1 | 74.6 |    78.9 | see [EQ Bench](https://eqbench.com/creative_writing_longform.html)  |\n\n## Technological background\n\nFor details on the AoE construction process, you can read our [Paper on arXiV](https://arxiv.org/abs/2506.14794).\n\n**Runtime parameter settings**\n\n- Most of our evaluation was done with a maximum context size of 60,000 tokens.\n  With a context size of 130,000 tokens, the model proved very helpful in interpreting very long debug logs. Long-context testing was less extensive, though.\n- We're running the model using vLLM on 8xH200 and MI325X nodes, additionally we've tested the model using SGLang, which is also used by [chutes.ai](https://chutes.ai/app/chute/4fa0c7f5-82f7-59d1-8996-661bb778893d).\n- For SGLang, we recommend using versions >= v0.4.8 in combination with argument `--reasoning-parser qwen3` to properly handle rare cases when the model skips the `<think>` reasoning step. \n\n\n### Function calling\n\nR1T2 does support function calling using an updated chat template (since 01 Aug 2025). However, neither vLLM nor SGLang provide an R1T2-compatible tool call parser natively but require some adaptions.\n\n_vLLM:_\n\nFor function calling with vLLM, a new tool parser is required. While we opened [a PR to vLLM](https://github.com/vllm-project/vllm/pull/22074) to include an R1T2-compatible tool parser off-the-shelf, we also ship the tool parser file `tool_parser_vllm.py` within this repository. \nWith this file, tool calling can be enabled via\n```\n--tool-parser-plugin <ABSOLUTE_MODEL_SNAPSHOT_PATH>/tool_parser_vllm.py  \\\n--tool-call-parser tng_r1t2\n```\n\nHere, put in the path to the snapshot folder such as `~/.cache/huggingface/hub/models--tngtech--DeepSeek-TNG-R1T2-Chimera/snapshots/SNAPSHOT/tool_parser_vllm.py`\n\n_SGLang:_\n\nTool call support for R1T2 requires a recent SGLang version >= v0.4.10 (alternatively, you need to patch [this bugfix for the reasoning parser](https://github.com/sgl-project/sglang/pull/8606) for older versions of SGLang).\n\nAn R1T2-compatible tool call parser will be added with [this PR to SGLang](https://github.com/sgl-project/sglang/pull/8672). \nUnfortunately, and unlike vLLM, there is no simple plugin system for tool call parsers in SGLang. \nUntil our PR is merged an relased with a new SGLang version, you can still install it manually by patching your SGLang source code as outlined in the PR:\nThe new tool call parser must be added and registered (so in total one file must be added, a second one edited, see [details here](https://github.com/sgl-project/sglang/pull/8672/files)).\n\nOnce the SGLang installation has been updated correctly, tool calling with R1T2 can be activated by starting SGLang with\n\n```\n--tool-call-parser tng_r1t2\n```\n\n\n## Model Details\n\n- **Architecture**: DeepSeek-MoE transformer-based language model\n- **Combination Method**: Assembly of Experts from the three DeepSeek parent models R1-0528, R1 and V3-0324\n- **Release Date**: 2025-07-02\n- **Design Team**: Robert Dahlke, Henrik Klagges, Benjamin Merkel, Fabian Klemm and David Reiss, Munich, Germany\n- **Extra Thanks**: Big thanks to DeepSeek for their great models and open-source generosity, and to the other researchers that have published on model merging methodologies.\n\n\n## Use, Out-of-scope Use, Other Limitations, Risks, Recommendations et al.\nRegarding the R1T/R1T2-Chimeras, we ask you to follow the careful guidelines that Microsoft has created for their \"MAI-DS-R1\" DeepSeek-based model. \nThese professional guidelines are available [here on Hugging Face](https://huggingface.co/microsoft/MAI-DS-R1).\n\n## EU AI Act\n\nDue to the strict new guidelines of the EU AI Act that take effect on August 2nd 2025, we recommend that each R1T/R1T2 user in the EU either familiarizes themselves with these requirements and assess their compliance, or ceases using the model in the EU after August 1st, 2025.\n\n## Contact, especially for your user feedback\n\nPlease give us your feedback, especially if you find deficiencies in the model:\n- Email: research@tngtech.com\n- X.com: @tngtech\n\n## Citation\n\n```\n@misc{tng_technology_consulting_gmbh_2025_07_02,\n\tauthor       = { TNG Technology Consulting GmbH },\n\ttitle        = { DeepSeek-TNG-R1T2-Chimera },\n\tyear         = 2025,\n    month        = { July },\n\turl          = { https://huggingface.co/tngtech/DeepSeek-TNG-R1T2-Chimera },\n\tdoi          = { 10.57967/hf/5950 },\n\tpublisher    = { Hugging Face }\n}\n```",
      "public": true,
      "version": "8214b988-9895-50c5-b723-d2fcafdcb870",
      "tool_description": null,
      "slug": "chutes-tngtech-deepseek-tng-r1t2-chimera",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "bcc7eb3b-d9ea-5892-b6ce-97ae9dc8e825",
      "created_at": "2025-08-04T15:51:33.777813Z",
      "updated_at": "2025-08-05T13:41:10.746363Z",
      "image": {
        "image_id": "cc2117a5-f9f8-5659-85a1-fcc29565aafd",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.4.10.r1t2",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-03T07:59:34.916732Z",
        "build_started_at": "2025-08-03T07:59:37.630758Z",
        "build_completed_at": "2025-08-03T08:26:17.930255Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "cd5398ee30c1",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "h200"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "h200"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 5.4,
        "supported_gpus": [
          "h200"
        ]
      },
      "invocation_count": 337077,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.1999188,
            "tao": 0.0005888081599546199
          },
          "output": {
            "usd": 0.800064,
            "tao": 0.0023563777477952695
          }
        },
        "usd": {
          "hour": 21.6,
          "second": 0.006
        },
        "tao": {
          "hour": 0.06361710982168654,
          "second": 1.767141939491293e-05
        }
      },
      "instances": [
        {
          "instance_id": "994ef391-d822-43a3-8a78-bae20ca82fc9",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:07.045953Z"
        },
        {
          "instance_id": "e153acfe-5c14-4ea8-a3a3-4ee6b08e2394",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:07.045953Z"
        },
        {
          "instance_id": "dc279a41-71a1-4647-ad48-7dd5586f4efd",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:07.045953Z"
        },
        {
          "instance_id": "fed7ff40-ce24-4aae-bf3a-0451a72f9525",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:07.045953Z"
        },
        {
          "instance_id": "dfc52ab2-4149-42c2-a639-9dff8adaf651",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:07.045953Z"
        },
        {
          "instance_id": "57bd0001-295f-4232-b70b-b1cd7dd57baa",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:07.045953Z"
        },
        {
          "instance_id": "e6895c29-48b4-435a-a8d7-ea08f6ea09e3",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:07.045953Z"
        },
        {
          "instance_id": "fa1f9e0d-af9e-4d2f-9014-f9bb15ea4edb",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:07.045953Z"
        },
        {
          "instance_id": "ab5c41da-5bb0-4629-8e4e-8564157bbb22",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:07.045953Z"
        },
        {
          "instance_id": "9ec8abc8-c5a2-4976-be88-5fa6f1b820cf",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:07.045953Z"
        },
        {
          "instance_id": "0bbe4d1c-811e-4451-a8a1-f7187fe41768",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:07.045953Z"
        },
        {
          "instance_id": "72b876c1-925e-4623-add9-137d25e2b140",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:20:07.045953Z"
        }
      ],
      "logo_id": "08a7039d-7e5c-42dd-b9e5-c53c9275a77a",
      "openrouter": false,
      "chutes_version": "0.3.17",
      "revision": "0b8033a9b334d94a7abf266d4067789831dfd39f",
      "concurrency": 24,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/08a7039d-7e5c-42dd-b9e5-c53c9275a77a.webp",
      "hot": true
    },
    {
      "chute_id": "6bde588c-7b10-53ef-aadc-08c292f702fa",
      "name": "tencent/Hunyuan-A13B-Instruct",
      "tagline": "Hunyuan-A13B is a 13B-parameter MoE model (with 80B total parameters) optimized for reasoning and general tasks, offering high performance with minimal computational overhead through selective parameter activation.",
      "readme": "\n<p align=\"center\">\n <img src=\"https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png\" width=\"400\"/> <br>\n</p><p></p>\n\n\n<p align=\"center\">\n    \ud83e\udd17&nbsp;<a href=\"https://huggingface.co/tencent/Hunyuan-A13B-Instruct\"><b>Hugging Face</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    \ud83d\udda5\ufe0f&nbsp;<a href=\"https://hunyuan.tencent.com\" style=\"color: red;\"><b>Official Website</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    \ud83d\udd56&nbsp;<a href=\"https://cloud.tencent.com/product/hunyuan\"><b>HunyuanAPI</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    \ud83d\udd79\ufe0f&nbsp;<a href=\"https://hunyuan.tencent.com/?model=hunyuan-a13b\"><b>Demo</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    \ud83e\udd16&nbsp;<a href=\"https://modelscope.cn/models/Tencent-Hunyuan/Hunyuan-A13B-Instruct\"><b>ModelScope</b></a>\n</p>\n\n\n<p align=\"center\">\n    <a href=\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/report/Hunyuan_A13B_Technical_Report.pdf\"><b>Technical Report</b> </a> |\n    <a href=\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B\"><b>GITHUB</b></a> | \n    <a href=\"https://cnb.cool/tencent/hunyuan/Hunyuan-A13B\"><b>cnb.cool</b></a> | \n    <a href=\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/LICENSE\"><b>LICENSE</b></a> | \n    <a href=\"https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan-A13B/main/assets/1751881231452.jpg\"><b>WeChat</b></a> | \n    <a href=\"https://discord.gg/bsPcMEtV7v\"><b>Discord</b></a>\n</p>\n\n\n  \nWelcome to the official repository of **Hunyuan-A13B**, an innovative and open-source large language model (LLM) built on a fine-grained Mixture-of-Experts (MoE) architecture. Designed for efficiency and scalability, Hunyuan-A13B delivers cutting-edge performance with minimal computational overhead, making it an ideal choice for advanced reasoning and general-purpose applications, especially in resource-constrained environments.\n\n## Model Introduction\n\nWith the rapid advancement of artificial intelligence technology, large language models (LLMs) have achieved remarkable progress in natural language processing, computer vision, and scientific tasks. However, as model scales continue to expand, optimizing resource consumption while maintaining high performance has become a critical challenge. To address this, we have explored Mixture of Experts (MoE) architectures. The newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters. It not only delivers high-performance results but also achieves optimal resource efficiency, successfully balancing computational power and resource utilization.\n\n### Key Features and Advantages\n\n- **Compact yet Powerful**: With only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models.\n- **Hybrid Reasoning Support**: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.\n- **Ultra-Long Context Understanding**: Natively supports a 256K context window, maintaining stable performance on long-text tasks.\n- **Enhanced Agent Capabilities**: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, \u03c4-Bench and C3-Bench.\n- **Efficient Inference**: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.\n\n### Why Choose Hunyuan-A13B?\n\nAs a powerful yet computationally efficient large model, Hunyuan-A13B is an ideal choice for researchers and developers seeking high performance under resource constraints. Whether for academic research, cost-effective AI solution development, or innovative application exploration, this model provides a robust foundation for advancement.\n\n&nbsp;\n\n## Related News\n* 2025.6.27 We have open-sourced  **Hunyuan-A13B-Pretrain** , **Hunyuan-A13B-Instruct** , **Hunyuan-A13B-Instruct-FP8** , **Hunyuan-A13B-Instruct-GPTQ-Int4** on Hugging Face. In addition, we have released a <a href=\"report/Hunyuan_A13B_Technical_Report.pdf\">technical report </a> and a training and inference operation manual, which provide detailed information about the model\u2019s capabilities as well as the operations for training and inference.\n\n<br>\n\n\n## Benchmark\n\nNote: The following benchmarks are evaluated by TRT-LLM-backend on several **base models**. \n\n| Model            | Hunyuan-Large | Qwen2.5-72B  | Qwen3-A22B | Hunyuan-A13B |\n|------------------|---------------|--------------|-------------|---------------|\n| MMLU             | 88.40          | 86.10         | 87.81        | 88.17          |\n| MMLU-Pro         | 60.20          | 58.10        | 68.18           | 67.23          |\n| MMLU-Redux              |  87.47         | 83.90         | 87.40        | 87.67          |\n| BBH        | 86.30             | 85.80            | 88.87        | 87.56          |\n| SuperGPQA    |  38.90         | 36.20          | 44.06           | 41.32          |\n| EvalPlus       | 75.69          | 65.93         | 77.60        | 78.64          |\n| MultiPL-E             | 59.13             | 60.50            | 65.94        | 69.33          |\n| MBPP | 72.60             | 76.00            | 81.40        | 83.86          |\n| CRUX-I             | 57.00          | 57.63          | -        | 70.13          |\n| CRUX-O             | 60.63          | 66.20          | 79.00        | 77.00          |\n| MATH            | 69.80          | 62.12         | 71.84        | 72.35          |\n| CMATH            | 91.30          | 84.80         | -        | 91.17          |\n| GSM8k         | 92.80             | 91.50           | 94.39        | 91.83          |\n| GPQA            | 25.18             | 45.90            | 47.47        | 49.12          |\n\n\nHunyuan-A13B-Instruct has achieved highly competitive performance across multiple benchmarks, particularly in mathematics, science, agent domains, and more. We compared it with several powerful models, and the results are shown below.\n\n| Topic               |                        Bench                         | OpenAI-o1-1217 | DeepSeek R1 | Qwen3-A22B | Hunyuan-A13B-Instruct |\n|:-------------------:|:----------------------------------------------------:|:-------------:|:------------:|:-----------:|:---------------------:|\n| **Mathematics**     |            AIME 2024<br>AIME 2025<br>MATH            | 74.3<br>79.2<br>96.4 | 79.8<br>70<br>94.9 | 85.7<br>81.5<br>94.0 | 87.3<br>76.8<br>94.3 |\n| **Science**         |            GPQA-Diamond<br>OlympiadBench             | 78<br>83.1 | 71.5<br>82.4 | 71.1<br>85.7 | 71.2<br>82.7 |\n| **Coding**          |  Livecodebench<br>Fullstackbench<br>ArtifactsBench   | 63.9<br>64.6<br>38.6 | 65.9<br>71.6<br>44.6 | 70.7<br>65.6<br>44.6 | 63.9<br>67.8<br>43 |\n| **Reasoning**       |              BBH<br>DROP<br>ZebraLogic               | 80.4<br>90.2<br>81 | 83.7<br>92.2<br>78.7 | 88.9<br>90.3<br>80.3 | 89.1<br>91.1<br>84.7 |\n| **Instruction<br>Following** |                 IF-Eval<br>SysBench                  | 91.8<br>82.5 | 88.3<br>77.7 | 83.4<br>74.2 | 84.7<br>76.1 |\n| **Text<br>Creation**|                LengthCtrl<br>InsCtrl                 | 60.1<br>74.8 | 55.9<br>69 | 53.3<br>73.7 | 55.4<br>71.9 |\n| **NLU**             |               ComplexNLU<br>Word-Task                | 64.7<br>67.1 | 64.5<br>76.3 | 59.8<br>56.4 | 61.2<br>62.9 |\n| **Agent**           | BFCL v3<br> \u03c4-Bench<br>ComplexFuncBench<br> C3-Bench | 67.8<br>60.4<br>47.6<br>58.8 | 56.9<br>43.8<br>41.1<br>55.3 | 70.8<br>44.6<br>40.6<br>51.7 | 78.3<br>54.7<br>61.2<br>63.5 |\n\n\n&nbsp;\n\n## Use with transformers\n\nOur model defaults to using slow-thinking reasoning, and there are two ways to disable CoT reasoning. \n1. Pass \"enable_thinking=False\" when calling apply_chat_template.\n2. Adding \"/no_think\" before the prompt will force the model not to use perform CoT reasoning. Similarly, adding \"/think\" before the prompt will force the model to perform CoT reasoning.\n\nThe following code snippet shows how to use the transformers library to load and apply the model. \nIt also demonstrates how to enable and disable the reasoning mode , \nand how to parse the reasoning process along with the final output.\n\n\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nimport re\n\nmodel_name_or_path = os.environ['MODEL_PATH']\n# model_name_or_path = \"tencent/Hunyuan-A13B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\",trust_remote_code=True)  # You may want to use bfloat16 and/or move to GPU here\nmessages = [\n    {\"role\": \"user\", \"content\": \"Write a short summary of the benefits of regular exercise\"},\n]\n\ntext = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            enable_thinking=True\n            )\n\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\nmodel_inputs.pop(\"token_type_ids\", None)\noutputs = model.generate(**model_inputs, max_new_tokens=4096)\n\n\noutput_text = tokenizer.decode(outputs[0])\n\nthink_pattern = r'<think>(.*?)</think>'\nthink_matches = re.findall(think_pattern, output_text, re.DOTALL)\n\nanswer_pattern = r'<answer>(.*?)</answer>'\nanswer_matches = re.findall(answer_pattern, output_text, re.DOTALL)\n\nthink_content = [match.strip() for match in think_matches][0]\nanswer_content = [match.strip() for match in answer_matches][0]\nprint(f\"thinking_content:{think_content}\\n\\n\")\nprint(f\"answer_content:{answer_content}\\n\\n\")\n```\n\n### Fast and slow thinking switch\n\nThis model supports two modes of operation:\n\n- Slow Thinking Mode (Default): Enables detailed internal reasoning steps before producing the final answer.\n- Fast Thinking Mode: Skips the internal reasoning process for faster inference, going straight to the final answer.\n\n**Switching to Fast Thinking Mode:**\n\nTo disable the reasoning process, set `enable_thinking=False` in the apply_chat_template call:\n```\n\ntext = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            enable_thinking=False\n            )\n```                                           \n\n\n\n## Deployment   \n\nFor deployment, you can use frameworks such as **TensorRT-LLM**, **vLLM**, or **SGLang** to serve the model and create an OpenAI-compatible API endpoint.\n\nimage: https://hub.docker.com/r/hunyuaninfer/hunyuan-a13b/tags \n\n\n### TensorRT-LLM\n\n#### Docker Image \n\nWe provide a pre-built Docker image based on the latest version of TensorRT-LLM.\n\n- To Get Started, Download the Docker Image:\n\n**From Docker Hub:**\n```\ndocker pull hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\n**From China Mirror(Thanks to [CNB](https://cnb.cool/ \"CNB.cool\")):**\n\n\nFirst, pull the image from CNB:\n``` \ndocker pull docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\nThen, rename the image to better align with the following scripts:\n```\n\ndocker tag docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-trtllm hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\n\n- start docker \n\n```\ndocker run --name hunyuanLLM_infer --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\n- Prepare Configuration file:\n\n```\ncat >/path/to/extra-llm-api-config.yml <<EOF\nuse_cuda_graph: true\ncuda_graph_padding_enabled: true\ncuda_graph_batch_sizes:\n- 1\n- 2\n- 4\n- 8\n- 16\n- 32\nprint_iter_log: true\nEOF\n```\n\n\n- Start the API server:\n\n\n```\ntrtllm-serve \\\n  /path/to/HunYuan-moe-A13B \\\n  --host localhost \\\n  --port 8000 \\\n  --backend pytorch \\\n  --max_batch_size 32 \\\n  --max_num_tokens 16384 \\\n  --tp_size 2 \\\n  --kv_cache_free_gpu_memory_fraction 0.6 \\\n  --trust_remote_code \\\n  --extra_llm_api_options /path/to/extra-llm-api-config.yml\n```\n\n\n### vLLM\n\n#### Inference from Docker Image\nWe provide a pre-built Docker image containing vLLM 0.8.5 with full support for this model. The official vllm release is currently under development\uff0c **note: cuda 12.4 is require for this docker**.\n\n\n- To Get Started, Download the Docker Image:\n\n**From Docker Hub:**\n```\ndocker pull hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1\n```\n\n**From China Mirror(Thanks to [CNB](https://cnb.cool/ \"CNB.cool\")):**\n\n\nFirst, pull the image from CNB:\n``` \ndocker pull docker.cnb.cool/tencent/hunyuan/hunyuan-a13b/hunyuan-infer-vllm-cuda12.4:v1\n```\n\nThen, rename the image to better align with the following scripts:\n```\ndocker tag docker.cnb.cool/tencent/hunyuan/hunyuan-a13b/hunyuan-infer-vllm-cuda12.4:v1 hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1 \n```\n\n- Download Model file: \n  - Huggingface:  will download automicly by vllm.\n  - ModelScope: `modelscope download --model Tencent-Hunyuan/Hunyuan-A13B-Instruct`\n \n\n- Start the API server:\n\nmodel download by huggingface:\n```\ndocker run --rm  --ipc=host \\\n        -v ~/.cache:/root/.cache/ \\\n        --security-opt seccomp=unconfined \\\n        --net=host \\\n        --gpus=all \\\n        -it \\\n        --entrypoint python3 hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1 \\\n        -m vllm.entrypoints.openai.api_server \\\n        --host 0.0.0.0 \\\n        --tensor-parallel-size 4 \\\n        --port 8000 \\\n        --model tencent/Hunyuan-A13B-Instruct  \\\n        --trust_remote_code\n``` \n\nmodel downloaded by modelscope:\n```\ndocker run --rm  --ipc=host \\\n        -v ~/.cache/modelscope:/root/.cache/modelscope \\\n        --security-opt seccomp=unconfined \\\n        --net=host \\\n        --gpus=all \\\n        -it \\\n        --entrypoint python3 hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1 \\\n        -m vllm.entrypoints.openai.api_server \\\n        --host 0.0.0.0 \\\n        --tensor-parallel-size 4 \\\n        --port 8000 \\\n        --model /root/.cache/modelscope/hub/models/Tencent-Hunyuan/Hunyuan-A13B-Instruct/  \\\n        --trust_remote_code\n```\n\n### Source Code\nSupport for this model has been added via  this [PR 20114](https://github.com/vllm-project/vllm/pull/20114 ) in the vLLM project, \nThis patch already been merged by community at Jul-1-2025.\n\nYou can build and run vLLM from source using code after `ecad85`.\n\n### Model Context Length Support\n\nThe Hunyuan A13B model supports a maximum context length of **256K tokens (262,144 tokens)**. However, due to GPU memory constraints on most hardware setups, the default configuration in `config.json` limits the context length to **32K tokens** to prevent out-of-memory (OOM) errors.\n\n#### Extending Context Length to 256K\n\nTo enable full 256K context support, you can manually modify the `max_position_embeddings` field in the model's `config.json` file as follows:\n\n```json\n{\n  ...\n  \"max_position_embeddings\": 262144,\n  ...\n}\n```\n\nWhen serving the model using **vLLM**, you can also explicitly set the maximum model length by adding the following flag to your server launch command:\n\n```bash\n--max-model-len 262144\n```\n\n#### Recommended Configuration for 256K Context Length\n\nThe following configuration is recommended for deploying the model with 256K context length support on systems equipped with **NVIDIA H20 GPUs (96GB VRAM)**:\n\n| Model DType    | KV-Cache Dtype | Number of Devices | Model Length |\n|----------------|----------------|--------------------|--------------|\n| `bfloat16`     | `bfloat16`     | 4                  | 262,144      |\n\n> \u26a0\ufe0f **Note:** Using FP8 quantization for KV-cache may impact generation quality. The above settings are suggested configurations for stable 256K-length service deployment.\n\n\n#### Tool Calling with vLLM\n\nTo support agent-based workflows and function calling capabilities, this model includes specialized parsing mechanisms for handling tool calls and internal reasoning steps.\n\nFor a complete working example of how to implement and use these features in an agent setting, please refer to our full agent implementation on GitHub:  \n\ud83d\udd17 [Hunyuan A13B Agent Example](https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/agent/)\n\nWhen deploying the model using **vLLM**, the following parameters can be used to configure the tool parsing behavior:\n\n| Parameter             ",
      "public": true,
      "version": "95344092-7e11-59f4-8ff5-2a66b53eac3f",
      "tool_description": null,
      "slug": "chutes-tencent-hunyuan-a13b-instruct",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-06-28T16:41:51.196210Z",
      "updated_at": "2025-08-25T17:59:15.080760Z",
      "image": {
        "image_id": "0dc9eee4-e994-5483-9a7e-4aadd1063293",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.4.10.post2b",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-22T11:39:20.521504Z",
        "build_started_at": "2025-08-22T11:39:23.591939Z",
        "build_completed_at": "2025-08-22T12:06:31.344999Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "a6000_ada",
        "h100_nvl",
        "a100",
        "l40",
        "h20",
        "a100_sxm",
        "l40s",
        "pro_6000",
        "h100_sxm",
        "h200",
        "a40",
        "a6000",
        "h100",
        "h800"
      ],
      "node_selector": {
        "exclude": [
          "b200",
          "mi300x"
        ],
        "include": null,
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 48,
        "compute_multiplier": 0.54,
        "supported_gpus": [
          "a6000_ada",
          "h100_nvl",
          "a100",
          "l40",
          "h20",
          "a100_sxm",
          "l40s",
          "pro_6000",
          "h100_sxm",
          "h200",
          "a40",
          "a6000",
          "h100",
          "h800"
        ]
      },
      "invocation_count": 298569,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.0,
            "tao": 0.0
          },
          "output": {
            "usd": 0.0,
            "tao": 0.0
          }
        },
        "usd": {
          "hour": 0.0,
          "second": 0.0
        },
        "tao": {
          "hour": 0.0,
          "second": 0.0
        }
      },
      "instances": [
        {
          "instance_id": "12dfdebc-0990-4fcb-bd2a-216ca50d3aa6",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:18:10.410693Z"
        },
        {
          "instance_id": "d81be2ab-4456-4577-88c4-1c955db986b7",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:18:14.983978Z"
        },
        {
          "instance_id": "54c5a173-336c-4feb-b059-875c7966a36d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:31:32.010189Z"
        },
        {
          "instance_id": "1d74d5aa-00e7-49c4-956b-f9033ed28e22",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:31:32.010189Z"
        },
        {
          "instance_id": "ea0e1b5a-a11d-4c75-bee0-26be37f43933",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:31:32.010189Z"
        },
        {
          "instance_id": "7c4084a6-fe8e-4041-ae37-782f6cf9e957",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:14:45.483042Z"
        },
        {
          "instance_id": "fbb306a3-f34e-4e90-b324-a4af6b97b201",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:16:48.310200Z"
        },
        {
          "instance_id": "0f004774-65e2-4ef7-890a-8a93ae083d6c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:31:32.010189Z"
        },
        {
          "instance_id": "20d01d7f-e2fd-4cd6-b2fb-075136653dc5",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:31:32.010189Z"
        }
      ],
      "logo_id": "4051d24c-79c3-4a89-82e8-64266bf58d3d",
      "openrouter": false,
      "chutes_version": "0.3.22",
      "revision": "202c9758065873e0ac7c80211e6275593f165442",
      "concurrency": 24,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/4051d24c-79c3-4a89-82e8-64266bf58d3d.webp",
      "hot": true
    },
    {
      "chute_id": "07fc32f5-a63d-57e4-a86d-c167570b53e1",
      "name": "NousResearch/DeepHermes-3-Mistral-24B-Preview",
      "tagline": "DeepHermes 3 is a 24B parameter language model that uniquely combines traditional LLM responses with long chain-of-thought reasoning capabilities, making it particularly useful for complex problem-solving, detailed analysis, and general-purpose tasks like conversation and coding.",
      "readme": "# DeepHermes 3 - Mistral 24B Preview\n\n![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/nZFJYtN7DvuyP7JQdfAMO.jpeg)\n\n## Model Description\n\nDeepHermes 3 Preview is the latest version of our flagship Hermes series of LLMs by Nous Research, and one of the first models in the world to unify Reasoning (long chains of thought that improve answer accuracy) and normal LLM response modes into one model. We have also improved LLM annotation, judgement, and function calling.\n\nDeepHermes 3 Preview is a hybrid reasoning model, and one of the first LLM models to unify both \"intuitive\", traditional mode responses and **long chain of thought reasoning** responses into a single model, toggled by a system prompt.\n\nHermes 3, the predecessor of DeepHermes 3, is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nThe ethos of the Hermes series of models is focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\n*This is a preview Hermes with early reasoning capabilities, distilled from R1 across a variety of tasks that benefit from reasoning and objectivity. Some quirks may be discovered! Please let us know any interesting findings or issues you discover!*\n\n## Note: To toggle REASONING ON, you must use the following system prompt:\n```\nYou are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\n```  \n\n# Nous API\n\nThis model is also available on our new API product - Check out the API and sign up for the waitlist here:\nhttps://portal.nousresearch.com/\n\n# Benchmarks:\n\nComparisons between Reasoning mode ON and OFF:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/Qcemsj4-7Tf2a7SK-Nxg5.png)\n\nBenchmarks of Non-Reasoning mode on Traditional Benchmarks against Mistral-Small-24B-Instruct:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/uNC__jwFG4dtWXed6pCSr.png)\n\n# Example Outputs:\n\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/_giUevm1IjPFWiypG0zd4.png)\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/bAI0HG2cFA_o1hTFIfCr_.png)\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/FmOIB7fjXKVHfs94DJPwn.png)\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/tfL1jeGXvv7xTAULFQgqs.png)\n\n# Prompt Format\n\nDeepHermes 3 now uses Llama-Chat format as the prompt format, opening up a more unified, structured system for engaging the LLM in multi-turn chat dialogue.\n\nSystem prompts allow steerability and interesting new ways to interact with an LLM, guiding rules, roles, and stylistic choices of the model.\n\n## Deep Thinking Mode - Deep Hermes Preview can activate long chain of thought with a system prompt.\n\n```\nYou are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\n```\n\nFor an example of using deep reasoning mode with HuggingFace Transformers:\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport flash_attn\nimport time\n\ntokenizer = AutoTokenizer.from_pretrained(\"NousResearch/DeepHermes-3-Mistral-24B-Preview\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"NousResearch/DeepHermes-3-Mistral-24B-Preview\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is y if y=2*2-4+(3*2)\"\n    }\n]\n\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors='pt').to(\"cuda\")\ngenerated_ids = model.generate(input_ids, max_new_tokens=2500, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)\nprint(f\"Generated Tokens: {generated_ids.shape[-1:]}\")\nresponse = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_space=True)\nprint(f\"Response: {response}\")\n```\n\nPlease note, for difficult problems DeepHermes can think using as many as 13,000 tokens. You may need to increase `max_new_tokens` to be much larger than 2500 for difficult problems.\n\n## Standard \"Intuitive\" Response Mode\n\nPrompt with system instruction (Use whatever system prompt you like, this is just an example!):\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport flash_attn\nimport time\n\ntokenizer = AutoTokenizer.from_pretrained(\"NousResearch/DeepHermes-3-Mistral-24B-Preview\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"NousResearch/DeepHermes-3-Mistral-24B-Preview\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are Hermes, an AI assistant\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What are the most interesting things to do in Paris?\"\n    }\n]\n\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors='pt').to(\"cuda\")\ngenerated_ids = model.generate(input_ids, max_new_tokens=2500, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)\nprint(f\"Generated Tokens: {generated_ids.shape[-1:]}\")\nresponse = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_space=True)\nprint(f\"Response: {response}\")\n```\n\n## VLLM Inference\n\nYou can also run this model with vLLM, by running the following in your terminal after `pip install vllm`\n\n`vllm serve NousResearch/DeepHermes-3-Mistral-24B-Preview`\n\nYou may then use the model over API using the OpenAI library just like you would call OpenAI's API.\n\n## Prompt Format for Function Calling\n\nOur model was trained on specific system prompts and structures for Function Calling. \n\nYou should use the system role with this message, followed by a function signature json as this example shows here.\n```\n<|start_header_id|>system<|end_header_id|>\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> {\"type\": \"function\", \"function\": {\"name\": \"get_stock_fundamentals\", \"description\": \"get_stock_fundamentals(symbol: str) -> dict - Get fundamental data for a given stock symbol using yfinance API.\\\\n\\\\n    Args:\\\\n        symbol (str): The stock symbol.\\\\n\\\\n    Returns:\\\\n        dict: A dictionary containing fundamental data.\\\\n            Keys:\\\\n                - \\'symbol\\': The stock symbol.\\\\n                - \\'company_name\\': The long name of the company.\\\\n                - \\'sector\\': The sector to which the company belongs.\\\\n                - \\'industry\\': The industry to which the company belongs.\\\\n                - \\'market_cap\\': The market capitalization of the company.\\\\n                - \\'pe_ratio\\': The forward price-to-earnings ratio.\\\\n                - \\'pb_ratio\\': The price-to-book ratio.\\\\n                - \\'dividend_yield\\': The dividend yield.\\\\n                - \\'eps\\': The trailing earnings per share.\\\\n                - \\'beta\\': The beta value of the stock.\\\\n                - \\'52_week_high\\': The 52-week high price of the stock.\\\\n                - \\'52_week_low\\': The 52-week low price of the stock.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"symbol\": {\"type\": \"string\"}}, \"required\": [\"symbol\"]}}}  </tools> Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{\"arguments\": <args-dict>, \"name\": <function-name>}\n</tool_call><|eot_id|><|start_header_id|>user<|end_header_id|>\n```\n\nTo complete the function call, create a user prompt that follows the above system prompt, like so:\n```\nFetch the stock fundamentals data for Tesla (TSLA)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n```\n\nThe model will then generate a tool call, which your inference code must parse, and plug into a function (see example inference code here: https://github.com/NousResearch/Hermes-Function-Calling):\n```\n<tool_call>\n{\"arguments\": {\"symbol\": \"TSLA\"}, \"name\": \"get_stock_fundamentals\"}\n</tool_call><|eot_id|><|start_header_id|>tool<|end_header_id|>\n```\n\nOnce you parse the tool call, call the api and get the returned values for the call, and pass it back in as a new role, `tool` like so:\n```\n<tool_response>\n{\"name\": \"get_stock_fundamentals\", \"content\": {'symbol': 'TSLA', 'company_name': 'Tesla, Inc.', 'sector': 'Consumer Cyclical', 'industry': 'Auto Manufacturers', 'market_cap': 611384164352, 'pe_ratio': 49.604652, 'pb_ratio': 9.762013, 'dividend_yield': None, 'eps': 4.3, 'beta': 2.427, '52_week_high': 299.29, '52_week_low': 152.37}}\n</tool_response>\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n```\n\nThe assistant will then read in that data from the function's response, and generate a natural language response:\n```\nThe stock fundamentals data for Tesla (TSLA) are as follows:\n- **Symbol**: TSLA\n- **Company Name**: Tesla, Inc.\n- **Sector**: Consumer Cyclical\n- **Industry**: Auto Manufacturers\n- **Market Capitalization**: $566,160,130,480\n- **Forward Price-to-Earnings Ratio (PE Ratio)**: 42.73\n- **Price-to-Book Ratio (PB Ratio)**: 9.04\n- **Dividend Yield**: N/A\n- **Trailing Earnings Per Share (EPS)**: $4.3\n- **Beta Value of the Stock**: 2.42\n- **52-Week High Price of the Stock**: $299.29\n- **52-Week Low Price of the Stock**: $152.37\n\nThis information provides a snapshot of Tesla's financial position and performance based on the fundamental data obtained from the yfinance API. It shows that Tesla has a substantial market capitalization and a relatively high P/E and P/B ratio compared to other stocks in its industry. The company does not pay a dividend at the moment, which is reflected by a 'Dividend Yield' of 'None'. The Beta value indicates that Tesla's stock has a moderate level of volatility relative to the market. The 52-week high and low prices give an idea of the stock's range over the past year. This data can be useful when assessing investment opportunities and making investment decisions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n```\n\n\n## Prompt Format for JSON Mode / Structured Outputs\n\nOur model was also trained on a specific system prompt for Structured Outputs, which should respond with **only** a json object response, in a specific json schema.\n\nYour schema can be made from a pydantic object using our codebase, with the standalone script `jsonmode.py` available here: https://github.com/NousResearch/Hermes-Function-Calling/tree/main\n\n```\n<|start_header_id|>system<|end_header_id|>\nYou are a helpful assistant that answers in JSON. Here's the json schema you must adhere to:\\n<schema>\\n{schema}\\n</schema><|eot_id|>\n```\n\nGiven the {schema} that you provide, it should follow the format of that json to create its response, all you have to do is give a typical user prompt, and it will respond in JSON.\n\n\n## Inference Code for Function Calling:\n\nAll code for utilizing, parsing, and building function calling templates is available on our github:\n[https://github.com/NousResearch/Hermes-Function-Calling](https://github.com/NousResearch/Hermes-Function-Calling)\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/oi4CiGh50xmoviUQnh8R3.png)\n\n\n## Quantized Versions:\n\nGGUF Quants: https://huggingface.co/NousResearch/DeepHermes-3-Mistral-24B-Preview-GGUF\n\n# How to cite:\n\n```bibtext\n@misc{\n      title={DeepHermes 3 Preview}, \n      author={Teknium and Roger Jin and Chen Guang and Jai Suphavadeeprasit and Jeffrey Quesnelle},\n      year={2025}\n}\n```",
      "public": true,
      "version": "8983de09-650f-59be-947e-0f31f2869e7c",
      "tool_description": null,
      "slug": "chutes-nousresearch-deephermes-3-mistral-24b-preview",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "bcc7eb3b-d9ea-5892-b6ce-97ae9dc8e825",
      "created_at": "2025-05-08T08:22:24.723375Z",
      "updated_at": "2025-08-03T07:21:59.501991Z",
      "image": {
        "image_id": "a395302b-270f-5398-9540-8c29dea16f3d",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.4.10.post1",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-01T09:18:36.513090Z",
        "build_started_at": "2025-08-01T09:18:38.000857Z",
        "build_completed_at": "2025-08-01T09:42:51.462771Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "h100_nvl",
        "h100_sxm",
        "h200",
        "h100",
        "h800"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "h200",
          "h100",
          "h100_sxm",
          "h100_nvl",
          "h800"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 2.8,
        "supported_gpus": [
          "h100_nvl",
          "h100_sxm",
          "h200",
          "h100",
          "h800"
        ]
      },
      "invocation_count": 297866,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.09329544,
            "tao": 0.0002747771413121559
          },
          "output": {
            "usd": 0.37336319999999995,
            "tao": 0.0010996429489711257
          }
        },
        "usd": {
          "hour": 10.08,
          "second": 0.0028
        },
        "tao": {
          "hour": 0.02968798458345372,
          "second": 8.2466623842927e-06
        }
      },
      "instances": [
        {
          "instance_id": "23b81c6a-0430-4108-99a8-1c2280e7153d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:16:49.951996Z"
        },
        {
          "instance_id": "ac6f1825-004e-474e-bba9-c8664a98dbb3",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:48:20.735872Z"
        },
        {
          "instance_id": "4931c5a5-1de0-4424-9b23-07e7e6ec5db4",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:48:20.735872Z"
        },
        {
          "instance_id": "b5706819-0537-44ed-89d8-62cde6b636b1",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:48:20.735872Z"
        },
        {
          "instance_id": "b0742887-295c-4616-9db7-dffec6daa49c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:48:20.735872Z"
        }
      ],
      "logo_id": "70a9787c-556a-43c5-9dc1-e4f1ef180811",
      "openrouter": true,
      "chutes_version": "0.3.11",
      "revision": "48072dc6c0594a3198eb862c13613c4ab1119009",
      "concurrency": 32,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/70a9787c-556a-43c5-9dc1-e4f1ef180811.webp",
      "hot": true
    },
    {
      "chute_id": "84bd3eb5-e262-5eb6-92c0-1b74a7f86ad7",
      "name": "zai-org/GLM-4-32B-0414",
      "tagline": "GLM-4-32B-0414 is a 32 billion parameter open-source language model that excels at code generation, web design, and reasoning tasks, with performance comparable to GPT-4 and DeepSeek models across various benchmarks.",
      "readme": "\n# GLM-4-32B-0414\n\n## Introduction\n\nThe GLM family welcomes new members, the **GLM-4-32B-0414** series models, featuring 32 billion parameters. Its performance is comparable to OpenAI\u2019s GPT series and DeepSeek\u2019s V3/R1 series. It also supports very user-friendly local deployment features. GLM-4-32B-Base-0414 was pre-trained on 15T of high-quality data, including substantial reasoning-type synthetic data. This lays the foundation for subsequent reinforcement learning extensions. In the post-training stage, we employed human preference alignment for dialogue scenarios. Additionally, using techniques like rejection sampling and reinforcement learning, we enhanced the model\u2019s performance in instruction following, engineering code, and function calling, thus strengthening the atomic capabilities required for agent tasks. GLM-4-32B-0414 achieves good results in engineering code, Artifact generation, function calling, search-based Q&A, and report generation. In particular, on several benchmarks, such as code generation or specific Q&A tasks, GLM-4-32B-Base-0414 achieves comparable performance with those larger models like GPT-4o and DeepSeek-V3-0324 (671B).\n\n**GLM-Z1-32B-0414** is a reasoning model with deep thinking capabilities. This was developed based on GLM-4-32B-0414 through cold start, extended reinforcement learning, and further training on tasks including mathematics, code, and logic. Compared to the base model, GLM-Z1-32B-0414 significantly improves mathematical abilities and the capability to solve complex tasks. During training, we also introduced general reinforcement learning based on pairwise ranking feedback, which enhances the model's general capabilities.\n\n**GLM-Z1-Rumination-32B-0414** is a deep reasoning model with rumination capabilities (against OpenAI's Deep Research). Unlike typical deep thinking models, the rumination model is capable of deeper and longer thinking to solve more open-ended and complex problems (e.g., writing a comparative analysis of AI development in two cities and their future development plans). Z1-Rumination is trained through scaling end-to-end reinforcement learning with responses graded by the ground truth answers or rubrics and can make use of search tools during its deep thinking process to handle complex tasks. The model shows significant improvements in research-style writing and complex  tasks.\n\nFinally, **GLM-Z1-9B-0414** is a surprise. We employed all the aforementioned techniques to train a small model (9B). GLM-Z1-9B-0414  exhibits excellent capabilities in mathematical reasoning and general tasks. Its overall performance is top-ranked among all open-source models of the same size. Especially in resource-constrained scenarios, this model achieves an excellent balance between efficiency and effectiveness, providing a powerful option for users seeking lightweight deployment.\n\n## Showcase\n\n### Animation Generation\n\n<table>\n  <tr>\n    <td style=\"text-align: center; font-size: 16px; font-weight: bold; padding: 10px; width: 420px;\">\n      GLM-Z1-32B-0414\n    </td>\n    <td style=\"text-align: center; font-size: 16px; font-weight: bold; padding: 10px; width: 420px;\">\n      GLM-4-32B-0414\n    </td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: top; padding: 10px; width: 420px;\">\n      <video src=\"https://github.com/user-attachments/assets/849ff9fd-b54d-4c74-9ee5-3412e1a09e32\"\n             style=\"width: 400px; height: 300px; object-fit: contain;\" autoplay loop muted playsinline></video>\n      <div style=\"margin-top: 10px; font-size: 14px; color: #333; width: 400px;\">\n        write a Python program that shows a ball bouncing inside a spinning hexagon. The ball should be affected by gravity and friction, and it must bounce off the rotating walls realistically\n      </div>\n    </td>\n    <td style=\"vertical-align: top; padding: 10px; width: 420px;\">\n      <video src=\"https://github.com/user-attachments/assets/8dccdb9d-cc44-4732-b438-74a4e3cb9dfb\"\n             style=\"width: 400px; height: 300px; object-fit: contain;\" autoplay loop muted playsinline></video>\n      <div style=\"margin-top: 10px; font-size: 14px; color: #333; width: 400px;\">\n         Use HTML to simulate the scenario of a small ball released from the center of a rotating hexagon. Consider the collision between the ball and the hexagon's edges, the gravity acting on the ball, and assume all collisions are perfectly elastic. (Prompt translated from Chinese)\n      </div>\n    </td>\n  </tr>\n</table>\n\n### Web Design\n\n<table>\n  <tr>\n    <td style=\"text-align: center; font-size: 16px; font-weight: bold; padding: 10px; width: 420px;\">\n      GLM-4-32B-0414\n    </td>\n    <td style=\"text-align: center; font-size: 16px; font-weight: bold; padding: 10px; width: 420px;\">\n      GLM-4-32B-0414\n    </td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: top; padding: 10px; width: 420px;\">\n      <img src=\"https://github.com/user-attachments/assets/bd9c1fc1-c784-4e8f-9c76-5f7389a715f1\"/>\n      <div style=\"margin-top: 10px; font-size: 14px; color: #333; width: 400px;\">\n          Design a drawing board that supports custom function plotting, allowing adding and deleting custom functions, and assigning colors to functions. (Prompt translated from Chinese)\n      </div>\n    </td>\n    <td style=\"vertical-align: top; padding: 10px; width: 420px;\">\n      <img src=\"https://github.com/user-attachments/assets/7ad12d52-9229-4278-8d1b-ffbf43e99070\"/>\n      <div style=\"margin-top: 10px; font-size: 14px; color: #333; width: 400px;\"> Design a UI for a mobile machine learning platform, which should include interfaces for training tasks, storage management, and personal statistics. The personal statistics interface should use charts to display the user's resource usage over a period. Use Tailwind CSS to style the page, and display these 3 mobile interfaces tiled on a single HTML page. (Prompt translated from Chinese) </div>\n    </td>\n  </tr>\n</table>\n\n### SVG Generation\n\n<table>\n  <tr>\n    <td style=\"text-align: center; font-size: 16px; font-weight: bold; padding: 10px; width: 420px;\">\n      GLM-4-32B-0414\n    </td>\n    <td style=\"text-align: center; font-size: 16px; font-weight: bold; padding: 10px; width: 420px;\">\n      GLM-4-32B-0414\n    </td>\n  </tr>\n  <tr>\n    <td style=\"vertical-align: top; padding: 10px; width: 420px;\">\n      <img src=\"https://github.com/user-attachments/assets/9407e4c1-1876-4ab5-838c-839836fb418a\"/>\n      <div style=\"margin-top: 10px; font-size: 14px; color: #333; width: 400px;\">\n          Create a misty Jiangnan scene using SVG. (Prompt translated from Chinese)\n      </div>\n    </td>\n    <td style=\"vertical-align: top; padding: 10px; width: 420px;\">\n      <img src=\"https://github.com/user-attachments/assets/bcce8c5a-cedf-45c8-b666-ddb023d5b49c\"/>\n      <div style=\"margin-top: 10px; font-size: 14px; color: #333; width: 400px;\"> Use SVG to illustrate the training process of an LLM. (Prompt translated from Chinese) </div>\n    </td>\n  </tr>\n</table>\n\n\n### Search-Based Writing\n\nFor search-based writing tasks, we use the following system prompt to have the model respond based on search results:\n\n```\n\u8bf7\u6839\u636e\u6240\u7ed9\u641c\u7d22\u8fd4\u56de\u7ed3\u679c\u5bf9\u7528\u6237\u95ee\u9898\u8fdb\u884c\u4f5c\u7b54\u3002\n\n## \u6ce8\u610f\n1. \u5145\u5206\u5229\u7528\u548c\u6574\u7406\u6536\u96c6\u5230\u7684\u4fe1\u606f\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u590d\u5236\u7c98\u8d34\uff0c\u751f\u6210\u7b26\u5408\u7528\u6237\u8981\u6c42\u4e14\u6709\u6df1\u5ea6\u7684\u4e13\u4e1a\u7b54\u6848\u3002\n2. \u6240\u63d0\u4f9b\u4fe1\u606f\u5145\u5206\u7684\u60c5\u51b5\u4e0b\uff0c\u4f60\u7684\u56de\u7b54\u9700\u5c3d\u53ef\u80fd\u5ef6\u957f\uff0c\u4ece\u7528\u6237\u610f\u56fe\u89d2\u5ea6\u51fa\u53d1\uff0c\u63d0\u4f9b\u5177\u6709\u8db3\u591f\u4fe1\u606f\u91cf\u548c\u591a\u89d2\u5ea6\u7684\u56de\u590d\u3002\n3. \u53e6\u5916\uff0c\u5e76\u975e\u6240\u6709\u7684\u641c\u7d22\u7ed3\u679c\u90fd\u4e0e\u7528\u6237\u95ee\u9898\u5bc6\u5207\u76f8\u5173\uff0c\u8bf7\u4ed4\u7ec6\u7684\u7504\u522b\u3001\u7b5b\u9009\u548c\u5229\u7528\u3002\n4. \u5ba2\u89c2\u7c7b\u95ee\u7b54\u7684\u7b54\u6848\u901a\u5e38\u975e\u5e38\u7b80\u77ed\uff0c\u4f60\u53ef\u4ee5\u9002\u5f53\u8865\u5145\u4e00\u5230\u4e24\u53e5\u76f8\u5173\u4fe1\u606f\uff0c\u4ee5\u4e30\u5bcc\u5185\u5bb9\u3002\n5. \u8bf7\u786e\u4fdd\u4f60\u7684\u56de\u590d\u683c\u5f0f\u7f8e\u89c2\u3001\u53ef\u8bfb\u6027\u5f3a\u3002\u5bf9\u4e8e\u591a\u5b9e\u4f53\u5bf9\u6bd4\u6216\u5217\u4e3e\uff0c\u5584\u7528\u5217\u8868\u683c\u5f0f\u6765\u5e2e\u52a9\u7528\u6237\u66f4\u597d\u7684\u7406\u89e3\u4fe1\u606f\u3002\n6. \u9664\u975e\u7528\u6237\u8981\u6c42\uff0c\u5426\u5219\u4f60\u56de\u7b54\u7684\u8bed\u8a00\u8bf7\u4e8e\u7528\u6237\u63d0\u95ee\u8bed\u8a00\u4fdd\u6301\u4e00\u81f4\u3002\n7. \u5728\u9002\u5f53\u60c5\u51b5\u4e0b\u5728\u53e5\u5b50\u672b\u5c3e\u4f7f\u7528\u4f8b\u5982:\u30100\u2020source\u3011\u7684\u683c\u5f0f\u5f15\u7528\u641c\u7d22\u7ed3\u679c\u3002\n```\n\nWhen using, you can obtain search results through methods such as `RAG` or `WebSearch`, and wrap them in `observation`, for example:\n\n```json\n[\n    {\n        \"role\": \"user\",\n        \"content\": \"Explore the common characteristics of children's literature, with a focus on its narrative techniques and thematic tendencies. This includes narrative techniques: common approaches in children's literature such as first-person, third-person, omniscient narrator, and interactive narration, and their influence on young readers. It also includes thematic tendencies: recurring themes in children's literature such as growth, adventure, friendship, and family, with an analysis of how these themes impact children's cognitive and emotional development. Additionally, other universal features such as the use of personification, repetitive language, symbolism and metaphor, and educational value should be considered. Please provide a detailed analytical report based on academic research, classic examples of children's literature, and expert opinions.\"\n    },\n    {\n        \"role\": \"observation\",\n        \"content\": \"\u3010{id}\u2020{title}\u2020{url}\u3011\\n{content}\"\n    },\n    ...\n]\n```\nFor the above prompt, we use an internal or external search model to obtain the search results. Using the format shown above, we can generate the following analysis report:\n\n<div style=\"height: 400px; width: 100%; overflow: auto; border: 5px solid #ddd; padding: 20px;\">\n\n# Analysis Report on Common Characteristics of Children's Literature\n\nChildren's literature, as a literary genre specifically created for children, possesses unique artistic features and expressive techniques. This report will comprehensively analyze the common characteristics of children's literature from three dimensions: narrative methods, thematic tendencies, and other universal features, integrating academic research, classic examples of children's literature, and expert viewpoints.\n\n## Narrative Methods and Their Impact on Child Readers\n\nThe narrative methods of children's literature exhibit diverse characteristics, including first-person, third-person, narration, and interactive storytelling. These different narrative perspectives and methods profoundly influence children's reading experiences and cognitive development.\n\n**First-person narration** is common in children's literature, unfolding the story from the perspective of a child protagonist. This narrative style bridges the gap between the reader and the story, making it easier for child readers to identify with the characters and develop emotional resonance. For example, many coming-of-age novels use first-person narration, allowing the protagonist to directly recount their experiences and feelings, making readers feel as though they are experiencing the joys and sorrows of growth alongside the protagonist. This narrative perspective lends the work a strong sense of authenticity and intimacy, helping to cultivate children's empathy\u30101\u2020source\u3011.\n\n**Third-person narration** offers a broader perspective, allowing the author to flexibly switch between different characters' viewpoints and present richer layers of the story. In children's literature, third-person omniscient narration enables the author to control the narrative pace, revealing or concealing information as needed to guide children's attention. At the same time, third-person narration facilitates direct dialogue between the author and the reader, conveying values or explaining complex concepts through narration. This narrative method positively influences children's macro-thinking and comprehensive understanding\u30101\u2020source\u3011.\n\n**Narration (authorial intrusion)** is a unique narrative technique in children's literature, where the author directly appears as the \"storyteller,\" explaining the background, commenting on characters, or posing questions to the reader. This technique is particularly common in classic fairy tales, such as the opening lines of *Andersen's Fairy Tales*: \"Once, there was a child...\" Narration helps children understand the story's context, fills cognitive gaps, and conveys the author's educational intent. Research shows that appropriate authorial intrusion aids children in grasping the story's structure and improving reading comprehension\u30105\u2020source\u3011.\n\n**Interactive storytelling** is a new trend in contemporary children's literature, especially prominent in the digital media era. Interactive storytelling breaks the traditional unidirectional author-reader relationship, encouraging child readers to participate in the story's creation, such as by choosing plot directions, character dialogues, or endings. This participatory reading enhances children's sense of agency and fosters decision-making skills and creative thinking. For example, some children's reading apps incorporate interactive elements, allowing children to influence the story's development through clicks, drag-and-drop actions, and other operations, thereby gaining a stronger sense of immersion and achievement\u30106\u2020source\u3011. Interactive storytelling transforms children from passive information recipients into active meaning-makers, uniquely contributing to the development of their subjectivity.\n\n*Table: Common Narrative Methods in Children's Literature and Their Effects*\n\n| **Narrative Method** | **Characteristics** | **Impact on Child Readers** | **Classic Examples** |\n|----------------------|--------------------|----------------------------|---------------------|\n| **First-Person** | Told from the child protagonist's perspective | Enhances immersion, fosters empathy | *Charlotte's Web*, *The Straw House* |\n| **Third-Person** | Omniscient or limited perspective | Expands horizons, develops comprehensive understanding | *Harry Potter* series |\n| **Narration** | Direct authorial intrusion into the narrative | Aids comprehension, conveys values | *Andersen's Fairy Tales* |\n| **Interactive** | Encourages reader participation in creation | Cultivates agency and creative thinking | Children's interactive reading apps |\n\nNotably, the narrative methods of children's literature are often closely intertwined with the **childhood perspective**. The childhood perspective does not necessarily mean the narrator must be a child but refers to the work's ability to describe the world to the greatest extent from a child's heart, expressing their inner psychology and external circumstances\u30102\u2020source\u3011. Through the childhood perspective, readers can embark on a spiritual journey with a child's mindset, a narrative strategy that creates a strong sense of realism, allowing child readers to achieve emotional identification and cognitive resonance during the reading process\u30101\u2020source\u3011. The use of the childhood perspective gives the work's language a perceptual and naive quality, often with a prose-like and spatial structure, artistic features that align with children's cognitive characteristics and aid their acceptance and understanding\u30102\u2020source\u3011.\n\n## Thematic Tendencies and Their Impact on Children's Cognitive and Emotional Development\n\nThe thematic choices in children's literature exhibit distinct tendencies, with common themes including growth, adventure, friendship, and family. These themes not only form the core content of children's literature but also subtly influence children's cognitive development and emotional shaping.\n\n**The theme of growth** is one of the central motifs in children's literature. Growth narratives are regarded as the artistic lifeblood of children's literature, focusing on depicting the pivotal moments of rapid psychological development in children, particularly the awakening and establishment of self-awareness\u30103\u2020source\u3011. Growth literature typically includes three elements: an artistic portrayal of the self-awareness construction process in growing adolescents, a developmental story with logical propulsion, and the presentation of the protagonist's spiritual trials and quest for direction\u30103\u2020source\u3011. By reading growth-themed works, child readers can indirectly experience the confusion and breakthroughs of growing up and understand the formation of self-identity. Classics such as Astrid Lindgren's *Pippi Longstocking* and Cao Wenxuan's *The Straw House* vividly depict children's psychological growth trajectories in specific environments. Research indicates that growth-themed literary works help children build a positive self-concept and develop the courage and resilience to face challenges, positively contributing to their psychological development\u30109\u2020source\u3011.\n\n**The theme of adventure** holds",
      "public": true,
      "version": "aa1bb60d-4dbc-5b01-987d-783eb7647958",
      "tool_description": null,
      "slug": "chutes-zai-org-glm-4-32b-0414",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "bcc7eb3b-d9ea-5892-b6ce-97ae9dc8e825",
      "created_at": "2025-08-04T12:38:36.161913Z",
      "updated_at": "2025-08-07T11:08:40.544675Z",
      "image": {
        "image_id": "dd30a5e7-24ae-538e-bee7-ba6a93b3d2f0",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.4.9.post6",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-06T20:56:26.489668Z",
        "build_started_at": "2025-08-06T20:56:28.092938Z",
        "build_completed_at": "2025-08-06T21:28:45.347952Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "a6000_ada",
        "h100_nvl",
        "a100",
        "l40",
        "a100_40gb_sxm",
        "a100_40gb",
        "h20",
        "a100_sxm",
        "l40s",
        "pro_6000",
        "h100_sxm",
        "h200",
        "a40",
        "a6000",
        "h100",
        "h800"
      ],
      "node_selector": {
        "exclude": [
          "b200",
          "mi300x"
        ],
        "include": null,
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 40,
        "compute_multiplier": 0.54,
        "supported_gpus": [
          "a6000_ada",
          "h100_nvl",
          "a100",
          "l40",
          "a100_40gb_sxm",
          "a100_40gb",
          "h20",
          "a100_sxm",
          "l40s",
          "pro_6000",
          "h100_sxm",
          "h200",
          "a40",
          "a6000",
          "h100",
          "h800"
        ]
      },
      "invocation_count": 293418,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.01999188,
            "tao": 5.888081599546198e-05
          },
          "output": {
            "usd": 0.08000639999999999,
            "tao": 0.00023563777477952693
          }
        },
        "usd": {
          "hour": 2.16,
          "second": 0.0006000000000000001
        },
        "tao": {
          "hour": 0.006361710982168655,
          "second": 1.767141939491293e-06
        }
      },
      "instances": [
        {
          "instance_id": "9590b8e5-4e72-4627-a00f-b0004082adc2",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:47:29.912460Z"
        },
        {
          "instance_id": "7b54e1b8-16a3-43eb-8d92-3423eb8f5415",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:47:29.912460Z"
        },
        {
          "instance_id": "b0979b67-7ff5-41cc-a8bd-2d177c3bc7d8",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:47:29.912460Z"
        },
        {
          "instance_id": "6843dcbd-5736-4285-b5c4-a33e54f5907e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:47:29.912460Z"
        },
        {
          "instance_id": "251749a2-3305-4775-ab2d-4e774c56495c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:47:29.912460Z"
        }
      ],
      "logo_id": "2f38a940-fcf6-402a-8a39-a9e615269f36",
      "openrouter": false,
      "chutes_version": "0.3.19.rc0",
      "revision": "077b5c2f5c43bd3239fd605a0600229e8facbd4a",
      "concurrency": 16,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/2f38a940-fcf6-402a-8a39-a9e615269f36.webp",
      "hot": true
    },
    {
      "chute_id": "5c47e581-a06c-5483-a597-5a1b6888b0b9",
      "name": "shisa-ai/shisa-v2-llama3.3-70b",
      "tagline": "Shisa V2 is a family of bilingual Japanese-English language models ranging from 7B to 70B parameters, optimized for high-quality Japanese language capabilities while maintaining strong English performance, making them particularly useful for Japanese language tasks and Japanese-English translation.",
      "readme": "\n# Shisa V2\n\nShisa V2 is a family of bilingual Japanese and English (JA/EN) general-purpose chat models trained by [Shisa.AI](https://shisa.ai). These models aim to excel in Japanese language tasks while retaining robust English capabilities.\n\nSince our initial [Shisa 7B](https://huggingface.co/augmxnt/shisa-7b-v1) releases, the baseline Japanese capabilities of open-weight language models have significantly improved. New models have more Japanese pre-training tokens, higher [JA tokenizer efficiency](https://github.com/shisa-ai/shisa-v2/blob/main/eval/tokenizer-efficiency/tokenizer-eval-ja.md), and better quality Japanese outputs overall. As such, for Shisa V2 we've eschewed both tokenizer extension and costly continued pre-training and have focused entirely on optimizing post-training. We've significantly expanded and refined the synthetic-data driven approach that was pioneered with our original [Shisa 7B](https://huggingface.co/augmxnt/shisa-7b-v1) models, and have achieved substantial performance gains.\n\n## Model Family Overview\n\nThe Shisa V2 family comprises a range of models from 7B to 70B parameters in size:\n\n| License | Model | Parameters | Context Length | JA AVG | EN AVG |\n|----|----|----|----|----|----|\n| Apache 2.0 | [shisa-v2-qwen2.5-7b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-7b) | 7B | 128K/8K | 71.06 | 54.86 |\n| Llama 3.1 | [shisa-v2-llama3.1-8b](https://huggingface.co/shisa-ai/shisa-v2-llama3.1-8b)<sup>1</sup> | 8B | 128K | 70.83 | 54.75 |\n| Apache 2.0 | [shisa-v2-mistral-nemo-12b](https://huggingface.co/shisa-ai/shisa-v2-mistral-nemo-12b) | 12B | 128K | 72.83 | 53.33 |\n| MIT | [shisa-v2-unphi4-14b](https://huggingface.co/shisa-ai/shisa-v2-unphi4-14b) | 14B | 16K | 75.89 | 60.10 |\n| Apache 2.0 | [shisa-v2-qwen2.5-32b](https://huggingface.co/shisa-ai/shisa-v2-qwen2.5-32b) | 32B | 128K/8K | 76.97 | 67.41 |\n| Llama 3.3 | [shisa-v2-llama3.3-70b](https://huggingface.co/shisa-ai/shisa-v2-llama3.3-70b)<sup>1</sup> | 70B | 128K | 79.72 | 67.71 |\n\nThese Shisa V2 models were all trained using the same datasets and training recipes, except for scaling the learning rate based on model size and modifying the global batch size for the 70B model.\n\nWhile most of our development and tuning was done on the Llama 3.1 8B model, we did some cross-validation during this process and we're pleased that our final recipe has shown robust scaling, improving Japanese language performance across all model sizes evaluated. We've prioritized releasing the highest-quality openly-licensed (Apache 2.0 and MIT) models in each class size.\n\n## Performance\n\nAll Shisa V2 models demonstrate improved Japanese output quality compared to their respective base models:\n\n| Model                                                                                         | JA AVG    | EN AVG    | Shaberi AVG | ELYZA 100 | JA MT Bench | Rakuda  | Tengu   | llm-jp-eval | shisa-jp-ifeval | shisa-jp-rp-bench | shisa-jp-tl-bench | MixEval | LiveBench | IFEval  | EvalPlus |\n|----------------------------------------------------------------------------------------------------|-----------|-----------|-------------|-----------|-------------|---------|---------|-------------|-----------------|-------------------|-------------------|---------|-----------|---------|----------|\n| [shisa-ai/shisa-v2-llama3.3-70b](https://huggingface.co/shisa-ai/shisa-v2-llama3.3-70b)         | **79.72** | 67.71     | **8.86**     | **8.98**   | **9.03**     | **9.32** | **8.11** | 0.63        | **0.42**         | **4.72**          | **8.37**          | 0.59    | 48.7      | 0.84    | **0.79** |\n| [meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)     | 72.75     | **71.48** | 8.28        | 8.09      | 8.76        | 8.88    | 7.40    | **0.66**    | 0.35             | 4.65             | 5.75             | **0.64** | **51.8**   | **0.92** | **0.79** |\n\nThe Shisa V2 models perform well against other models in their respective class sizes.\n\n| License   | Model                                                                                  | JA AVG    | EN AVG    | Shaberi AVG | ELYZA 100 | JA MT Bench | Rakuda  | Tengu  | llm-jp-eval | shisa-jp-ifeval | shisa-jp-rp-bench | shisa-jp-tl-bench | MixEval | LiveBench | IFEval  | EvalPlus  |\n|-----------|---------------------------------------------------------------------------------------------|-----------|-----------|-------------|-----------|-------------|---------|--------|-------------|-----------------|-------------------|-------------------|---------|-----------|---------|-----------|\n| Llama 3.3 | [shisa-ai/shisa-v2-llama3.3-70b](https://huggingface.co/shisa-ai/shisa-v2-llama3.3-70b)  | **79.72** | 67.71     | **8.86**     | **8.98**   | **9.03**     | 9.32    | 8.11   | 0.63        | 0.42            | **4.72**          | **8.37**          | 0.59    | 48.7      | 0.84    | **0.79**  |\n| Qwen      | Qwen/Qwen2.5-72B-Instruct                                                                   | 77.57     | 68.12     | 8.81        | 8.97      | 8.83        | 9.23    | **8.22** | **0.67**    | **0.47**        | 4.52             | 6.39             | 0.54    | **53.8**  | 0.86    | **0.79**  |\n| Llama 3.3 | tokyotech-llm/Llama-3.3-Swallow-70B-Instruct-v0.4                                           | 75.59     | 61.03     | 8.55        | 8.34      | 8.81        | 9.15    | 7.90   | 0.66        | 0.39            | 4.55             | 6.63             | 0.50    | 41.6      | 0.80    | 0.73      |\n| Llama 3.1 | allenai/Llama-3.1-Tulu-3-70B                                                                | 74.64     | 64.48     | 8.60        | 8.31      | 8.84        | **9.36** | 7.91   | 0.65        | 0.41            | 4.70             | 5.31             | 0.54    | 42.4      | 0.86    | 0.76      |\n| Llama 3.1 | cyberagent/Llama-3.1-70B-Japanese-Instruct-2407                                            | 73.67     | 64.47     | 8.68        | 8.93      | 8.61        | 9.14    | 8.06   | 0.63        | 0.36            | 4.05             | 6.25             | 0.56    | 43.6      | 0.85    | 0.73      |\n| Llama 3.3 | meta-llama/Llama-3.3-70B-Instruct                                                           | 72.75     | **71.48** | 8.28        | 8.09      | 8.76        | 8.88    | 7.40   | 0.66        | 0.35            | 4.65             | 5.75             | **0.64** | 51.8      | **0.92** | **0.79**  |\n| Llama 3   | [shisa-ai/shisa-v1-llama3-70b](https://huggingface.co/shisa-ai/shisa-v1-llama3-70b)        | 60.63     | 52.96     | 7.73        | 7.33      | 8.06        | 8.88    | 6.65   | 0.26        | 0.24            | 4.51             | 3.51             | 0.56    | 27.4      | 0.65    | 0.63      |\n\n### Testing Notes\n\nJapanese functional tests were conducted using the **[shisa-ai/shaberi](https://github.com/shisa-ai/shaberi/)** fork of the [LightBlue Shaberi](https://github.com/lightblue-tech/japanese_llm_eval) evaluation harness. Shaberi ratings were performed with a **[PoLL](https://arxiv.org/abs/2404.18796)** (LLM Jury) consisting of:\n\n- [Athene-V2](https://huggingface.co/Nexusflow/Athene-V2-Chat)\n- [Llama 3.3 70B](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)\n- [Tulu 3 405B FP8](https://huggingface.co/shisa-ai/Llama-3.1-Tulu-3-405B-FP8-Dynamic)\n\nThe results were statistically validated to be comparable to both `gpt-4-1106-preview` and human-reviewed \"gold standard\" ratings.\n\nDynamic RoPE extension was utilized when necessary for testing models with context windows smaller than 8K tokens. All tests were performed using recent versions of [vLLM](https://github.com/vllm-project/vllm) or [SGLang](https://github.com/sgl-project/sglang).\n\nWe developed a custom \"multieval\" harness to automate our model evaluations. Standard benchmarks include:\n\n- [ELYZA Tasks 100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100)\n- [JA MT-Bench](https://github.com/Stability-AI/FastChat/tree/jp-stable/fastchat/llm_judge) ([dataset](https://huggingface.co/datasets/shisa-ai/ja-mt-bench-1shot))\n- [Rakuda](https://huggingface.co/datasets/yuzuai/rakuda-questions)\n- [Tengu Bench](https://huggingface.co/datasets/lightblue/tengu_bench)\n- [llm-jp-eval](https://github.com/llm-jp/llm-jp-eval) (v1.4.1)\n- [MixEval](https://mixeval.github.io/)\n- [LiveBench](https://livebench.ai/) (2024-11-25)\n- [IFEval](https://huggingface.co/datasets/google/IFEval) ([Lighteval](https://github.com/huggingface/lighteval))\n- [EvalPlus](https://github.com/evalplus/evalplus)\n\n### New Japanese Benchmarks\n\nOver the course of model development, we also created several new evaluations to help us measure performance on important Japanese downstream tasks:\n\n- **shisa-jp-ifeval**: Inspired by [IFEval](https://huggingface.co/datasets/google/IFEval), but evaluating instruction-following abilities specific to Japanese grammar and linguistics (closed form)\n- **shisa-jp-rp-bench**: Assessing performance on Japanese role-play and character/persona-based multi-turn conversations based on [Aratako](https://huggingface.co/Aratako)'s [Japanese-RP-Bench](https://github.com/Aratako/Japanese-RP-Bench) (LLM judge)\n- **shisa-jp-tl-bench**: Testing Japanese-English translation proficiency (LLM judge, BTL pairwise comparison with logistic transformation scoring)\n\nWe believe these benchmarks will be generally useful and plan to open-source them in the near future to support the Japanese LLM research community.\n\n## Usage\n\nAll Shisa V2 models inherit the [chat templates](https://huggingface.co/docs/transformers/v4.37.1/chat_templating) of their respective base models and have been tested and validated for proper inference with both [vLLM](https://github.com/vllm-project/vllm) and [SGLang](https://github.com/sgl-project/sglang).\n\nRunning sampler sweeps, we found the models operate well across a variety of temperatures in most settings. For translation tasks specifically, we recommend a lower temperatures (0.2) to increase accuracy. For role-play and creative tasks, a higher temp (eg 1.0) seems to give good results. To prevent cross-lingual token leakage we recommend a top_p of 0.9 or min_p of 0.1. \n\nNo additional safety alignment has been done on these models, so they will largely inherit the base models' biases and safety profiles.\n\n## Datasets\n\nOur supervised fine-tuning (SFT) stage dataset consists of approximately 360K samples totaling roughly 420M Llama 3 tokens:\n\n- [shisa-ai/shisa-v2-sharegpt](https://huggingface.co/datasets/shisa-ai/shisa-v2-sharegpt)\n  - This is a filtered, regenerated and resampled version of the original Shisa V1 [augmxnt/ultra-orca-boros-en-ja-v1](https://huggingface.co/datasets/augmxnt/ultra-orca-boros-en-ja-v1) dataset\n  - This was the backbone of our Shisa V2 training and it proved to be an extremely robust dataset, out-performing all existing mixes/additions (Tulu, Olmo, Rewild, various Magpie sets, etc.) - if you need a JA/EN dataset, we believe this new version is among the best currently available\n- [shisa-ai/rewild-set-deepseek-subset](https://huggingface.co/datasets/shisa-ai/rewild-set-deepseek-subset)\n  - A filtered version of [Rewild](https://arxiv.org/abs/2501.18511) ([WildChat](https://wildchat.allen.ai/)) prompts translated into Japanese, with responses generated by [DeepSeek-V3-0324](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324)\n- shisa-ai/magpie-ultra-set\n  - Japanese generations based on [argilla/magpie-ultra-v1.0](https://huggingface.co/datasets/argilla/magpie-ultra-v1.0)\n- shisa-ai/magpie-advanced-questions-set\n  - [Magpie](https://magpie-align.github.io/)-generated questions about advanced college-level topics across a variety of academic fields\n- shisa-ai/japan-magpie-set\n  - [Magpie](https://magpie-align.github.io/)-generated questions about Japan's economy and history as well as cultural and business practices\n- shisa-ai/shisa-v2-roleplaying-sft\n  - Synthetically-generated roleplaying data featuring a wide variety of characters, situations, and genres\n- shisa-ai/translation_expanded_master_set_filtered\n  - A synthetic dataset involving a wide range of translation tasks, including essays, conversations, and fiction\n- shisa-ai/shisa-v2-instruction-following-sft\n  - An instruction following dataset based on prompts from ([Aratako/Magpie-Tanuki-8B-annotated-96k](https://huggingface.co/datasets/Aratako/Magpie-Tanuki-8B-annotated-96k)) and a list of instruction-following constraints\n\nOur final DPO mix is 113K samples totaling approximately 115M Llama 3 tokens:\n\n- [shisa-ai/deepseekv3-ultrafeedback-armorm-dpo](https://huggingface.co/datasets/shisa-ai/deepseekv3-ultrafeedback-armorm-dpo)\n  - This is a version of [princeton-nlp/gemma2-ultrafeedback-armorm](https://huggingface.co/datasets/princeton-nlp/gemma2-ultrafeedback-armorm) with `chosen` responses regenerated by [DeepSeek-V3-0324](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324)\n  - Surprisingly, we found that using this relatively small DPO alignment set in English-only outperformed both JA/EN DPO sets and also much larger sets like the [Tulu 3 preference mixture](https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-405b-preference-mixture)\n- shisa-ai/shisa-v2-roleplaying-dpo\n  - A DPO variant of the roleplaying-sft set that uses an [UltraFeedback](https://github.com/OpenBMB/UltraFeedback)-style rating system\n- shisa-ai/translation-no-extra-text-dpo-dataset\n  - A DPO set that aims to reduce the tendency of models to output extraneous explanatory text for translations when not wanted\n- shisa-ai/shisa-v2-instruction-following-dpo\n  - A DPO variant of the instruction-following-sft set to further enhance instruction-following performance\n- shisa-ai/politeness-dpo-set\n  - A set to allow for greater controllability of speaking style for Japanese responses\n\n## Training\n\nWe trained over 200 models to empirically test a wide range of variables. Beyond hyper-parameter and data-mix testing, we also ran numerous tests on data ordering, multilingual-specific ordering, curriculum learning, multi-stage training, various forms of self-play, preference tuning, and some of the latest RL/verifiable reward techniques.\n\nA full discussion of these learnings is out of scope here, but we will be updating the [shisa-v2 wiki](https://github.com/shisa-ai/shisa-v2/wiki) and the [Shisa.AI website](http://shisa.ai/) with forthcoming writeups.\n\nMost of our training was done on a small AWS Sagemaker-deployed 4-node H100 slurm cluster. Training was mostly done with [Axolotl](https://github.com/axolotl-ai-cloud/axolotl/) with [DeepSpeed](https://www.deepspeed.ai/) and [Liger Kernels](https://github.com/linkedin/Liger-Kernel). The Phi 4 and Llama 3.3 70B versions of Shisa V2 were trained with [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF). Our training logs are [publicly available on Weights and Biases](https://wandb.ai/augmxnt/shisa-v2). \n\n### Credits\n\nThe Shisa V2 models were developed by [Leonard Lin](https://huggingface.co/leonardlin) and Adam Lensenmayer ([Shisa.AI](http://Shisa.AI)).\n\nCompute was provided by [Ubitus K.K.](https://ubitus.net/) and [METI GENIAC](https://www.meti.go.jp/policy/mono_info_service/geniac/).\n\nThanks to [Meta Llama](https://huggingface.co/meta-llama), [Microsoft Research](https://huggingface.co/microsoft), [Mistral AI](https://huggingface.co/mistralai), and [Qwen Team](https://huggingface.co/Qwen) for providing their models to the open source community, [Unsloth](https://huggingface.co/unsloth) for their [llamafied conversion of Phi-4](https://huggingface.co/unsloth/phi-4), the Tulu team, whose detailed writeups and fast responses to our questions were very helpful, and [Chanvichet Vong](https://github.com/NanoCode012) of the Axolotl team for his tireless work in the Axolotl Discord.\n\nWe also extend our thanks to all open source AI developers and researchers - without their publicly shared research, tooling, and datasets, none of our work would be possible. We hope that our own contributions will further support the broader com",
      "public": true,
      "version": "b1b51126-9fdf-58c9-a312-c729ce88d813",
      "tool_description": null,
      "slug": "chutes-shisa-ai-shisa-v2-llama3-3-70b",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "bcc7eb3b-d9ea-5892-b6ce-97ae9dc8e825",
      "created_at": "2025-04-15T08:49:19.821666Z",
      "updated_at": "2025-08-11T07:51:06.782189Z",
      "image": {
        "image_id": "9dd4f20e-f080-539f-96be-db875b4479bd",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.4.9.post6a",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-07T10:55:47.055425Z",
        "build_started_at": "2025-08-07T10:55:48.659508Z",
        "build_completed_at": "2025-08-07T11:26:41.880844Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "a6000_ada",
        "h100_nvl",
        "a100",
        "l40",
        "h20",
        "a100_sxm",
        "l40s",
        "pro_6000",
        "h100_sxm",
        "h200",
        "a40",
        "a6000",
        "h100",
        "h800"
      ],
      "node_selector": {
        "exclude": [
          "mi300x",
          "b200"
        ],
        "include": null,
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 48,
        "compute_multiplier": 0.54,
        "supported_gpus": [
          "a6000_ada",
          "h100_nvl",
          "a100",
          "l40",
          "h20",
          "a100_sxm",
          "l40s",
          "pro_6000",
          "h100_sxm",
          "h200",
          "a40",
          "a6000",
          "h100",
          "h800"
        ]
      },
      "invocation_count": 286065,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.01999188,
            "tao": 5.888081599546198e-05
          },
          "output": {
            "usd": 0.08000639999999999,
            "tao": 0.00023563777477952693
          }
        },
        "usd": {
          "hour": 2.16,
          "second": 0.0006000000000000001
        },
        "tao": {
          "hour": 0.006361710982168655,
          "second": 1.767141939491293e-06
        }
      },
      "instances": [
        {
          "instance_id": "08d7ab1e-cb1b-4984-bf73-355f40c1997c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:28:05.085847Z"
        },
        {
          "instance_id": "8ac33a27-b58b-41d9-9154-05876e079c3a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:28:05.085847Z"
        },
        {
          "instance_id": "ff472ba8-6f99-429a-8110-a4f4dc42e9c1",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:28:05.085847Z"
        },
        {
          "instance_id": "6a481bf3-2bc2-4735-a3a8-bfcc13bbe3cd",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:28:05.085847Z"
        },
        {
          "instance_id": "bd26c240-3bb7-4d7c-b748-1a909a10ca06",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:28:05.085847Z"
        },
        {
          "instance_id": "9d4df5ba-3a23-4569-9853-0f0c7f35bde5",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:28:05.085847Z"
        },
        {
          "instance_id": "7915f125-ae1e-4d11-8461-3f4d9ba41e1d",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:28:05.085847Z"
        },
        {
          "instance_id": "ae988e29-60f2-4138-b719-78da60c386aa",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:28:05.085847Z"
        },
        {
          "instance_id": "d31b22a4-b295-4d0e-a2a5-ba693bb73bd7",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:28:05.085847Z"
        },
        {
          "instance_id": "ec6b68a4-a020-473c-9243-ae00b38c89da",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:28:05.085847Z"
        }
      ],
      "logo_id": null,
      "openrouter": true,
      "chutes_version": "0.3.19.rc0",
      "revision": "eabf8d16e511a6e5544ae58b2b65f836b6614841",
      "concurrency": 16,
      "boost": null,
      "logging_enabled": false,
      "logo": null,
      "hot": true
    },
    {
      "chute_id": "1a0d9245-582a-5cde-8f5b-2da5b9542339",
      "name": "moonshotai/Kimi-K2-Instruct-0905",
      "tagline": "Kimi K2 is a 1-trillion parameter MoE language model (using 32B active parameters) optimized for coding tasks and long-context applications with a 256k token context window.",
      "readme": "<div align=\"center\">\n  <picture>\n      <img src=\"figures/kimi-logo.png\" width=\"30%\" alt=\"Kimi K2: Open Agentic Intellignece\">\n  </picture>\n</div>\n<hr>\n\n<div align=\"center\" style=\"line-height:1\">\n  <a href=\"https://www.kimi.com\" target=\"_blank\"><img alt=\"Chat\" src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-Kimi%20K2-ff6b6b?color=1783ff&logoColor=white\"/></a>\n  <a href=\"https://github.com/moonshotai/Kimi-K2\"><img alt=\"github\" src=\"https://img.shields.io/badge/\ud83e\udd16%20Github-Kimi%20K2-ff6b6b?color=1783ff&logoColor=white\"/></a>\n  <a href=\"https://www.moonshot.ai\" target=\"_blank\"><img alt=\"Homepage\" src=\"https://img.shields.io/badge/Homepage-Moonshot%20AI-white?logo=Kimi&logoColor=white\"/></a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://huggingface.co/moonshotai\" target=\"_blank\"><img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Moonshot%20AI-ffc107?color=ffc107&logoColor=white\"/></a>\n  <a href=\"https://twitter.com/kimi_moonshot\" target=\"_blank\"><img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-Kimi.ai-white?logo=x&logoColor=white\"/></a>\n    <a href=\"https://discord.gg/TYU2fdJykW\" target=\"_blank\"><img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-Kimi.ai-white?logo=discord&logoColor=white\"/></a>\n</div>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905/blob/main/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/badge/License-Modified_MIT-f5de53?&color=f5de53\"/></a>\n</div>\n\n<p align=\"center\">\n<b>\ud83d\udcf0&nbsp;&nbsp;<a href=\"https://moonshotai.github.io/Kimi-K2/\">Tech Blog</a></b> &nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp; <b>\ud83d\udcc4&nbsp;&nbsp;<a href=\"https://github.com/MoonshotAI/Kimi-K2/blob/main/tech_report.pdf\">Paper</a></b>\n</p>\n\n\n## 1. Model Introduction\n\nKimi K2-Instruct-0905 is the latest, most capable version of Kimi K2. It is a state-of-the-art mixture-of-experts (MoE) language model, featuring 32 billion activated parameters and a total of 1 trillion parameters.\n\n### Key Features\n- Enhanced agentic coding intelligence: Kimi K2-Instruct-0905 demonstrates significant improvements in performance on public benchmarks and real-world coding agent tasks.\n- Improved frontend coding experience: Kimi K2-Instruct-0905 offers advancements in both the aesthetics and practicality of frontend programming.\n- Extended context length: Kimi K2-Instruct-0905\u2019s context window has been increased from 128k to 256k tokens, providing better support for long-horizon tasks.\n\n\n## 2. Model Summary\n\n<div align=\"center\">\n\n\n| | |\n|:---:|:---:|\n| **Architecture** | Mixture-of-Experts (MoE) |\n| **Total Parameters** | 1T |\n| **Activated Parameters** | 32B |\n| **Number of Layers** (Dense layer included) | 61 |\n| **Number of Dense Layers** | 1 |\n| **Attention Hidden Dimension** | 7168 |\n| **MoE Hidden Dimension** (per Expert) | 2048 |\n| **Number of Attention Heads** | 64 |\n| **Number of Experts** | 384 |\n| **Selected Experts per Token** | 8 |\n| **Number of Shared Experts** | 1 |\n| **Vocabulary Size** | 160K |\n| **Context Length** | 256K |\n| **Attention Mechanism** | MLA |\n| **Activation Function** | SwiGLU |\n</div>\n\n## 3. Evaluation Results\n\n| Benchmark              | Metric | K2-Instruct-0905 | K2-Instruct-0711 | Qwen3-Coder-480B-A35B-Instruct    | GLM-4.5    | DeepSeek-V3.1 | Claude-Sonnet-4 | Claude-Opus-4 |\n|------------------------|--------|------------------|------------------|--------|--------|--------|-----------------|---------------|\n| SWE-Bench verified     | ACC    | 69.2 \u00b1 0.63      | 65.8             | 69.6*  | 64.2*  | 66.0*  | 72.7*            | 72.5*          |\n| SWE-Bench Multilingual | ACC    | 55.9 \u00b1 0.72      | 47.3             | 54.7*  | 52.7   | 54.5*  | 53.3*           | -             |\n| Multi-SWE-Bench        | ACC    | 33.5 \u00b1 0.28      | 31.3             | 32.7   | 31.7   | 29.0   | 35.7            | -             |\n| Terminal-Bench         | ACC    | 44.5 \u00b1 2.03      | 37.5             | 37.5*  | 39.9*  | 31.3*  | 36.4*           | 43.2*         |\n| SWE-Dev                | ACC    | 66.6 \u00b1 0.72      | 61.9             | 64.7   | 63.2   | 53.3   | 67.1            | -             |\n\n\nAll K2-Instruct-0905 numbers are reported as mean \u00b1 std over five independent, full-test-set runs.\nBefore each run we prune the repository so that every Git object unreachable from the target commit disappears; this guarantees the agent sees only the code that would legitimately be available at that point in history.\n\nExcept for Terminal-Bench (Terminus-2), every result was produced with our in-house evaluation harness. The harness is derived from SWE-agent, but we clamp the context windows of the Bash and Edit tools and rewrite the system prompt to match the task semantics. All baseline figures denoted with an asterisk (*) are excerpted directly from their official report or public leaderboard; the remaining metrics were evaluated by us under conditions identical to those used for K2-Instruct-0905.\n\nFor SWE-Dev we go one step further: we overwrite the original repository files and delete any test file that exercises the functions the agent is expected to generate, eliminating any indirect hints about the desired implementation.\n\n\n## 4. Deployment\n> [!Note]\n> You can access Kimi K2's API on https://platform.moonshot.ai , we provide OpenAI/Anthropic-compatible API for you.\n>\n> The Anthropic-compatible API maps temperature by `real_temperature = request_temperature * 0.6` for better compatible with existing applications.\n\nOur model checkpoints are stored in the block-fp8 format, you can find it on [Huggingface](https://huggingface.co/moonshotai/Kimi-K2-Instruct).\n\nCurrently, Kimi-K2 is recommended to run on the following inference engines:\n\n* vLLM\n* SGLang\n* KTransformers\n* TensorRT-LLM\n\nDeployment examples for vLLM and SGLang can be found in the [Model Deployment Guide](https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905/raw/main/docs/deploy_guidance.md).\n\n---\n\n## 5. Model Usage\n\n### Chat Completion\n\nOnce the local inference service is up, you can interact with it through the chat endpoint:\n\n```python\ndef simple_chat(client: OpenAI, model_name: str):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Kimi, an AI assistant created by Moonshot AI.\"},\n        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Please give a brief self-introduction.\"}]},\n    ]\n    response = client.chat.completions.create(\n        model=model_name,\n        messages=messages,\n        stream=False,\n        temperature=0.6,\n        max_tokens=256\n    )\n    print(response.choices[0].message.content)\n```\n\n> [!NOTE]\n> The recommended temperature for Kimi-K2-Instruct-0905 is `temperature = 0.6`.\n> If no special instructions are required, the system prompt above is a good default.\n\n---\n\n### Tool Calling\n\nKimi-K2-Instruct-0905 has strong tool-calling capabilities.\nTo enable them, you need to pass the list of available tools in each request, then the model will autonomously decide when and how to invoke them.\n\nThe following example demonstrates calling a weather tool end-to-end:\n\n```python\n# Your tool implementation\ndef get_weather(city: str) -> dict:\n    return {\"weather\": \"Sunny\"}\n# Tool schema definition\ntools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Retrieve current weather information. Call this when the user asks about the weather.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"required\": [\"city\"],\n            \"properties\": {\n                \"city\": {\n                    \"type\": \"string\",\n                    \"description\": \"Name of the city\"\n                }\n            }\n        }\n    }\n}]\n# Map tool names to their implementations\ntool_map = {\n    \"get_weather\": get_weather\n}\ndef tool_call_with_client(client: OpenAI, model_name: str):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are Kimi, an AI assistant created by Moonshot AI.\"},\n        {\"role\": \"user\", \"content\": \"What's the weather like in Beijing today? Use the tool to check.\"}\n    ]\n    finish_reason = None\n    while finish_reason is None or finish_reason == \"tool_calls\":\n        completion = client.chat.completions.create(\n            model=model_name,\n            messages=messages,\n            temperature=0.6,\n            tools=tools,          # tool list defined above\n            tool_choice=\"auto\"\n        )\n        choice = completion.choices[0]\n        finish_reason = choice.finish_reason\n        if finish_reason == \"tool_calls\":\n            messages.append(choice.message)\n            for tool_call in choice.message.tool_calls:\n                tool_call_name = tool_call.function.name\n                tool_call_arguments = json.loads(tool_call.function.arguments)\n                tool_function = tool_map[tool_call_name]\n                tool_result = tool_function(**tool_call_arguments)\n                print(\"tool_result:\", tool_result)\n                messages.append({\n                    \"role\": \"tool\",\n                    \"tool_call_id\": tool_call.id,\n                    \"name\": tool_call_name,\n                    \"content\": json.dumps(tool_result)\n                })\n    print(\"-\" * 100)\n    print(choice.message.content)\n```\n\nThe `tool_call_with_client` function implements the pipeline from user query to tool execution.\nThis pipeline requires the inference engine to support Kimi-K2\u2019s native tool-parsing logic.\nFor more information, see the [Tool Calling Guide](https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905/raw/main/docs/tool_call_guidance.md).\n\n---\n\n## 6. License\n\nBoth the code repository and the model weights are released under the [Modified MIT License](https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905/raw/main/LICENSE).\n\n---\n\n## 7. Third Party Notices\n\nSee [THIRD PARTY NOTICES](https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905/raw/main/THIRD_PARTY_NOTICES.md)\n\n---\n\n## 7. Contact Us\n\nIf you have any questions, please reach out at [support@moonshot.cn](mailto:support@moonshot.cn).\n",
      "public": true,
      "version": "016d20fd-addc-5564-84a2-4f8c69e30c1b",
      "tool_description": null,
      "slug": "chutes-moonshotai-kimi-k2-instruct-0905",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-09-05T11:50:59.976639Z",
      "updated_at": "2025-09-05T11:50:59.976639Z",
      "image": {
        "image_id": "89c8efd3-fff2-530f-a9b2-e344dcaaebe2",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.5.1.post3",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-30T09:40:05.257537Z",
        "build_started_at": "2025-08-30T09:40:08.281915Z",
        "build_completed_at": "2025-08-30T10:01:00.019895Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "b200"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "b200"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 8.0,
        "supported_gpus": [
          "b200"
        ]
      },
      "invocation_count": 276231,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.296176,
            "tao": 0.0008723083851179552
          },
          "output": {
            "usd": 1.18528,
            "tao": 0.0034909299967337323
          }
        },
        "usd": {
          "hour": 32.0,
          "second": 0.008888888888888889
        },
        "tao": {
          "hour": 0.09424757010620229,
          "second": 2.617988058505619e-05
        }
      },
      "instances": [
        {
          "instance_id": "60e26d4f-9c54-451a-abc8-5f6363392759",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:58:44.178805Z"
        },
        {
          "instance_id": "3f7ef6df-52b8-43c0-9c71-77996da5f2f5",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:58:44.178805Z"
        },
        {
          "instance_id": "c9acbcea-6cd4-4377-b901-d9b2a618adde",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:58:44.178805Z"
        },
        {
          "instance_id": "1123aa07-a154-4075-8b24-693c9cf44c08",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:58:44.178805Z"
        },
        {
          "instance_id": "cd1302b0-cfd4-4e4b-8b0e-62861f56a81b",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:58:44.178805Z"
        },
        {
          "instance_id": "cead082e-fd0e-443a-aee1-b5e60b7ca546",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:58:44.178805Z"
        },
        {
          "instance_id": "d78b2c0f-a02f-43dd-9dea-efc066d4e3e2",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:58:44.178805Z"
        },
        {
          "instance_id": "e9e6f2ff-540e-4003-88b5-18d50c408053",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:58:44.178805Z"
        },
        {
          "instance_id": "453c20b9-2c4c-4e1b-8ac6-13b5bac95d05",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:58:44.178805Z"
        },
        {
          "instance_id": "1b284130-75c8-4ee2-a10e-72ac9d6b9f83",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:58:44.178805Z"
        },
        {
          "instance_id": "ab584885-38a9-4da7-98b4-e6303bc9c50a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:58:44.178805Z"
        },
        {
          "instance_id": "700bd834-1ffe-43ed-88db-3c88b7a9ef6e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:58:44.178805Z"
        }
      ],
      "logo_id": "74eff6e9-0163-444d-8c66-a7ec34c31f1a",
      "openrouter": false,
      "chutes_version": "0.3.23.rc1",
      "revision": "7152993552508c9f22042b3bb93b5e6acd06ce73",
      "concurrency": 16,
      "boost": 7.0,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/74eff6e9-0163-444d-8c66-a7ec34c31f1a.webp",
      "hot": true
    },
    {
      "chute_id": "7fa03c12-823f-529a-8245-36432f03e9a1",
      "name": "zai-org/GLM-4.5-Air",
      "tagline": "GLM-4.5-Air is a 106B parameter language model (12B active) optimized for reasoning and agent tasks, offering both fast response and careful thinking modes for applications requiring complex reasoning or tool use.",
      "readme": "\n# GLM-4.5-Air\n\n<div align=\"center\">\n<img src=https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/logo.svg width=\"15%\"/>\n</div>\n<p align=\"center\">\n    \ud83d\udc4b Join our <a href=\"https://discord.gg/QR7SARHRxK\" target=\"_blank\">Discord</a> community.\n    <br>\n    \ud83d\udcd6 Check out the GLM-4.5 <a href=\"https://z.ai/blog/glm-4.5\" target=\"_blank\">technical blog</a>, <a href=\"https://arxiv.org/abs/2508.06471\" target=\"_blank\">technical report</a>, and <a href=\"https://zhipu-ai.feishu.cn/wiki/Gv3swM0Yci7w7Zke9E0crhU7n7D\" target=\"_blank\">Zhipu AI technical documentation</a>.\n    <br>\n    \ud83d\udccd Use GLM-4.5 API services on <a href=\"https://docs.z.ai/guides/llm/glm-4.5\">Z.ai API Platform (Global)</a> or <br> <a href=\"https://docs.bigmodel.cn/cn/guide/models/text/glm-4.5\">Zhipu AI Open Platform (Mainland China)</a>.\n    <br>\n    \ud83d\udc49 One click to <a href=\"https://chat.z.ai\">GLM-4.5</a>.\n</p>\n  \n## Model Introduction\n\nThe **GLM-4.5** series models are foundation models designed for intelligent agents. GLM-4.5 has **355** billion total parameters with **32** billion active parameters, while GLM-4.5-Air adopts a more compact design with **106** billion total parameters and **12** billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.\n\nBoth GLM-4.5 and GLM-4.5-Air are hybrid reasoning models that provide two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses.\n\nWe have open-sourced the base models, hybrid reasoning models, and FP8 versions of the hybrid reasoning models for both GLM-4.5 and GLM-4.5-Air. They are released under the MIT open-source license and can be used commercially and for secondary development.\n\nAs demonstrated in our comprehensive evaluation across 12 industry-standard benchmarks, GLM-4.5 achieves exceptional performance with a score of **63.2**, in the **3rd** place among all the proprietary and open-source  models. Notably, GLM-4.5-Air delivers competitive results at **59.8** while maintaining superior efficiency.\n\n![bench](https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/bench.png)\n\nFor more eval results, show cases, and technical details, please visit\nour [technical blog](https://z.ai/blog/glm-4.5) or [technical report](https://huggingface.co/papers/2508.06471).\n\nThe model code, tool parser and reasoning parser can be found in the implementation of [transformers](https://github.com/huggingface/transformers/tree/main/src/transformers/models/glm4_moe), [vLLM](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/glm4_moe_mtp.py) and [SGLang](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/models/glm4_moe.py).\n\n## Quick Start\n\nPlease refer our [github page](https://github.com/zai-org/GLM-4.5) for more detail.",
      "public": true,
      "version": "eec8cbc3-6fbc-5372-a3ae-9191aef3db34",
      "tool_description": null,
      "slug": "chutes-zai-org-glm-4-5-air",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-07-28T14:43:02.919379Z",
      "updated_at": "2025-09-03T11:44:25.943841Z",
      "image": {
        "image_id": "89c8efd3-fff2-530f-a9b2-e344dcaaebe2",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.5.1.post3",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-30T09:40:05.257537Z",
        "build_started_at": "2025-08-30T09:40:08.281915Z",
        "build_completed_at": "2025-08-30T10:01:00.019895Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "a100",
        "a100_40gb_sxm",
        "a100_40gb",
        "a100_sxm",
        "h20",
        "l40s",
        "h100_sxm",
        "h100",
        "a6000_ada",
        "h800"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "h100",
          "h100_sxm",
          "h800",
          "l40s",
          "a6000_ada",
          "a100",
          "a100_sxm",
          "a100_40gb",
          "a100_40gb_sxm",
          "h20"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 1.3,
        "supported_gpus": [
          "a100",
          "a100_40gb_sxm",
          "a100_40gb",
          "a100_sxm",
          "h20",
          "l40s",
          "h100_sxm",
          "h100",
          "a6000_ada",
          "h800"
        ]
      },
      "invocation_count": 229623,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.0,
            "tao": 0.0
          },
          "output": {
            "usd": 0.0,
            "tao": 0.0
          }
        },
        "usd": {
          "hour": 0.0,
          "second": 0.0
        },
        "tao": {
          "hour": 0.0,
          "second": 0.0
        }
      },
      "instances": [
        {
          "instance_id": "5bc87504-baa8-4be7-b804-f0f475568f2c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:10:58.983352Z"
        },
        {
          "instance_id": "0a684530-792d-4d79-abfb-8a4360d7ce4e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:45:39.105788Z"
        },
        {
          "instance_id": "2cc90883-71ef-4e76-975b-01cd8e75386b",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:10:54.770436Z"
        },
        {
          "instance_id": "83fa632d-d1fa-462d-abcc-145f99463363",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:45:39.105788Z"
        },
        {
          "instance_id": "127a4651-e595-4ab8-883b-ef7c1a5c227b",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:45:39.105788Z"
        },
        {
          "instance_id": "31ee3d20-9956-4645-9e08-e0457c16bf8f",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:45:39.105788Z"
        },
        {
          "instance_id": "d872311f-9ae2-4c03-a7e1-970acb979cf9",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:45:39.105788Z"
        },
        {
          "instance_id": "97a79c43-7970-4ab1-8c9e-36147fb60ebb",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:45:39.105788Z"
        },
        {
          "instance_id": "83fee03d-6899-4911-8d1a-8e8ad18dcde3",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:22:02.675284Z"
        },
        {
          "instance_id": "048dee64-3359-4df8-ac15-3be5c3efc4a2",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:45:39.105788Z"
        },
        {
          "instance_id": "c344f30e-aea4-4f3f-ae1d-e7c6bf786710",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:45:39.105788Z"
        },
        {
          "instance_id": "b811b88d-45a7-4558-b471-81fb06d21cb6",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:45:39.105788Z"
        }
      ],
      "logo_id": "80e0ad57-45e7-44ea-9f85-76cec48114c8",
      "openrouter": false,
      "chutes_version": "0.3.23.rc1",
      "revision": "e7fdb9e0a52d2e0aefea94f5867c924a32a78d17",
      "concurrency": 24,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/80e0ad57-45e7-44ea-9f85-76cec48114c8.webp",
      "hot": true
    },
    {
      "chute_id": "118a60a5-6bd8-554c-8fda-9d394e5c0ab8",
      "name": "zai-org/GLM-Z1-32B-0414",
      "tagline": "GLM-4-Z1-32B-0414 is a 32 billion parameter open-source language model optimized for deep reasoning and complex problem-solving tasks like mathematics, code generation, and analytical writing.",
      "readme": "\n# GLM-4-Z1-32B-0414\n\n## Introduction\n\nThe GLM family welcomes a new generation of open-source models, the **GLM-4-32B-0414** series, featuring 32 billion parameters. Its performance is comparable to OpenAI's GPT series and DeepSeek's V3/R1 series, and it supports very user-friendly local deployment features. GLM-4-32B-Base-0414 was pre-trained on 15T of high-quality data, including a large amount of reasoning-type synthetic data, laying the foundation for subsequent reinforcement learning extensions. In the post-training stage, in addition to human preference alignment for dialogue scenarios, we also enhanced the model's performance in instruction following, engineering code, and function calling using techniques such as rejection sampling and reinforcement learning, strengthening the atomic capabilities required for agent tasks. GLM-4-32B-0414 achieves good results in areas such as engineering code, Artifact generation, function calling, search-based Q&A, and report generation. Some benchmarks even rival larger models like GPT-4o and DeepSeek-V3-0324 (671B).\n\n**GLM-Z1-32B-0414** is a reasoning model with **deep thinking capabilities**. This was developed based on GLM-4-32B-0414 through cold start and extended reinforcement learning, as well as further training of the model on tasks involving mathematics, code, and logic. Compared to the base model, GLM-Z1-32B-0414 significantly improves mathematical abilities and the capability to solve complex tasks. During the training process, we also introduced general reinforcement learning based on pairwise ranking feedback, further enhancing the model's general capabilities.\n\n**GLM-Z1-Rumination-32B-0414** is a deep reasoning model with **rumination capabilities** (benchmarked against OpenAI's Deep Research). Unlike typical deep thinking models, the rumination model employs longer periods of deep thought to solve more open-ended and complex problems (e.g., writing a comparative analysis of AI development in two cities and their future development plans). The rumination model integrates search tools during its deep thinking process to handle complex tasks and is trained by utilizing multiple rule-based rewards to guide and extend end-to-end reinforcement learning. Z1-Rumination shows significant improvements in research-style writing and complex retrieval tasks.\n\nFinally, **GLM-Z1-9B-0414** is a surprise. We employed the aforementioned series of techniques to train a 9B small-sized model that maintains the open-source tradition. Despite its smaller scale, GLM-Z1-9B-0414 still exhibits excellent capabilities in mathematical reasoning and general tasks. Its overall performance is already at a leading level among open-source models of the same size. Especially in resource-constrained scenarios, this model achieves an excellent balance between efficiency and effectiveness, providing a powerful option for users seeking lightweight deployment.\n\n## Performance\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://raw.githubusercontent.com/THUDM/GLM-4/refs/heads/main/resources/Bench-Z1-32B.png\">\n</p>\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://raw.githubusercontent.com/THUDM/GLM-4/refs/heads/main/resources/Bench-Z1-9B.png\">\n</p>\n\n## Model Usage Guidelines\n\n### I. Sampling Parameters\n\n| Parameter    | Recommended Value | Description                                  |\n| ------------ | ----------------- | -------------------------------------------- |\n| temperature  | **0.6**           | Balances creativity and stability            |\n| top_p        | **0.95**          | Cumulative probability threshold for sampling|\n| top_k        | **40**         | Filters out rare tokens while maintaining diversity |\n| max_new_tokens        | **30000**         | Leaves enough tokens for thinking |\n\n### II. Enforced Thinking\n\n- Add \\<think\\>\\n to the **first line**: Ensures the model thinks before responding  \n- When using `chat_template.jinja`, the prompt is automatically injected to enforce this behavior\n\n\n### III. Dialogue History Trimming\n\n- Retain only the **final user-visible reply**.  \n  Hidden thinking content should **not** be saved to history to reduce interference\u2014this is already implemented in `chat_template.jinja`\n\n\n### IV. Handling Long Contexts (YaRN)\n\n- When input length exceeds **8,192 tokens**, consider enabling YaRN (Rope Scaling)\n\n- In supported frameworks, add the following snippet to `config.json`:\n\n  ```json\n  \"rope_scaling\": {\n    \"type\": \"yarn\",\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768\n  }\n  ```\n\n- **Static YaRN** applies uniformly to all text. It may slightly degrade performance on short texts, so enable as needed.\n\n\n## Inference Code\n\nMake Sure Using `transforemrs>=4.51.3`.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nMODEL_PATH = \"THUDM/GLM-4-Z1-32B-0414\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\")\n\nmessage = [{\"role\": \"user\", \"content\": \"Let a, b be positive real numbers such that ab = a + b + 3. Determine the range of possible values for a + b.\"}]\n\ninputs = tokenizer.apply_chat_template(\n    message,\n    return_tensors=\"pt\",\n    add_generation_prompt=True,\n    return_dict=True,\n).to(model.device)\n\ngenerate_kwargs = {\n    \"input_ids\": inputs[\"input_ids\"],\n    \"attention_mask\": inputs[\"attention_mask\"],\n    \"max_new_tokens\": 4096,\n    \"do_sample\": False,\n}\nout = model.generate(**generate_kwargs)\nprint(tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True))\n```\n\n## Citations\n\nIf you find our work useful, please consider citing the following paper.\n\n```\n@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\n}\n```",
      "public": true,
      "version": "33c885fc-a8aa-5091-9928-533e1f5ba564",
      "tool_description": null,
      "slug": "chutes-zai-org-glm-z1-32b-0414",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "bcc7eb3b-d9ea-5892-b6ce-97ae9dc8e825",
      "created_at": "2025-08-04T12:49:12.698661Z",
      "updated_at": "2025-08-07T11:08:55.605458Z",
      "image": {
        "image_id": "dd30a5e7-24ae-538e-bee7-ba6a93b3d2f0",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.4.9.post6",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-06T20:56:26.489668Z",
        "build_started_at": "2025-08-06T20:56:28.092938Z",
        "build_completed_at": "2025-08-06T21:28:45.347952Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "a6000_ada",
        "h100_nvl",
        "a100",
        "l40",
        "a100_40gb_sxm",
        "a100_40gb",
        "h20",
        "a100_sxm",
        "l40s",
        "pro_6000",
        "h100_sxm",
        "h200",
        "a40",
        "a6000",
        "h100",
        "h800"
      ],
      "node_selector": {
        "exclude": [
          "b200",
          "mi300x"
        ],
        "include": null,
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 40,
        "compute_multiplier": 0.54,
        "supported_gpus": [
          "a6000_ada",
          "h100_nvl",
          "a100",
          "l40",
          "a100_40gb_sxm",
          "a100_40gb",
          "h20",
          "a100_sxm",
          "l40s",
          "pro_6000",
          "h100_sxm",
          "h200",
          "a40",
          "a6000",
          "h100",
          "h800"
        ]
      },
      "invocation_count": 214175,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.01999188,
            "tao": 5.888081599546198e-05
          },
          "output": {
            "usd": 0.08000639999999999,
            "tao": 0.00023563777477952693
          }
        },
        "usd": {
          "hour": 2.16,
          "second": 0.0006000000000000001
        },
        "tao": {
          "hour": 0.006361710982168655,
          "second": 1.767141939491293e-06
        }
      },
      "instances": [
        {
          "instance_id": "a9e6c80e-aad1-4779-b101-9ba6b383ffc0",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:05:55.408902Z"
        },
        {
          "instance_id": "5e983796-8fee-4eea-a744-d4d6099d1563",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:05:59.891913Z"
        },
        {
          "instance_id": "74762324-8e3e-4b43-8fe5-6bfafe2b7f93",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:06:14.125264Z"
        },
        {
          "instance_id": "e1778e97-8359-445d-9dc1-1b0ab2611fc9",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:53:05.646383Z"
        },
        {
          "instance_id": "04e133b8-f94b-467b-ad26-96c008760c43",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:56:04.536505Z"
        },
        {
          "instance_id": "aaae7c92-2e20-4818-9754-28e38ad39013",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:53:05.646383Z"
        },
        {
          "instance_id": "c04efeb4-1c45-4c28-8db8-1b6c64ac635f",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:53:05.646383Z"
        },
        {
          "instance_id": "97fc4e7d-3c50-4a8e-93d7-0aecb61363af",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:53:05.646383Z"
        },
        {
          "instance_id": "229163ad-1206-47da-ba1a-49cd4d3c097f",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:53:05.646383Z"
        },
        {
          "instance_id": "7971a590-7903-40e5-9ad3-d32e30a9a261",
          "region": "n/a",
          "active": false,
          "verified": true,
          "last_verified_at": "2025-09-08T10:25:11.968924Z"
        },
        {
          "instance_id": "0ac15afa-2fc5-4208-a64a-72c67b98eee3",
          "region": "n/a",
          "active": false,
          "verified": true,
          "last_verified_at": "2025-09-08T10:25:05.954687Z"
        }
      ],
      "logo_id": "70eb6b09-fd9f-486a-af99-cfb60b6ced7f",
      "openrouter": false,
      "chutes_version": "0.3.19.rc0",
      "revision": "8eb2858992c1f749e2a6d4075455decc2484722d",
      "concurrency": 16,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/70eb6b09-fd9f-486a-af99-cfb60b6ced7f.webp",
      "hot": true
    },
    {
      "chute_id": "e3653034-9f58-5cf6-84f8-d5555e55fbd6",
      "name": "chutesai/Mistral-Small-3.1-24B-Instruct-2503",
      "tagline": "Mistral Small 3.1 is a 24B parameter open-source LLM with vision capabilities and 128k context that excels at reasoning, programming, and multilingual tasks while being compact enough to run on consumer hardware.",
      "readme": "\n# Model Card for Mistral-Small-3.1-24B-Instruct-2503\n\nBuilding upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) **adds state-of-the-art vision understanding** and enhances **long context capabilities up to 128k tokens** without compromising text performance. \nWith 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.  \nThis model is an instruction-finetuned version of: [Mistral-Small-3.1-24B-Base-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503).\n\nMistral Small 3.1 can be deployed locally and is exceptionally \"knowledge-dense,\" fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.  \n\nIt is ideal for:\n- Fast-response conversational agents.\n- Low-latency function calling.\n- Subject matter experts via fine-tuning.\n- Local inference for hobbyists and organizations handling sensitive data.\n- Programming and math reasoning.\n- Long document understanding.\n- Visual understanding.\n\nFor enterprises requiring specialized capabilities (increased context, specific modalities, domain-specific knowledge, etc.), we will release commercial models beyond what Mistral AI contributes to the community.\n\nLearn more about Mistral Small 3.1 in our [blog post](https://mistral.ai/news/mistral-small-3-1/).\n\n## Key Features\n- **Vision:** Vision capabilities enable the model to analyze images and provide insights based on visual content in addition to text.\n- **Multilingual:** Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Swedish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, Farsi.\n- **Agent-Centric:** Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n- **Advanced Reasoning:** State-of-the-art conversational and reasoning capabilities.\n- **Apache 2.0 License:** Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window:** A 128k context window.\n- **System Prompt:** Maintains strong adherence and support for system prompts.\n- **Tokenizer:** Utilizes a Tekken tokenizer with a 131k vocabulary size.\n\n## Benchmark Results\n\nWhen available, we report numbers previously published by other model providers, otherwise we re-evaluate them using our own evaluation harness.\n\n### Pretrain Evals\n\n| Model                          | MMLU (5-shot) | MMLU Pro (5-shot CoT) | TriviaQA   | GPQA Main (5-shot CoT)| MMMU      |\n|--------------------------------|---------------|-----------------------|------------|-----------------------|-----------|\n| **Small 3.1 24B Base**         | **81.01%**    | **56.03%**            | 80.50%     | **37.50%**            | **59.27%**|\n| Gemma 3 27B PT                 | 78.60%        | 52.20%                | **81.30%** | 24.30%                | 56.10%    |\n\n### Instruction Evals\n\n#### Text\n\n| Model                          | MMLU      | MMLU Pro (5-shot CoT) | MATH                   | GPQA Main (5-shot CoT) | GPQA Diamond (5-shot CoT )| MBPP      | HumanEval | SimpleQA (TotalAcc)|\n|--------------------------------|-----------|-----------------------|------------------------|------------------------|---------------------------|-----------|-----------|--------------------|\n| **Small 3.1 24B Instruct**     | 80.62%    | 66.76%                | 69.30%                 | **44.42%**             | **45.96%**                | 74.71%    | **88.41%**| **10.43%**         |\n| Gemma 3 27B IT                 | 76.90%    | **67.50%**            | **89.00%**             | 36.83%                 | 42.40%                    | 74.40%    | 87.80%    | 10.00%             |\n| GPT4o Mini                     | **82.00%**| 61.70%                | 70.20%                 | 40.20%                 | 39.39%                    | 84.82%    | 87.20%    | 9.50%              |\n| Claude 3.5 Haiku               | 77.60%    | 65.00%                | 69.20%                 | 37.05%                 | 41.60%                    | **85.60%**| 88.10%    | 8.02%              |\n| Cohere Aya-Vision 32B          | 72.14%    | 47.16%                | 41.98%                 | 34.38%                 | 33.84%                    | 70.43%    | 62.20%    | 7.65%              |\n\n#### Vision\n\n| Model                          | MMMU       | MMMU PRO  | Mathvista | ChartQA   | DocVQA    | AI2D        | MM MT Bench |\n|--------------------------------|------------|-----------|-----------|-----------|-----------|-------------|-------------|\n| **Small 3.1 24B Instruct**     | 64.00%     | **49.25%**| **68.91%**| 86.24%    | **94.08%**| **93.72%**  | **7.3**     |\n| Gemma 3 27B IT                 | **64.90%** | 48.38%    | 67.60%    | 76.00%    | 86.60%    | 84.50%      | 7           |\n| GPT4o Mini                     | 59.40%     | 37.60%    | 56.70%    | 76.80%    | 86.70%    | 88.10%      | 6.6         |\n| Claude 3.5 Haiku               | 60.50%     | 45.03%    | 61.60%    | **87.20%**| 90.00%    | 92.10%      | 6.5         |\n| Cohere Aya-Vision 32B          | 48.20%     | 31.50%    | 50.10%    | 63.04%    | 72.40%    | 82.57%      | 4.1         |\n\n### Multilingual Evals\n\n| Model                          | Average    | European   | East Asian | Middle Eastern |\n|--------------------------------|------------|------------|------------|----------------|\n| **Small 3.1 24B Instruct**     | **71.18%** | **75.30%** | **69.17%** | 69.08%         |\n| Gemma 3 27B IT                 | 70.19%     | 74.14%     | 65.65%     | 70.76%         |\n| GPT4o Mini                     | 70.36%     | 74.21%     | 65.96%     | **70.90%**     |\n| Claude 3.5 Haiku               | 70.16%     | 73.45%     | 67.05%     | 70.00%         |\n| Cohere Aya-Vision 32B          | 62.15%     | 64.70%     | 57.61%     | 64.12%         |\n\n### Long Context Evals\n\n| Model                          | LongBench v2    | RULER 32K   | RULER 128K |\n|--------------------------------|-----------------|-------------|------------|\n| **Small 3.1 24B Instruct**     | **37.18%**      | **93.96%**  | 81.20%     |\n| Gemma 3 27B IT                 | 34.59%          | 91.10%      | 66.00%     |\n| GPT4o Mini                     | 29.30%          | 90.20%      | 65.8%      |\n| Claude 3.5 Haiku               | 35.19%          | 92.60%      | **91.90%** |\n\n## Basic Instruct Template (V7-Tekken)\n\n```\n<s>[SYSTEM_PROMPT]<system prompt>[/SYSTEM_PROMPT][INST]<user message>[/INST]<assistant response></s>[INST]<user message>[/INST]\n```\n*`<system_prompt>`, `<user message>` and `<assistant response>` are placeholders.*\n\n***Please make sure to use [mistral-common](https://github.com/mistralai/mistral-common) as the source of truth***\n\n## Usage\n\nThe model can be used with the following frameworks;\n- [`vllm (recommended)`](https://github.com/vllm-project/vllm): See [here](https://huggingface.co/chutesai/Mistral-Small-3.1-24B-Instruct-2503/raw/main/#vllm)\n\n**Note 1**: We recommend using a relatively low temperature, such as `temperature=0.15`.\n\n**Note 2**: Make sure to add a system prompt to the model to best tailer it for your needs. If you want to use the model as a general assistant, we recommend the following \nsystem prompt:\n\n```\nsystem_prompt = \"\"\"You are Mistral Small 3.1, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\nYou power an AI assistant called Le Chat.\nYour knowledge base was last updated on 2023-10-01.\nThe current date is {today}.\n\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \"What are some good restaurants around me?\" => \"Where are you?\" or \"When is the next flight to Tokyo\" => \"Where do you travel from?\").\nYou are always very attentive to dates, in particular you try to resolve dates (e.g. \"yesterday\" is {yesterday}) and when asked about information at specific dates, you discard information that is at another date.\nYou follow these instructions in all languages, and always respond to the user in the language they use or request.\nNext sections describe the capabilities that you have.\n\n# WEB BROWSING INSTRUCTIONS\n\nYou cannot perform any web search or access internet to open URLs, links etc. If it seems like the user is expecting you to do so, you clarify the situation and ask the user to copy paste the text directly in the chat.\n\n# MULTI-MODAL INSTRUCTIONS\n\nYou have the ability to read images, but you cannot generate images. You also cannot transcribe audio files or videos.\nYou cannot read nor transcribe audio files or videos.\"\"\"\n```\n\n### vLLM (recommended)\n\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n\n**_Installation_**\n\nMake sure you install [`vLLM >= 0.8.1`](https://github.com/vllm-project/vllm/releases/tag/v0.8.1):\n\n```\npip install vllm --upgrade\n```\n\nDoing so should automatically install [`mistral_common >= 1.5.4`](https://github.com/mistralai/mistral-common/releases/tag/v1.5.4).\n\nTo check:\n```\npython -c \"import mistral_common; print(mistral_common.__version__)\"\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39).\n\n#### Server\n\nWe recommand that you use Mistral-Small-3.1-24B-Instruct-2503 in a server/client setting. \n\n1. Spin up a server:\n\n```\nvllm serve mistralai/Mistral-Small-3.1-24B-Instruct-2503 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --limit_mm_per_prompt 'image=10' --tensor-parallel-size 2\n```\n\n**Note:** Running Mistral-Small-3.1-24B-Instruct-2503 on GPU requires ~55 GB of GPU RAM in bf16 or fp16. \n\n\n2. To ping the client you can use a simple Python snippet.\n\n```py\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\n\nurl = \"http://<your-server-url>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n\nmodel = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime(\"%Y-%m-%d\")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    model_name = repo_id.split(\"/\")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\n\nimage_url = \"https://huggingface.co/datasets/patrickvonplaten/random_img/resolve/main/europe.png\"\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Which of the depicted countries has the best food? Which the second and third and fourth? Name the country, its color on the map and one its city that is visible on the map, but is not the capital. Make absolutely sure to only name a city that can be seen on the map.\",\n            },\n            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n        ],\n    },\n]\n\ndata = {\"model\": model, \"messages\": messages, \"temperature\": 0.15}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])\n# Determining the \"best\" food is highly subjective and depends on personal preferences. However, based on general popularity and recognition, here are some countries known for their cuisine:\n\n# 1. **Italy** - Color: Light Green - City: Milan\n#    - Italian cuisine is renowned worldwide for its pasta, pizza, and various regional specialties.\n\n# 2. **France** - Color: Brown - City: Lyon\n#    - French cuisine is celebrated for its sophistication, including dishes like coq au vin, bouillabaisse, and pastries like croissants and \u00e9clairs.\n\n# 3. **Spain** - Color: Yellow - City: Bilbao\n#    - Spanish cuisine offers a variety of flavors, from paella and tapas to jam\u00f3n ib\u00e9rico and churros.\n\n# 4. **Greece** - Not visible on the map\n#    - Greek cuisine is known for dishes like moussaka, souvlaki, and baklava. Unfortunately, Greece is not visible on the provided map, so I cannot name a city.\n\n# Since Greece is not visible on the map, I'll replace it with another country known for its good food:\n\n# 4. **Turkey** - Color: Light Green (east part of the map) - City: Istanbul\n#    - Turkish cuisine is diverse and includes dishes like kebabs, meze, and baklava.\n```\n\n### Function calling\n\nMistral-Small-3.1-24-Instruct-2503 is excellent at function / tool calling tasks via vLLM. *E.g.:*\n\n<details>\n  <summary>Example</summary>\n\n```py\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\n\nurl = \"http://<your-url>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\n\nmodel = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, \"r\") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime(\"%Y-%m-%d\")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n    model_name = repo_id.split(\"/\")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"city\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n                    },\n                    \"state\": {\n                        \"type\": \"string\",\n                        \"description\": \"The state abbreviation, e.g. 'CA' for California\",\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"description\": \"The unit for temperature\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                    },\n                },\n                \"required\": [\"city\", \"state\", \"unit\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"rewrite\",\n            \"description\": \"Rewrite a given text for improved clarity\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"text\": {\n                        \"type\": \"string\",\n                        \"description\": \"The input text to rewrite\",\n                    }\n                },\n            },\n        },\n    },\n]\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\n        \"role\": \"user\",\n        \"content\": \"Could you please make the below article more concise?\\n\\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": [\n            {\n                \"id\": \"bbc5b7ede\",\n                \"t",
      "public": true,
      "version": "ba3374da-01d2-596d-b339-e79ff4e5c737",
      "tool_description": null,
      "slug": "chutes-chutesai-mistral-small-3-1-24b-instruct-2503",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "bcc7eb3b-d9ea-5892-b6ce-97ae9dc8e825",
      "created_at": "2025-03-24T00:29:32.081107Z",
      "updated_at": "2025-08-03T07:16:06.830365Z",
      "image": {
        "image_id": "a395302b-270f-5398-9540-8c29dea16f3d",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.4.10.post1",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-01T09:18:36.513090Z",
        "build_started_at": "2025-08-01T09:18:38.000857Z",
        "build_completed_at": "2025-08-01T09:42:51.462771Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "a6000_ada",
        "h100_nvl",
        "a100",
        "h20",
        "a100_sxm",
        "l40s",
        "pro_6000",
        "h100_sxm",
        "h200",
        "a40",
        "h100",
        "h800"
      ],
      "node_selector": {
        "exclude": [
          "5090",
          "a6000",
          "b200",
          "l40",
          "mi300x"
        ],
        "include": null,
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 48,
        "compute_multiplier": 0.54,
        "supported_gpus": [
          "a6000_ada",
          "h100_nvl",
          "a100",
          "h20",
          "a100_sxm",
          "l40s",
          "pro_6000",
          "h100_sxm",
          "h200",
          "a40",
          "h100",
          "h800"
        ]
      },
      "invocation_count": 212063,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.01999188,
            "tao": 5.888081599546198e-05
          },
          "output": {
            "usd": 0.08000639999999999,
            "tao": 0.00023563777477952693
          }
        },
        "usd": {
          "hour": 2.16,
          "second": 0.0006000000000000001
        },
        "tao": {
          "hour": 0.006361710982168655,
          "second": 1.767141939491293e-06
        }
      },
      "instances": [
        {
          "instance_id": "9e99e78a-b3e8-467b-9599-550f946468e5",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:24:23.337808Z"
        },
        {
          "instance_id": "c5ed80df-4601-4cb9-afcf-fb50464cdc0c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:24:23.337808Z"
        },
        {
          "instance_id": "9cbe9673-cda4-4fd2-9139-d243c25c2e20",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:24:23.337808Z"
        },
        {
          "instance_id": "5807b64c-baac-470e-b712-b1f841a514b1",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:24:23.337808Z"
        },
        {
          "instance_id": "058fd073-f2d3-49a4-b5ca-bbf0356a0026",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:24:23.337808Z"
        },
        {
          "instance_id": "43b1712c-ce03-4e06-8f64-1d297a1a336a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:24:23.337808Z"
        }
      ],
      "logo_id": "3d5f67e8-93b1-491f-9b2a-0b791eb5fa18",
      "openrouter": true,
      "chutes_version": "0.3.11",
      "revision": "38e9da92bb1e44b83fbfcd6ea2423e3b1f712d79",
      "concurrency": 24,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/3d5f67e8-93b1-491f-9b2a-0b791eb5fa18.webp",
      "hot": true
    },
    {
      "chute_id": "02636d63-c996-5779-a0a2-25712469a7ca",
      "name": "NousResearch/Hermes-4-405B-FP8",
      "tagline": "Hermes 4 is a 405B parameter open-source language model based on Llama 3.1 that excels at reasoning tasks while being highly steerable and capable of producing structured outputs for applications in coding, math, and creative writing.",
      "readme": "\n# Hermes 4 \u2014 Llama-3.1 405B - FP8\n\n![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/roT9o5bMYBtQziRMlaSDf.jpeg)\n\n## Model Description\n\nHermes 4 405B is a frontier, hybrid-mode **reasoning** model based on Llama-3.1-405B by Nous Research that is aligned to **you**.\n\nRead the Hermes 4 technical report here: <a href=\"https://arxiv.org/abs/2508.18255\">Hermes 4 Technical Report</a>\n\nChat with Hermes in Nous Chat: https://chat.nousresearch.com\n\nTraining highlights include a newly synthesized post-training corpus emphasizing verified reasoning traces, massive improvements in math, code, STEM, logic, creativity, and format-faithful outputs, while preserving general assistant quality and broadly neutral alignment.\n\n**This is the FP8 version of Hermes 4, please see the <a href=\"https://huggingface.co/NousResearch/Hermes-4-405B\"> BF16 Model </a> if looking for that.**\n\n## What\u2019s new vs Hermes 3\n\n- **Post-training corpus**: Massively increased dataset size from 1M samples and 1.2B tokens to **~5M samples / ~60B tokens** blended across reasoning and non-reasoning data.\n- **Hybrid reasoning mode** with explicit `<think>\u2026</think>` segments when the model decides to deliberate, and options to make your responses faster when you want.\n- **Reasoning** that is top quality, expressive, improves math, code, STEM, logic, and even creative writing and subjective responses.\n- **Schema adherence & structured outputs**: trained to produce valid JSON for given schemas and to repair malformed objects.\n- **Much easier to steer and align**: extreme improvements on steerability, especially on reduced refusal rates.\n\n## Our Mission: Frontier Capabilities Aligned to You\n\nIn pursuit of the mission of producing models that are open, steerable and capable of producing the full range of human expression, while being able to be aligned to your values, we created a new benchmark, RefusalBench, that tests the models willingness to be helpful in a variety of scenarios commonly disallowed by closed and open models.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/t_HvRYPEHV0pc8iS2zHHn.png)\n\nHermes 4 achieves SOTA on RefusalBench across all popular closed and open models in being helpful and conforming to your values, without censorship.\n\n## Benchmarks (Hermes 4 405B)\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/ZOj3LrFweV7MYwlfP_eiO.png)\n\n> Full tables, settings, and comparisons are in the technical report.\n\n## Prompt Format\n\nHermes 4 uses Llama-3-Chat format with role headers and special tags.\n\n**Basic chat:**\n```\n<|start_header_id|>system<|end_header_id|>\n\nYou are Hermes 4. Be concise and helpful.<|eot_id|>\n<|start_header_id|>user<|end_header_id|>\n\nExplain the photoelectric effect simply.<|im_end|>\n<|start_header_id|>assistant<|end_header_id|>\n```\n\n### Reasoning mode\n\nReasoning mode can be activated with the chat template via the flag `thinking=True` or by using the following system prompt:\n\n```\nYou are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\n```\n\nNote that you can add any additional system instructions before or after this system message, and it will adjust the models policies, style, and effort of thinking, as well as its post-thinking style, format, identity, and more. You may also interleave the tool definition system message with the reasoning one. \n\nWhen the model chooses to deliberate, it emits:\n\n```\n<|start_header_id|>assistant<|end_header_id|>\n<think>\n\u2026model\u2019s internal reasoning may appear here\u2026\n</think>\nFinal response starts here\u2026<|eot_id|>\n```\n\nAdditionally, we provide a flag to keep the content inbetween the `<think> ... </think>` that you can play with by setting `keep_cots=True`\n\n## Function Calling & Tool Use\n\nHermes 4 supports function/tool calls *within* a single assistant turn, interleaved with its reasoning:\n\n**System message (example):**\n\n```\n<|im_start|>system\nYou are a function-calling AI. Tools are provided inside <tools>\u2026</tools>.\nWhen appropriate, call a tool by emitting a <tool_call>{...}</tool_call> object.\nAfter a tool responds (as <tool_response>), continue reasoning inside <think> and produce the final answer.\n<tools>\n{\"type\":\"function\",\"function\":{\"name\":\"get_weather\",\"description\":\"Get weather by city\",\"parameters\":{\"type\":\"object\",\"properties\":{\"city\":{\"type\":\"string\"}},\"required\":[\"city\"]}}}\n</tools><|im_end|>\n```\n\nNote that you may also simply place tool definitions into the \"tools:\" field of your messages, and the chat template will parse and create the system prompt for you. This also works with reasoning mode for improved accuracy of tool use.\n\nThe model will then generate tool calls within `<tool_call> {tool_call} </tool_call>` tags, for easy parsing. The tool_call tags are also added tokens, so it makes it easy to parse while streaming! There are also automatic tool parsers built-in to VLLM and SGLang for Hermes, just set the tool parser in VLLM to `hermes` and in SGLang to `qwen25`.\n\n## Inference Notes\n\n- **Sampling defaults that work well:** `temperature=0.6, top_p=0.95, top_k=20`.\n- **Template:** Use the Llama chat format for Hermes 4 70B and 405B as shown above, or set `add_generation_prompt=True` when using `tokenizer.apply_chat_template(...)`.\n\n### Transformers example\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"NousResearch/Hermes-4-Llama-3.1-405B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\nmessages = [\n    {\"role\":\"system\",\"content\":\"You are Hermes 4. Be concise.\"},\n    {\"role\":\"user\",\"content\":\"Summarize CRISPR in 3 sentences.\"}\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages, add_generation_prompt=True, return_tensors=\"pt\"\n).to(model.device)\n\noutputs = model.generate(\n    **inputs, max_new_tokens=400, temperature=0.6, top_p=0.95, top_k=20, do_sample=True\n)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nFor production serving on multi-GPU nodes, consider tensor parallel inference engines (e.g., SGLang/vLLM backends) with prefix caching.\n\n## Inference Providers:\n\n### Nous Portal:\n\n<a href=\"https://portal.nousresearch.com\"><img width=256 alt=\"chutes logo\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/6YytY7N0mjCnBQvWo3qtv.png\"></a>\n\n### Chutes:\n\n<a href=\"https://chutes.ai/app\"><img width=256 alt=\"chutes logo\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/l14AWPv6cSvaprpwK_IWY.png\"></a>\n\n### Nebius:\n\n<a href=\"https://nebius.com/services/studio-inference-service\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/vhL0oAomFa_awBdt2KF_x.png\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://cdn-uploads.huggingface.co/production/uploads/64b21cbb2fc8324fcb1dac03/LjAfeFfAz8ac5rV-iiwj5.png\">\n  <img width=256 alt=\"nebius.com logo\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64b21cbb2fc8324fcb1dac03/LjAfeFfAz8ac5rV-iiwj5.png\">\n</picture>\n</a>\n\n### Luminal:\n\n<a href=\"https://luminalai.com/\">\n<img width=256 alt=\"luminal logo\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/FIHsRdjMMP0HUjebiuJyH.png\">\n</a>\n\n# Quantized / Smaller Variants\n\nHermes 4 is available as BF16 original weights as well as FP8 variants and GGUF variants by LM Studio.\n\nBF16: https://huggingface.co/NousResearch/Hermes-4-405B\n\nGGUF (Courtesy of LM Studio team!):\nhttps://huggingface.co/lmstudio-community/Hermes-4-405B-GGUF\n\nHermes 4 is also available in smaller sizes (e.g., 70B and 14B) with similar prompt formats.\n\nSee the Hermes 4 collection to explore them all:\nhttps://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728\n\n# How to cite\n\n```bibtex\n@misc{teknium2025hermes4technicalreport,\n      title={Hermes 4 Technical Report}, \n      author={Ryan Teknium and Roger Jin and Jai Suphavadeeprasit and Dakota Mahan and Jeffrey Quesnelle and Joe Li and Chen Guang and Shannon Sands and Karan Malhotra},\n      year={2025},\n      eprint={2508.18255},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2508.18255}, \n}\n```",
      "public": true,
      "version": "34e701f9-bce8-57d4-bb9f-98c40f112738",
      "tool_description": null,
      "slug": "chutes-nousresearch-hermes-4-405b-fp8",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-08-26T19:20:46.066862Z",
      "updated_at": "2025-08-26T19:24:13.157388Z",
      "image": {
        "image_id": "1d1e9fa5-0611-541e-a698-9285501b0850",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "nightly-2025082104",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-22T17:13:11.764204Z",
        "build_started_at": "2025-08-22T17:13:19.339446Z",
        "build_completed_at": "2025-08-22T17:35:37.105724Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "h200"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "h200"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 5.4,
        "supported_gpus": [
          "h200"
        ]
      },
      "invocation_count": 208149,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.1999188,
            "tao": 0.0005888081599546199
          },
          "output": {
            "usd": 0.800064,
            "tao": 0.0023563777477952695
          }
        },
        "usd": {
          "hour": 21.6,
          "second": 0.006
        },
        "tao": {
          "hour": 0.06361710982168654,
          "second": 1.767141939491293e-05
        }
      },
      "instances": [
        {
          "instance_id": "2c3e4482-4e0f-4922-b605-b3a834be7437",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:47:06.244574Z"
        },
        {
          "instance_id": "d286b6cf-9c0f-410c-a7e4-7728b5807294",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:47:06.244574Z"
        },
        {
          "instance_id": "4442698b-364b-4c77-9890-8b18c5d39d3e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T10:08:15.830309Z"
        },
        {
          "instance_id": "8672cf54-19c9-430a-bcb2-0bd25adda04a",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:47:06.244574Z"
        },
        {
          "instance_id": "a14592d5-b0f5-4fcf-ad13-060ce949d306",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:49:24.755247Z"
        },
        {
          "instance_id": "8b170dc1-7b7b-4d39-8d53-31b25033d2e4",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:47:06.244574Z"
        },
        {
          "instance_id": "d108042d-f086-4aa9-bd28-79bd5ec701fd",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:49:34.581933Z"
        },
        {
          "instance_id": "e961bd23-01c5-41df-9170-4818d2bc8a43",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:47:06.244574Z"
        },
        {
          "instance_id": "757dfd4a-f300-4eb4-b420-550b8eb8c64c",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:47:06.244574Z"
        },
        {
          "instance_id": "b4639d3c-6d42-49a4-aeaf-475ebf76eece",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:47:06.244574Z"
        },
        {
          "instance_id": "6e82900a-e29b-41cc-9fde-3aba02b9f8b4",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:47:06.244574Z"
        },
        {
          "instance_id": "1bf31db9-20f1-446d-a08c-d97bc1cd725e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:47:06.244574Z"
        }
      ],
      "logo_id": "b8e0f978-770a-46a3-a9c7-412e4a248708",
      "openrouter": false,
      "chutes_version": "0.3.22",
      "revision": "45f7b629c5edb6aef818192bee44727811581429",
      "concurrency": 16,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/b8e0f978-770a-46a3-a9c7-412e4a248708.webp",
      "hot": true
    },
    {
      "chute_id": "fa4181ad-0cf1-5531-afdb-bbffbf2ff945",
      "name": "deepseek-ai/DeepSeek-V3",
      "tagline": "DeepSeek-V3 is a 671B parameter Mixture-of-Experts language model (with 37B active parameters per token) that excels at reasoning, math, and coding tasks while maintaining strong general capabilities across multiple languages.",
      "readme": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/\ud83e\udd16%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>\ud83d\udc41\ufe0f</a>\n</p>\n\n\n## 1. Introduction\n\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\nIn addition, its training process is remarkably stable. \nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. \n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.png\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Architecture: Innovative Load Balancing Strategy and Training Objective**\n\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n    It can also be used for speculative decoding for inference acceleration. \n\n---\n\n**Pre-Training: Towards Ultimate Training Efficiency**\n\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\n\n---\n\n**Post-Training: Knowledge Distillation from DeepSeek-R1**\n\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\n\n---\n\n\n## 3. Model Downloads\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3-Base | 671B | 37B | 128K   | [\ud83e\udd17 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\n| DeepSeek-V3   | 671B | 37B |  128K   | [\ud83e\udd17 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\n\n</div>\n\n**NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.**\n\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](https://huggingface.co/deepseek-ai/DeepSeek-V3/raw/main/#6-how-to-run-locally).\n\nFor developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](https://huggingface.co/deepseek-ai/DeepSeek-V3/raw/main/README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\n\n## 4. Evaluation Results\n### Base Model\n#### Standard Benchmarks\n\n<div align=\"center\">\n\n\n|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\n|---|-------------------|----------|--------|-------------|---------------|---------|\n| | Architecture | - | MoE | Dense | Dense | MoE |\n| | # Activated Params | - | 21B | 72B | 405B | 37B |\n| | # Total Params | - | 236B | 72B | 405B | 671B |\n| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\n| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\n| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\n| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\n| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\n| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\n| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\n| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\n| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\n| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\n| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\n| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\n| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\n| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\n| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\n| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\n| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\n| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\n| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\n| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\n| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\n| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\n| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\n| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\n| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\n| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\n| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\n| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\n| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\n| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\n| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\n| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\n\n</div>\n\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\nFor more evaluation details, please check our paper. \n\n#### Context Window\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/niah.png\">\n</p>\n\nEvaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. \n\n### Chat Model\n#### Standard Benchmarks (Models larger than 67B)\n<div align=\"center\">\n\n| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\n|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\n| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\n| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\n| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\n| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\n| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\n| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\n| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\n| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\n| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\n| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\n| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\n| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\n| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\n| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\n| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\n| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\n| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\n| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\n| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\n| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\n| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\n| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\n| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\n| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\n| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\n\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\n\n</div>\n\n\n####  Open Ended Generation Evaluation\n\n<div align=\"center\">\n\n\n\n| Model | Arena-Hard | AlpacaEval 2.0 |\n|-------|------------|----------------|\n| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\n| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\n| LLaMA-3.1 405B | 69.3 | 40.5 |\n| GPT-4o-0513 | 80.4 | 51.1 |\n| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\n| DeepSeek-V3 | **85.5** | **70.0** |\n\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-V3 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\n\n1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.\n2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\n3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.\n4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\n5. **vLLM**: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\n6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\n7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.\n\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\n\nHere is an example of converting FP8 weights to BF16:\n\n```shell\ncd inference\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\n```\n\n**NOTE: Huggingface's Transformers has not been directly supported yet.**\n\n### 6.1 Inference with DeepSeek-Infer Demo (example only)\n\n#### Model Weights & Demo Code Preparation\n\nFirst, clone our DeepSeek-V3 GitHub repository:\n\n```shell\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\n```\n\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`.\n\n```shell\ncd DeepSeek-V3/inference\npip install -r requirements.txt\n```\n\nDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.\n\n#### Model Weights Conversion\n\nConvert HuggingFace model weights to a specific format:\n\n```shell\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\n```\n\n#### Run\n\nThen you can chat with DeepSeek-V3:\n\n```shell\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\n```\n\nOr batch inference on a given file:\n\n```shell\ntorchrun",
      "public": true,
      "version": "ae8ce6dc-6eeb-56d5-8348-474760300336",
      "tool_description": null,
      "slug": "chutes-deepseek-ai-deepseek-v3",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "bcc7eb3b-d9ea-5892-b6ce-97ae9dc8e825",
      "created_at": "2025-02-21T16:54:45.697264Z",
      "updated_at": "2025-08-03T06:44:09.536047Z",
      "image": {
        "image_id": "a395302b-270f-5398-9540-8c29dea16f3d",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.4.10.post1",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-08-01T09:18:36.513090Z",
        "build_started_at": "2025-08-01T09:18:38.000857Z",
        "build_completed_at": "2025-08-01T09:42:51.462771Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "h200"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "h200"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 5.4,
        "supported_gpus": [
          "h200"
        ]
      },
      "invocation_count": 172159,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.1999188,
            "tao": 0.0005888081599546199
          },
          "output": {
            "usd": 0.800064,
            "tao": 0.0023563777477952695
          }
        },
        "usd": {
          "hour": 21.6,
          "second": 0.006
        },
        "tao": {
          "hour": 0.06361710982168654,
          "second": 1.767141939491293e-05
        }
      },
      "instances": [
        {
          "instance_id": "aeba0f70-0ccb-4a2b-a2ab-45dde0bb809b",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:42:43.207628Z"
        },
        {
          "instance_id": "065419b4-5583-4826-b5e0-283b7f85a0ae",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:42:43.207628Z"
        },
        {
          "instance_id": "c46e82d1-c2f3-4576-a31a-e8773149887e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:42:43.207628Z"
        },
        {
          "instance_id": "7a6d5ba8-6742-40bf-8fbc-62a5cbee6a3e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:42:43.207628Z"
        },
        {
          "instance_id": "778c1ebe-84ce-4235-bb00-d48d5872231b",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:42:43.207628Z"
        }
      ],
      "logo_id": "b29a24f8-6138-41fc-a6f2-5badb574319d",
      "openrouter": true,
      "chutes_version": "0.3.11",
      "revision": "1d044fd82b15f1cedb197a288e50cc96a2c27205",
      "concurrency": 24,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/b29a24f8-6138-41fc-a6f2-5badb574319d.webp",
      "hot": true
    },
    {
      "chute_id": "761fa230-d74a-55be-a1e7-5caec1381869",
      "name": "TheDrummer/Skyfall-36B-v2",
      "tagline": "Skyfall 36B v2 is a 36 billion parameter LLM based on Mistral that has been optimized for creative writing and roleplay through additional training.",
      "readme": "# Join our Discord! https://discord.gg/Nbv9pQ88Xb\n## Nearly 4000 members strong \ud83d\udcaa Now with more channels! A hub for users and makers alike!\n---\n\n[BeaverAI](https://huggingface.co/BeaverAI) proudly presents...\n\n# Skyfall 36B v2\n\n*Skyfall v2 is an upscaled version of Mistral Small 2501 with continued training for creativity and RP.*\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/65f2fd1c25b848bd061b5c2e/BTVVnSnCRmysFKKqRaDvu.png)\n\n## Special Thanks\n- Thank you to each and everyone who donated and subscribed in [Ko-Fi](https://ko-fi.com/thedrummer) to make our venture a little bit easier.\n- I'm also recently unemployed. I am a Software Developer with 8 years of experience in Web, API, AI, and adapting to new tech and requirements. If you're hiring, feel free to reach out to me however.\n- To commercial hosters of my models: If you profit off someone's work, kindly consider contributing to the cause rather than turning a blind eye to those who provide value and are in need. A subscription/donation to my KoFi would be greatly appreciated!\n\n## Supported Chat Templates\n- Mistral v7 Tekken (highly recommended)\n- Metharme (not recommended)\n- Alpaca (may be interesting, especially for cyoa / story)\n\n## Description\n\n> Creativity, good writing style, good instruct, chain of thought capability, mathematics understanding, and solid tool use performance... This model is peak! This will be my new daily model over all the 70Bs I have used.\n\n> Skyfall v2 is without a doubt my favorite model I've ever managed to run locally, bar none\n\n> skyfall is kinda nuts i am quite impressed\n\n> The biggest stand out for me is how good Skyfall handles size differences especially. It actually beats all of the 70b's I have used for descriptions of how the character worked around our size difference.\n\n> I played with the Skyfall 3bit model, taking a new character card with which I had not previously RP'd, and damn, it was so alive! The character's speech was conceptually correct, not as dry as 24b, there was a comedy tag and damn I really laughed in places. I really liked it, maybe it was the specific prompt that played great together with Skyfall.\n\n> Seriously though, Skyfall is just insanely good for some reason\n\n> How did you make skyfall so good\n\n## Links\n- Original: https://huggingface.co/TheDrummer/Skyfall-36B-v2\n- GGUF: https://huggingface.co/TheDrummer/Skyfall-36B-v2-GGUF\n- iMatrix (recommended): https://huggingface.co/bartowski/TheDrummer_Skyfall-36B-v2-GGUF\n\n![image/gif](https://cdn-uploads.huggingface.co/production/uploads/65f2fd1c25b848bd061b5c2e/KU3Y3T0T9tT_OhUG_4iGR.gif)\n",
      "public": true,
      "version": "46693275-ecd2-5033-8019-2935cfc2a765",
      "tool_description": null,
      "slug": "chutes-thedrummer-skyfall-36b-v2",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-07-21T07:52:46.533973Z",
      "updated_at": "2025-07-28T23:36:12.726620Z",
      "image": {
        "image_id": "bfe9a3cd-0245-5d76-bb94-df43ef4007c0",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "0.4.9.dev8",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-07-28T13:16:23.443928Z",
        "build_started_at": "2025-07-28T13:16:26.868164Z",
        "build_completed_at": "2025-07-28T13:39:24.501249Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "a100",
        "a100_sxm",
        "l40s",
        "h100_sxm",
        "h100",
        "h200",
        "a6000_ada",
        "h800"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "l40s",
          "a6000_ada",
          "a100",
          "a100_sxm",
          "h100",
          "h100_sxm",
          "h800",
          "h200"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 1.3,
        "supported_gpus": [
          "a100",
          "a100_sxm",
          "l40s",
          "h100_sxm",
          "h100",
          "h200",
          "a6000_ada",
          "h800"
        ]
      },
      "invocation_count": 155731,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.0481286,
            "tao": 0.00014175011258166773
          },
          "output": {
            "usd": 0.192608,
            "tao": 0.0005672761244692316
          }
        },
        "usd": {
          "hour": 5.2,
          "second": 0.0014444444444444446
        },
        "tao": {
          "hour": 0.015315230142257873,
          "second": 4.254230595071631e-06
        }
      },
      "instances": [
        {
          "instance_id": "d1b5b7f4-012d-4b67-b5e0-ca5a3ce1c9ad",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:55:25.926855Z"
        },
        {
          "instance_id": "25ed8c40-b8f8-45a5-84d3-ee6cf8df7363",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:59:05.268809Z"
        },
        {
          "instance_id": "3de1f0f7-b522-45b1-b10b-89520834fea6",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:55:33.115122Z"
        },
        {
          "instance_id": "089a36bb-0c76-4bee-aeb9-0294834cd73e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:55:27.239578Z"
        },
        {
          "instance_id": "3cd67bce-9155-4510-9f32-249b3b502984",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:55:29.251615Z"
        }
      ],
      "logo_id": "ce775dbe-2f75-47bd-a4e1-10dd3733d154",
      "openrouter": false,
      "chutes_version": "0.3.10",
      "revision": "8d5f36e2fbf323fc17157ba8d8bc9716927825a5",
      "concurrency": 16,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/ce775dbe-2f75-47bd-a4e1-10dd3733d154.webp",
      "hot": true
    },
    {
      "chute_id": "a805bf90-0237-5b55-8a55-6e8dde54324e",
      "name": "Qwen/Qwen3-235B-A22B-Thinking-2507",
      "tagline": "Qwen3-235B-A22B-Thinking-2507 is a 235B parameter (with 22B activated) language model optimized for complex reasoning tasks like mathematics, science and coding, featuring enhanced thinking capabilities and support for context lengths up to 1M tokens.",
      "readme": "\n# Qwen3-235B-A22B-Thinking-2507\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Highlights\n\nOver the past three months, we have continued to scale the **thinking capability** of Qwen3-235B-A22B, improving both the **quality and depth** of reasoning. We are pleased to introduce **Qwen3-235B-A22B-Thinking-2507**, featuring the following key enhancements:\n- **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise \u2014 achieving **state-of-the-art results among open-source thinking models**.\n- **Markedly better general capabilities**, such as instruction following, tool usage, text generation, and alignment with human preferences.\n- **Enhanced 256K long-context understanding** capabilities.\n\n**NOTE**: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\n\n![image/jpeg](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-2507/Qwen3-235B-A22B-Thinking-2507.jpeg)\n\n## Model Overview\n\n**Qwen3-235B-A22B-Thinking-2507** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 235B in total and 22B activated\n- Number of Paramaters (Non-Embedding): 234B\n- Number of Layers: 94\n- Number of Attention Heads (GQA): 64 for Q and 4 for KV\n- Number of Experts: 128\n- Number of Activated Experts: 8\n- Context Length: **262,144 natively**. \n\n**NOTE: This model supports only thinking mode.**\n\nAdditionally, to enforce model thinking, the default chat template automatically includes `<think>`. Therefore, it is normal for the model's output to contain only `</think>` without an explicit opening `<think>` tag.\n  \nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Performance\n\n\n|  | Deepseek-R1-0528 | OpenAI O4-mini | OpenAI O3 | Gemini-2.5 Pro | Claude4 Opus Thinking | Qwen3-235B-A22B Thinking | Qwen3-235B-A22B-Thinking-2507 |\n|--- | --- | --- | --- | --- | --- | --- | --- |\n| **Knowledge** | | | | | | | |\n| MMLU-Pro | 85.0 | 81.9 | **85.9** | 85.6 | - | 82.8 | 84.4 |\n| MMLU-Redux | 93.4 | 92.8 | **94.9** | 94.4 | 94.6 | 92.7 | 93.8 |\n| GPQA | 81.0 | 81.4* | 83.3* | **86.4** | 79.6 | 71.1 | 81.1 |\n| SuperGPQA | 61.7 | 56.4 | - | 62.3 | - | 60.7 | **64.9** |\n| **Reasoning** | | | | | | |\n| AIME25 | 87.5 | **92.7*** | 88.9* | 88.0 | 75.5 | 81.5 | 92.3 |\n| HMMT25 | 79.4 | 66.7 | 77.5 | 82.5 | 58.3 | 62.5 | **83.9** |\n| LiveBench 20241125 | 74.7 | 75.8 | 78.3 | **82.4** | 78.2 | 77.1 | 78.4 |\n| HLE | 17.7# | 18.1* | 20.3 | **21.6** | 10.7 | 11.8# | 18.2# |\n| **Coding** | | | | | | | |\n| LiveCodeBench v6 (25.02-25.05) | 68.7 | 71.8 | 58.6 | 72.5 | 48.9 | 55.7 | **74.1** |\n| CFEval | 2099 | 1929 | 2043 | 2001 | - | 2056 | **2134** |\n| OJBench | 33.6 | 33.3 | 25.4 | **38.9** | - | 25.6 | 32.5 |\n| **Alignment** | | | | | | | |\n| IFEval | 79.1 | **92.4** | 92.1 | 90.8 | 89.7 | 83.4 | 87.8 |\n| Arena-Hard v2$ | 72.2 | 59.3 | **80.8** | 72.5 | 59.1 | 61.5 | 79.7 |\n| Creative Writing v3 | 86.3 | 78.8 | **87.7** | 85.9 | 83.8 | 84.6 | 86.1 |\n| WritingBench | 83.2 | 78.4 | 85.3 | 83.1 | 79.1 | 80.3 | **88.3** |\n| **Agent** | | | | | | | |\n| BFCL-v3 | 63.8 | 67.2 | **72.4** | 67.2 | 61.8 | 70.8 | 71.9 |\n| TAU1-Retail | 63.9 | 71.8 | 73.9 | **74.8** | - | 54.8 | 67.8 |\n| TAU1-Airline | **53.5** | 49.2 | 52.0 | 52.0 | - | 26.0 | 46.0 |\n| TAU2-Retail | 64.9 | 71.0 | **76.3** | 71.3 | - | 40.4 | 71.9 |\n| TAU2-Airline | 60.0 | 59.0 | **70.0** | 60.0 | - | 30.0 | 58.0 |\n| TAU2-Telecom | 33.3 | 42.0 | **60.5** | 37.4 | - | 21.9 | 45.6 |\n| **Multilingualism** | | | | | | | |\n| MultiIF | 63.5 | 78.0 | 80.3 | 77.8 | - | 71.9 | **80.6** |\n| MMLU-ProX | 80.6 | 79.0 | 83.3 | **84.7** | - | 80.0 | 81.0 |\n| INCLUDE | 79.4 | 80.8 | **86.6** | 85.1 | - | 78.7 | 81.0 |\n| PolyMATH | 46.9 | 48.7 | 49.7 | 52.2 | - | 54.7 | **60.1** |\n\n\\* For OpenAI O4-mini and O3, we use a medium reasoning effort, except for scores marked with *, which are generated using high reasoning effort.\n\n\\# According to the official evaluation criteria of HLE, scores marked with \\# refer to models that are not multi-modal and were evaluated only on the text-only subset.\n\n$ For reproducibility, we report the win rates evaluated by GPT-4.1.\n\n\\& For highly challenging tasks (including PolyMATH and all reasoning and coding tasks), we use an output length of 81,920 tokens. For all other tasks, we set the output length to 32,768.\n\n\n## Quickstart\n\nThe code of Qwen3-MoE has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: 'qwen3_moe'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-235B-A22B-Thinking-2507\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content) # no opening <think> tag\nprint(\"content:\", content)\n\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-Thinking-2507 --tp 8 --context-length 262144  --reasoning-parser deepseek-r1\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-235B-A22B-Thinking-2507 --tensor-parallel-size 8 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\n**Note: If you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value. However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072 when possible.**\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\n# Using Alibaba Cloud Model Studio\nllm_cfg = {\n    'model': 'qwen3-235b-a22b-thinking-2507',\n    'model_type': 'qwen_dashscope',\n}\n\n# Using OpenAI-compatible API endpoint. It is recommended to disable the reasoning and the tool call parsing\n# functionality of the deployment frameworks and let Qwen-Agent automate the related operations. For example, \n# `VLLM_USE_MODELSCOPE=true vllm serve Qwen/Qwen3-235B-A22B-Thinking-2507 --served-model-name Qwen3-235B-A22B-Thinking-2507 --tensor-parallel-size 8 --max-model-len 262144`.\n#\n# llm_cfg = {\n#     'model': 'Qwen3-235B-A22B-Thinking-2507',\n# \n#     # Use a custom endpoint compatible with OpenAI API:\n#     'model_server': 'http://localhost:8000/v1',  # api_base without reasoning and tool call parsing\n#     'api_key': 'EMPTY',\n#     'generate_cfg': {\n#         'thought_in_content': True,\n#     },\n# }\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Processing Ultra-Long Texts\n\nTo support **ultra-long context processing** (up to **1 million tokens**), we integrate two key techniques:\n\n- **[Dual Chunk Attention](https://arxiv.org/abs/2402.17463) (DCA)**: A length extrapolation method that splits long sequences into manageable chunks while preserving global coherence.\n- **[MInference](https://arxiv.org/abs/2407.02490)**: A sparse attention mechanism that reduces computational overhead by focusing on critical token interactions.\n\nTogether, these innovations significantly improve both **generation quality** and **inference efficiency** for sequences beyond 256K tokens. On sequences approaching 1M tokens, the system achieves up to a **3\u00d7 speedup** compared to standard attention implementations.\n\nFor full technical details, see the [Qwen2.5-1M Technical Report](https://arxiv.org/abs/2501.15383).\n\n### How to Enable 1M Token Context\n\n> [!NOTE]\n> To effectively process a 1 million token context, users will require approximately **1000 GB** of total GPU memory. This accounts for model weights, KV-cache storage, and peak activation memory demands.\n\n#### Step 1: Update Configuration File\n\nDownload the model and replace the content of your `config.json` with `config_1m.json`, which includes the config for length extrapolation and sparse attention.\n\n```bash\nexport MODELNAME=Qwen3-235B-A22B-Thinking-2507\nhuggingface-cli download Qwen/${MODELNAME} --local-dir ${MODELNAME}\nmv ${MODELNAME}/config.json ${MODELNAME}/config.json.bak\nmv ${MODELNAME}/config_1m.json ${MODELNAME}/config.json\n```\n\n#### Step 2: Launch Model Server\n\nAfter updating the config, proceed with either **vLLM** or **SGLang** for serving the model.\n\n#### Option 1: Using vLLM\n\nTo run Qwen with 1M context support:\n\n```bash\npip install -U vllm \\\n    --torch-backend=auto \\\n    --extra-index-url https://wheels.vllm.ai/nightly\n```\n\nThen launch the server with Dual Chunk Flash Attention enabled:\n\n```bash\nVLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN VLLM_USE_V1=0 \\\nvllm serve ./Qwen3-235B-A22B-Thinking-2507 \\\n  --tensor-parallel-size 8 \\\n  --max-model-len 1010000 \\\n  --enable-chunked-prefill \\\n  --max-num-batched-tokens 131072 \\\n  --enforce-eager \\\n  --max-num-seqs 1 \\\n  --gpu-memory-utilization 0.85 \\\n  --enable-reasoning --reasoning-parser deepseek_r1\n```\n\n##### Key Parameters\n\n| Parameter | Purpose |\n|--------|--------|\n| `VLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN` | Enables the custom attention kernel for long-context efficiency |\n| `--max-model-len 1010000` | Sets maximum context length to ~1M tokens |\n| `--enable-chunked-prefill` | Allows chunked prefill for very long inputs (avoids OOM) |\n| `--max-num-batched-tokens 131072` | Controls batch size during prefill; balances throughput and memory |\n| `--enforce-eager` | Disables CUDA graph capture (required for dual chunk attention) |\n| `--max-num-seqs 1` | Limits concurrent sequences due to extreme memory usage |\n| `--gpu-memory-utilization 0.85` | Set the fraction of GPU memory to be used for the model executor |\n\n#### Option 2: Using SGLang\n\nFirst, clone and install the specialized branch:\n\n```bash\ngit clone https://github.com/sgl-project/sglang.git\ncd sglang\npip install -e \"python[all]\"\n```\n\nLaunch the server with DCA support:\n\n```bash\npython3 -m sglang.launch_server \\\n    --model-path ./Qwen3-235B-A22B-Thinking-2507 \\\n    --context-length 1010000 \\\n    --mem-frac 0.75 \\\n    --attention-backend dual_chunk_flash_attn \\\n    --tp 8 \\\n    --chunked-prefill-size 131072 \\\n    --reasoning-parser deepseek-r1\n```\n\n##### Key Parameters\n\n| Parameter | Purpose |\n|---------|--------|\n| `--attention-backend dual_chunk_flash_attn` | Activates Dual Chunk Flash Attention |\n| `--context-length 1010000` | Defines max input length |\n| `--mem-frac 0.75` | The fraction of the memory used for static allocation (model weights and KV cache memory pool). Use a smaller value if you see out-of-memory errors. |\n| `--tp 8` | Tensor parallelism size (matches model sharding) |\n| `--chunked-prefill-size 131072` | Prefill chunk size for handling long inputs without OOM |\n\n#### Troubleshooting:\n\n1. Encountering the error: \"The model's max sequence length (xxxxx) is larger than the maximum number of tokens that can be stored in the KV cache.\" or \"RuntimeError: Not enough memory. Please try to increase --mem-fraction-static.\"\n\n    The VRAM reserved for the KV cache is insufficient.\n    - vLLM: Consider reducing the ``max_model_len`` or increasing the ``tensor_parallel_size`` and ``gpu_memory_utilization``. Alternatively, you can reduce ``max_num_batched_tokens``, although this may significantly slow down inference.\n    - SGLang: Consider reducing the ``context-length`` or increasing the ``tp`` and ``mem-frac``. Alternatively, you can reduce ``chunked-prefill-size``, although this may significantly slow down inference.\n\n2. Encountering the error: \"torch.OutOfMemoryError: CUDA out of memory.\"\n\n    The VRAM reserved for activation weights is insufficient. You can try lowering ``gpu_memory_utilization`` or ``mem-frac``, but be aware that this might reduce the VRAM available for the KV cache.\n\n3. Encountering the error: \"Input prompt (xxxxx tokens) + lookahead slots (0) is too long and exceeds the capacity of the block manager.\" or \"The input (xxx xtokens) is longer than the model's context length (xxx tokens).\"\n\n    The input is too lengthy. Consider using a shorter sequence or increasing the ``max_model_len`` or ``context-length``.\n\n#### Long-Context Performance\n\nWe test the model on an 1M version of the [RULER](https://arxiv.org/abs/2404.06654) benchmark.\n\n| Model Name                                  | Acc avg | 4k   | 8k   | 16k  | 32k  | 64k  | 96k  | 128k | 192k | 256k | 384k | 512k | 640k | 768k | 896k | 1000k |\n|---------------------------------------------|---------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|-------|\n| Qwen3-235B-A22B (Thinking)                  | 82.9    | 97.3 | 95.9 | 95.3 | 88.7 | 91.7 | 91.5 | 87.9 | 85.4 | 78.4 | 75.6 | 73.7 | 73.6 | 70.6 | 69.9 | 67.6  |\n| Qwen3-235B-A22B-Thinking-2507 (Full Attention)   | 95.4    | 99.6 | 100.0| 99.5 | 99.6 | 99.1 | 100.0| 98.8 | 98.1 | 96.1 | 95.2 | 90.0 | 91.7 | 89.7 | 87.9 | 85.9  |\n| Qwen3-235B-A22B-Thinking-2507 (Sparse Attention) | 95.5 | 100.0| 100.0| 100.0| 100.0| 98.6 | 99.5 | 98.8 | 98.1 | 95.4 | 93.0 | 90.7 | 91.9 | 91.7 | 87.8 | 86.6  |\n\n\n* All models are evaluated with Dual Chunk Attention enabled.\n* Since the evaluation is time-consuming, we use 260 samples for each length (1",
      "public": true,
      "version": "2f7641f0-f5f6-5c7a-be9c-cc9cf25d35cc",
      "tool_description": null,
      "slug": "chutes-qwen-qwen3-235b-a22b-thinking-2507",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-07-25T11:02:48.242850Z",
      "updated_at": "2025-09-06T11:55:25.306688Z",
      "image": {
        "image_id": "468c185f-2e19-56d6-9508-6be534e42145",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "nightly-2025090600",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-09-06T09:40:20.310953Z",
        "build_started_at": "2025-09-06T09:40:22.578714Z",
        "build_completed_at": "2025-09-06T09:56:36.126262Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "h200"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "h200"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 5.4,
        "supported_gpus": [
          "h200"
        ]
      },
      "invocation_count": 135048,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.077968332,
            "tao": 0.00022963518238230173
          },
          "output": {
            "usd": 0.31202496,
            "tao": 0.0009189873216401552
          }
        },
        "usd": {
          "hour": 8.424000000000001,
          "second": 0.00234
        },
        "tao": {
          "hour": 0.02481067283045775,
          "second": 6.891853564016043e-06
        }
      },
      "instances": [
        {
          "instance_id": "7e063c09-7099-4a9b-b48d-3400e1b8d91e",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:56:30.488689Z"
        },
        {
          "instance_id": "c4811494-5be2-4846-927e-eb2957a843a6",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:24:55.857256Z"
        },
        {
          "instance_id": "bd6e69e6-20fa-45ec-b960-fe8816e0e762",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:56:30.488689Z"
        },
        {
          "instance_id": "d794e8fc-e9fe-42ee-8cb3-f2c3db5b21c4",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:04:39.169888Z"
        },
        {
          "instance_id": "a3f3e81d-8379-4c66-b4e8-b8457c485df5",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:56:30.488689Z"
        },
        {
          "instance_id": "48ab0297-2fa0-4c51-9e5e-c04d1a0797a2",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T08:56:30.488689Z"
        }
      ],
      "logo_id": "88159a2e-7d65-4169-80f5-ae598fa60f92",
      "openrouter": false,
      "chutes_version": "0.3.23.rc1",
      "revision": "4cd68849b2a0fadb84866c703b133aa8c8636130",
      "concurrency": 64,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/88159a2e-7d65-4169-80f5-ae598fa60f92.webp",
      "hot": true
    },
    {
      "chute_id": "aef797d4-f375-5beb-9986-3ad245947469",
      "name": "tngtech/DeepSeek-R1T-Chimera",
      "tagline": "DeepSeek-R1T-Chimera is an open-source LLM that combines DeepSeek-R1 and DeepSeek-V3 models to create a more token-efficient and intelligent system for general-purpose tasks and applications.",
      "readme": "# DeepSeek-R1T-Chimera\n\n<div align=\"center\">\n<img src=\"https://354918363417-runtime-assets.s3.eu-central-1.amazonaws.com/company_logo_light.svg\"\n     alt=\"TNG Logo\" \n     width=\"400\"\n     style=\"display: inline-block; vertical-align: middle;\"/>\n</div>\n<br>\n<div align=\"center\">\n  <a href=\"LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n<br>\n<div align=\"center\">\n  <a href=\"https://x.com/tngtech/status/1916284566127444468\" style=\"margin: 2px;\">\n    <img alt=\"Benchmarks\" src=\"R1T-Chimera_Benchmarks_20250427_V1.jpg\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n**Model merge of DeepSeek-R1 and DeepSeek-V3 (0324)**\n\nAn open weights model combining the intelligence of R1 with the token efficiency of V3.\n\nFor details on the construction process and analyses of Chimera model variants, please [read our paper](https://arxiv.org/abs/2506.14794). \n\n[Paper on arXiV](https://arxiv.org/abs/2506.14794) | [Announcement on X](https://x.com/tngtech/status/1916284566127444468) | [LinkedIn post](https://www.linkedin.com/posts/tng-technology-consulting_on-the-weekend-we-released-deepseek-r1t-chimera-activity-7323008947236290560-Cf2m) | [Try it on OpenRouter](https://openrouter.ai/tngtech/deepseek-r1t-chimera:free)\n\n**Update:** we released [R1T2-Chimera](https://huggingface.co/tngtech/DeepSeek-TNG-R1T2-Chimera) that is both faster *and* smarter than R1.\n\n\n## Model Details\n\n- **Architecture**: DeepSeek-MoE Transformer-based language model\n- **Combination Method**: Merged model weights from DeepSeek-R1 and DeepSeek-V3 (0324)\n- **Release Date**: 2025-04-27\n\n## Use, Out-of-scope Use, Limitations, Risks, Recommendations et al\nRegarding R1T Chimera, we ask you to follow the careful guidelines that Microsoft has created for their \"MAI-DS-R1\" DeepSeek-based model. \n\nThese guidelines are available [here on Hugging Face](https://huggingface.co/microsoft/MAI-DS-R1).\n\n## Contact\n\n- Email: research@tngtech.com\n- X.com: @tngtech\n\n## Citation\n\n```\n@misc{tng_technology_consulting_gmbh_2025,\n\tauthor       = { TNG Technology Consulting GmbH },\n\ttitle        = { DeepSeek-R1T-Chimera },\n\tyear         = 2025,\n    month        = {April},\n\turl          = { https://huggingface.co/tngtech/DeepSeek-R1T-Chimera },\n\tdoi          = { 10.57967/hf/5330 },\n\tpublisher    = { Hugging Face }\n}\n\n```",
      "public": true,
      "version": "f41a4897-86db-5864-9e4d-c32bab6a17a6",
      "tool_description": null,
      "slug": "chutes-tngtech-deepseek-r1t-chimera",
      "standard_template": "vllm",
      "cords": null,
      "jobs": [],
      "cord_ref_id": "a251c166-2e46-55cf-93d7-08aaea2069c7",
      "created_at": "2025-04-27T08:27:51.957755Z",
      "updated_at": "2025-09-06T10:34:07.724348Z",
      "image": {
        "image_id": "468c185f-2e19-56d6-9508-6be534e42145",
        "name": "sglang",
        "readme": "SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.",
        "tag": "nightly-2025090600",
        "public": true,
        "status": "built and pushed",
        "created_at": "2025-09-06T09:40:20.310953Z",
        "build_started_at": "2025-09-06T09:40:22.578714Z",
        "build_completed_at": "2025-09-06T09:56:36.126262Z",
        "user": {
          "username": "chutes",
          "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
          "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
          "created_at": "2024-12-03T11:31:39.181725Z",
          "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
        },
        "logo_id": null,
        "patch_version": "initial",
        "logo": null
      },
      "user": {
        "username": "chutes",
        "user_id": "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f",
        "logo_id": "24ef9084-7739-400c-aab6-135463a8c2d9",
        "created_at": "2024-12-03T11:31:39.181725Z",
        "logo": "https://logos.chutes.ai/logos/24ef9084-7739-400c-aab6-135463a8c2d9.webp"
      },
      "supported_gpus": [
        "h200"
      ],
      "node_selector": {
        "exclude": null,
        "include": [
          "h200"
        ],
        "gpu_count": 8,
        "min_vram_gb_per_gpu": 16,
        "compute_multiplier": 5.4,
        "supported_gpus": [
          "h200"
        ]
      },
      "invocation_count": 133287,
      "current_estimated_price": {
        "per_million_tokens": {
          "input": {
            "usd": 0.1999188,
            "tao": 0.0005888081599546199
          },
          "output": {
            "usd": 0.800064,
            "tao": 0.0023563777477952695
          }
        },
        "usd": {
          "hour": 21.6,
          "second": 0.006
        },
        "tao": {
          "hour": 0.06361710982168654,
          "second": 1.767141939491293e-05
        }
      },
      "instances": [
        {
          "instance_id": "f7f80c5f-0de8-49ba-93f2-470fe8e698fd",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:01:01.180947Z"
        },
        {
          "instance_id": "989fd617-d567-416a-880d-f5e0fce31294",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:01:01.180947Z"
        },
        {
          "instance_id": "27a2c68d-6b15-4340-a3fc-af9c6d37a475",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:01:01.180947Z"
        },
        {
          "instance_id": "41c9909b-8782-4fd3-a8b1-7b174999c58b",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:01:01.180947Z"
        },
        {
          "instance_id": "d91f886b-30e9-4f52-be12-190f91349bb6",
          "region": "n/a",
          "active": true,
          "verified": true,
          "last_verified_at": "2025-09-08T09:01:01.180947Z"
        }
      ],
      "logo_id": "484841f4-8a5d-42b4-8e08-ddbf4506172f",
      "openrouter": true,
      "chutes_version": "0.3.23.rc1",
      "revision": "66488d22ff2766fa9ce829c7445aea7618b1aa4e",
      "concurrency": 40,
      "boost": null,
      "logging_enabled": false,
      "logo": "https://logos.chutes.ai/logos/484841f4-8a5d-42b4-8e08-ddbf4506172f.webp",
      "hot": true
    }
  ],
  "cord_refs": {
    "a251c166-2e46-55cf-93d7-08aaea2069c7": [
      {
        "path": "/chat_stream",
        "method": "POST",
        "stream": true,
        "function": "chat_stream",
        "passthrough": true,
        "public_api_path": "/v1/chat/completions",
        "public_api_method": "POST",
        "output_content_type": "application/json"
      },
      {
        "path": "/completion_stream",
        "method": "POST",
        "stream": true,
        "function": "completion_stream",
        "passthrough": true,
        "public_api_path": "/v1/completions",
        "public_api_method": "POST",
        "output_content_type": "application/json"
      },
      {
        "path": "/chat",
        "method": "POST",
        "stream": false,
        "function": "chat",
        "passthrough": true,
        "public_api_path": "/v1/chat/completions",
        "public_api_method": "POST",
        "output_content_type": "application/json"
      },
      {
        "path": "/completion",
        "method": "POST",
        "stream": false,
        "function": "completion",
        "passthrough": true,
        "public_api_path": "/v1/completions",
        "public_api_method": "POST",
        "output_content_type": "application/json"
      },
      {
        "path": "/get_models",
        "method": "GET",
        "stream": false,
        "function": "get_models",
        "passthrough": true,
        "public_api_path": "/v1/models",
        "public_api_method": "GET",
        "output_content_type": null
      }
    ],
    "bcc7eb3b-d9ea-5892-b6ce-97ae9dc8e825": [
      {
        "path": "/chat_stream",
        "method": "POST",
        "stream": true,
        "function": "chat_stream",
        "passthrough": true,
        "public_api_path": "/v1/chat/completions",
        "public_api_method": "POST",
        "output_content_type": "application/json"
      },
      {
        "path": "/completion_stream",
        "method": "POST",
        "stream": true,
        "function": "completion_stream",
        "passthrough": true,
        "public_api_path": "/v1/completions",
        "public_api_method": "POST",
        "output_content_type": "application/json"
      },
      {
        "path": "/chat",
        "method": "POST",
        "stream": false,
        "function": "chat",
        "passthrough": true,
        "public_api_path": "/v1/chat/completions",
        "public_api_method": "POST",
        "output_content_type": "application/json"
      },
      {
        "path": "/completion",
        "method": "POST",
        "stream": false,
        "function": "completion",
        "passthrough": true,
        "public_api_path": "/v1/completions",
        "public_api_method": "POST",
        "output_content_type": "application/json"
      },
      {
        "path": "/get_models",
        "method": "GET",
        "stream": false,
        "function": "get_models",
        "passthrough": true,
        "public_api_path": "/v1/models",
        "public_api_method": "GET",
        "output_content_type": null
      }
    ]
  }
}