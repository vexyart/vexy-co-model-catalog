Project Structure:
📁 vexy-co-model-catalog
├── 📁 .github
│   └── 📁 workflows
│       ├── 📄 push.yml
│       └── 📄 release.yml
├── 📁 config
│   ├── 📁 aichat
│   │   └── 📄 config.yaml
│   ├── 📁 codex
│   │   └── 📄 config.toml
│   └── 📁 mods
│       └── 📄 mods.yml
├── 📁 external
│   ├── 📁 bak
│   ├── 📄 chutes_chutes.json
│   ├── 📄 dump_models.py
│   └── 📄 failed_models.json
├── 📁 src
│   └── 📁 vexy_co_model_catalog
│       ├── 📁 core
│       │   ├── 📄 __init__.py
│       │   ├── 📄 catalog.py
│       │   ├── 📄 fetcher.py
│       │   ├── 📄 provider.py
│       │   └── 📄 storage.py
│       ├── 📁 utils
│       │   ├── 📄 __init__.py
│       │   └── 📄 exceptions.py
│       ├── 📄 __init__.py
│       ├── 📄 __main__.py
│       ├── 📄 _version.py
│       ├── 📄 cli.py
│       └── 📄 vexy_co_model_catalog.py
├── 📁 tests
│   └── 📄 test_package.py
├── 📄 .gitignore
├── 📄 AGENTS.md
├── 📄 CLAUDE.md
├── 📄 GEMINI.md
├── 📄 LICENSE
├── 📄 package.toml
├── 📄 PLAN.md
├── 📄 pyproject.toml
├── 📄 QWEN.md
├── 📄 README.md
└── 📄 TODO.md


<documents>
<document index="1">
<source>.cursorrules</source>
<document_content>


# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Work in rounds: 

- Create `PLAN.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PLAN.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</document_content>
</document>

<document index="2">
<source>.github/workflows/push.yml</source>
<document_content>
name: Build & Test

on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"

      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"

  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}

      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"

      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/vexy_co_model_catalog --cov=tests tests/

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml

  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5 
</document_content>
</document>

<document index="3">
<source>.github/workflows/release.yml</source>
<document_content>
name: Release

on:
  push:
    tags: ["v*"]

permissions:
  contents: write
  id-token: write

jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/vexy-co-model-catalog
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} 
</document_content>
</document>

<document index="4">
<source>.gitignore</source>
<document_content>
*_autogen/
.DS_Store
__version__.py
__pycache__/
_Chutzpah*
_deps
_NCrunch_*
_pkginfo.txt
_Pvt_Extensions
_ReSharper*/
_TeamCity*
_UpgradeReport_Files/
!?*.[Cc]ache/
!.axoCover/settings.json
!.vscode/extensions.json
!.vscode/launch.json
!.vscode/settings.json
!.vscode/tasks.json
!**/[Pp]ackages/build/
!Directory.Build.rsp
.*crunch*.local.xml
.axoCover/*
.builds
.cr/personal
.fake/
.history/
.ionide/
.localhistory/
.mfractor/
.ntvs_analysis.dat
.paket/paket.exe
.sass-cache/
.vs/
.vscode
.vscode/*
.vshistory/
[Aa][Rr][Mm]/
[Aa][Rr][Mm]64/
[Bb]in/
[Bb]uild[Ll]og.*
[Dd]ebug/
[Dd]ebugPS/
[Dd]ebugPublic/
[Ee]xpress/
[Ll]og/
[Ll]ogs/
[Oo]bj/
[Rr]elease/
[Rr]eleasePS/
[Rr]eleases/
[Tt]est[Rr]esult*/
[Ww][Ii][Nn]32/
*_h.h
*_i.c
*_p.c
*_wpftmp.csproj
*- [Bb]ackup ([0-9]).rdl
*- [Bb]ackup ([0-9][0-9]).rdl
*- [Bb]ackup.rdl
*.[Cc]ache
*.[Pp]ublish.xml
*.[Rr]e[Ss]harper
*.a
*.app
*.appx
*.appxbundle
*.appxupload
*.aps
*.azurePubxml
*.bim_*.settings
*.bim.layout
*.binlog
*.btm.cs
*.btp.cs
*.build.csdef
*.cab
*.cachefile
*.code-workspace
*.coverage
*.coveragexml
*.d
*.dbmdl
*.dbproj.schemaview
*.dll
*.dotCover
*.DotSettings.user
*.dsp
*.dsw
*.dylib
*.e2e
*.exe
*.gch
*.GhostDoc.xml
*.gpState
*.ilk
*.iobj
*.ipdb
*.jfm
*.jmconfig
*.la
*.lai
*.ldf
*.lib
*.lo
*.log
*.mdf
*.meta
*.mm.*
*.mod
*.msi
*.msix
*.msm
*.msp
*.ncb
*.ndf
*.nuget.props
*.nuget.targets
*.nupkg
*.nvuser
*.o
*.obj
*.odx.cs
*.opendb
*.opensdf
*.opt
*.out
*.pch
*.pdb
*.pfx
*.pgc
*.pgd
*.pidb
*.plg
*.psess
*.publishproj
*.publishsettings
*.pubxml
*.pyc
*.rdl.data
*.rptproj.bak
*.rptproj.rsuser
*.rsp
*.rsuser
*.sap
*.sbr
*.scc
*.sdf
*.sln.docstates
*.sln.iml
*.slo
*.smod
*.snupkg
*.so
*.suo
*.svclog
*.tlb
*.tlh
*.tli
*.tlog
*.tmp
*.tmp_proj
*.tss
*.user
*.userosscache
*.userprefs
*.vbp
*.vbw
*.VC.db
*.VC.VC.opendb
*.VisualState.xml
*.vsp
*.vspscc
*.vspx
*.vssscc
*.xsd.cs
**/[Pp]ackages/*
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.HTMLClient/GeneratedArtifacts
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
*~
~$*
$tf/
AppPackages/
artifacts/
ASALocalRun/
AutoTest.Net/
Backup*/
BenchmarkDotNet.Artifacts/
bld/
BundleArtifacts/
ClientBin/
cmake_install.cmake
CMakeCache.txt
CMakeFiles
CMakeLists.txt.user
CMakeScripts
CMakeUserPresets.json
compile_commands.json
coverage*.info
coverage*.json
coverage*.xml
csx/
CTestTestfile.cmake
dlldata.c
DocProject/buildhelp/
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/*.HxC
DocProject/Help/*.HxT
DocProject/Help/html
DocProject/Help/Html2
ecf/
FakesAssemblies/
FodyWeavers.xsd
Generated_Code/
Generated\ Files/
healthchecksdb
install_manifest.txt
ipch/
Makefile
MigrationBackup/
mono_crash.*
nCrunchTemp_*
node_modules/
nunit-*.xml
OpenCover/
orleans.codegen.cs
Package.StoreAssociation.xml
paket-files/
project.fragment.lock.json
project.lock.json
publish/
PublishScripts/
rcf/
ScaffoldingReadMe.txt
ServiceFabricBackup/
StyleCopReport.xml
Testing
TestResult.xml
UpgradeLog*.htm
UpgradeLog*.XML
x64/
x86/
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Distribution / packaging
!dist/.gitkeep

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
.ruff_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
__version__.py
_private
VERSION.txt

.mypy_cache
</document_content>
</document>

<document index="5">
<source>.pre-commit-config.yaml</source>
<document_content>
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.4
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
        args: [--respect-gitignore]
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
      - id: debug-statements
      - id: check-case-conflict
      - id: mixed-line-ending
        args: [--fix=lf] 
</document_content>
</document>

<document index="6">
<source>AGENTS.md</source>
<document_content>


# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Work in rounds: 

- Create `PLAN.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PLAN.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</document_content>
</document>

<document index="7">
<source>CLAUDE.md</source>
<document_content>


# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Work in rounds: 

- Create `PLAN.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PLAN.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</document_content>
</document>

<document index="8">
<source>GEMINI.md</source>
<document_content>


# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Work in rounds: 

- Create `PLAN.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PLAN.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</document_content>
</document>

<document index="9">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Fontlab Ltd.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</document_content>
</document>

<document index="10">
<source>PLAN.md</source>
<document_content>
---
this_file: PLAN.md
---

# Model Catalog Manager — Detailed SPEC and Plan

## Overview

Goal: Provide a robust, import-safe Python package and CLI to fetch, normalize, and store AI model catalogs from many providers, while keeping minimal surface at import time. The system replaces and extends the functionality of `external/dump_models.py` with a cleaner architecture and better config file integration.

## Analysis of Existing Implementation

Based on `external/dump_models.py` and `external/bak/` artifacts:

### Current Functionality (dump_models.py)
- Fetches models from 45+ AI providers (OpenAI, Anthropic, Groq, etc.)
- Supports 3 provider types: `oai` (OpenAI-compatible), `ant` (Anthropic), `url` (direct JSON)
- Generates 3 file types per provider:
  - `models_<provider>.json` — raw API response
  - `models_<provider>.txt` — simple model ID list
  - `models_<provider>.toml` — Codex-compatible config with profiles
- Special handling for Chutes provider (merges two APIs)
- Failed provider tracking in `failed_models.json`
- 112 files produced total (38 JSON, 38 TXT, 36 TOML)

### Target Config Integration
The new system must integrate with existing config files:
- `config/aichat/config.yaml` — aichat tool configuration
- `config/codex/config.toml` — Codex tool configuration  
- `config/mods/mods.yml` — mods tool configuration

### Enhanced Requirements
1. **Config File Modification**: Update existing tool configs with new providers/models
2. **Per-Provider Config Generation**: Generate tool-specific config files like `config/codex/models_anthropic.toml`
3. **Structured Output Directories**: 
   - JSON files → `config/json/`
   - TXT files → `config/txt/`
   - Tool-specific configs → `config/<tool>/models_<provider>.<ext>`
4. **Provider Coverage**: Match or exceed the 45+ providers from dump_models.py
5. **Compatibility**: Maintain backward compatibility with existing external tool output format



## Architecture

- Core layers
  - utils.exceptions: Typed exception hierarchy for clear failure modes.
  - core.storage: Atomic file I/O for JSON, TOML, TXT, and auxiliary data; simple directory layout under `models/`.
  - core.provider: ProviderKind enum, Model and ProviderConfig data classes. Encapsulates provider metadata and env overrides.
  - core.fetcher: Async JSON fetcher with concurrency limit, retries, and basic error mapping (401/403→AuthenticationError, 429→RateLimitError, others→FetchError).
  - core.catalog: Registry for provider instances and a catalog façade to orchestrate fetching and persistence (to be expanded).

- CLI
  - A Fire-based CLI that keeps imports light. Current commands: `version`, `list_providers` (placeholder).
  - Planned commands: `fetch`, `stats`, `clean`, `providers add/list/remove`, `gen-config` for TOML generation (optional).

- External utility
  - external/dump_models.py: Standalone async script to query provider `/models` endpoints or data URLs, write JSON, TXT, TOML, and maintain a `failed_models.json` registry. This remains a useful companion tool and a reference for provider coverage.

## Data and Storage Model

- Directory layout (root-relative):
  - `models/json/` — per-provider JSON payloads and TOML outputs.
  - `models/text/` — per-provider TXT lists (model IDs).
  - `models/extra/` — auxiliary JSON (summary stats, failures, run metadata).
  - `config/` — optional generated configs for external tools.

- File naming conventions:
  - Provider artifacts: `models_{provider}.json`, `models_{provider}.txt`, `models_{provider}.toml`.
  - Extra artifacts: `summary_stats.json`, `failed_providers.json`, `run_{timestamp}.json`.

## Provider Abstraction

- ProviderConfig: identifies provider kind, URL, headers, and env-based overrides.
- ProviderKind: `openai`, `anthropic`, `url` (direct JSON sources).
- Model: normalized metadata for downstream use (id, provider, costs, limits, flags, optional raw payload).
- Provider instance (future): encapsulates how to build requests (base URL, headers) and how to transform raw payload → List[Model].

## Fetching Flow

- CLI/Orchestrator resolves set of providers to fetch.
- For each provider:
  - Build endpoint URL (e.g., `${BASE}/models` for OpenAI-like providers, or direct URL for `url` kind).
  - Use ModelFetcher.get_json with small retry/backoff.
  - Optionally merge or enhance data for special providers (e.g., chutes), using side API calls.
  - Sort/stabilize JSON for deterministic outputs.
  - Extract model IDs to generate TXT.
  - Build TOML profiles with name, base URL, context window, max output tokens, and any pricing if present.
- Persist artifacts atomically via StorageManager; update auxiliary `failed_providers` and `summary_stats`.

## Error Handling

- AuthenticationError: missing/invalid credentials; provider marked failed.
- RateLimitError: 429 after retries; provider marked failed (with retry info).
- FetchError: network/HTTP/parse issues; provider marked failed.
- StorageError: file I/O problems; surfaced early and fail fast.

## CLI Commands (target)

- `version` — print version.
- `providers list` — show registered providers and status.
- `providers add NAME --kind KIND [--api-key-env ENV] [--base-url BASE] [--base-url-env ENV]` — add/update provider.
- `providers remove NAME` — remove provider.
- `fetch [NAME1,NAME2,...] [--all] [--force] [--max-concurrency N]` — fetch models, write JSON/TXT/TOML, update extras.
- `stats` — show counts from storage plus last summary.
- `clean [--temp]` — remove temp artifacts; optionally add full cleanup with confirmation.

## Testing and Verification

- Unit tests: ensure `import vexy_co_model_catalog` is light and exposes `__version__`.
- Storage roundtrip tests: write/read JSON, TOML, TXT; ensure atomic writes do not leave temp files.
- Fetcher tests (mocked httpx): retry logic and error mapping.
- CLI smoke tests: `version`, `providers list` do not error.

## Migration Notes

- The new minimal modules are import-safe. Higher-level features will be layered in small increments to avoid regressions.

## Future Considerations

- Normalization schema: introduce a simple adapter per provider to map API → Model fields.
- Pricing and context windows: unify extraction across providers.
- Config generators: produce config blocks for third-party tools (e.g., LiteLLM, OpenRouter-compatible clients).
- Caching and ETag/Last-Modified support to reduce bandwidth.


</document_content>
</document>

<document index="11">
<source>QWEN.md</source>
<document_content>


# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Work in rounds: 

- Create `PLAN.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PLAN.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</document_content>
</document>

<document index="12">
<source>README.md</source>
<document_content>
# 



## Features

- Modern Python packaging with PEP 621 compliance
- Type hints and runtime type checking
- Comprehensive test suite and documentation
- CI/CD ready configuration

## Installation

```bash
pip install vexy-co-model-catalog
```

## Usage

```python
import vexy_co_model_catalog
```

## Development

This project uses [Hatch](https://hatch.pypa.io/) for development workflow management.

### Setup Development Environment

```bash
# Install hatch if you haven't already
pip install hatch

# Create and activate development environment
hatch shell

# Run tests
hatch run test

# Run tests with coverage
hatch run test-cov

# Run linting
hatch run lint

# Format code
hatch run format
```

## License

MIT License 
</document_content>
</document>

<document index="13">
<source>TODO.md</source>
<document_content>
---
this_file: TODO.md
---

- [ ] CLI: add `providers` group with `list/add/remove` commands
- [ ] CLI: implement `fetch` command (names or --all, --force, --max-concurrency)
- [ ] CLI: implement `stats` and `clean --temp` wired to StorageManager
- [ ] Catalog: implement fetch orchestration (per-provider tasks, summary, failures)
- [ ] Catalog: persist `summary_stats.json` and `failed_providers.json`
- [ ] Provider: define simple adapter interface to transform raw → List[Model]
- [ ] Provider: implement OpenAI-compatible adapter (list models → id/provider/etc.)
- [ ] Provider: implement URL kind adapter (direct JSON or list)
- [ ] Fetcher: unit tests (mocked) for retries and error mapping
- [ ] Storage: unit tests for atomic writes, read_json, list_files, cleanup
- [ ] CLI: smoke tests for `version` and `providers list`
- [ ] External: align external/dump_models TOML generation with core format
- [ ] Documentation: README updates for usage and CLI examples
- [ ] Add contribution guide for adding new providers/adapters
- [ ] Optional: config generators for third-party tools (TOML sections)


</document_content>
</document>

<document index="14">
<source>config/aichat/config.yaml</source>
<document_content>
---
agent_prelude: null
# ---- clients ----
clients:now I want to commit all fuck
  - api_base: https://api.openai.com/v1
    api_key: sk-proj-***
    type: openai
  - api_base: https://generativelanguage.googleapis.com/v1beta
    api_key: '***'
    patch:
      chat_completions:
        .*:
          body:
            safetySettings:
              - category: HARM_CATEGORY_HARASSMENT
                threshold: BLOCK_NONE
              - category: HARM_CATEGORY_HATE_SPEECH
                threshold: BLOCK_NONE
              - category: HARM_CATEGORY_SEXUALLY_EXPLICIT
                threshold: BLOCK_NONE
              - category: HARM_CATEGORY_DANGEROUS_CONTENT
                threshold: BLOCK_NONE
    type: gemini
  - api_base: https://api.anthropic.com/v1
    api_key: sk-ant-***
    type: claude
  - api_base: https://api.mistral.ai/v1
    api_key: '***'
    name: mistral
    type: openai-compatible
  - api_base: https://api.ai21.com/studio/v1
    api_key: '***'
    name: ai12
    type: openai-compatible
  - api_base: https://api.cohere.ai/v2
    api_key: '***'
    type: cohere
  - api_base: https://api.perplexity.ai
    api_key: pplx-***
    name: perplexity
    type: openai-compatible
  - api_base: https://api.groq.com/openai/v1
    api_key: gsk_***
    name: groq
    type: openai-compatible
  - api_base: https://api.jina.ai/v1
    api_key: jina_***
    name: jina
    type: openai-compatible
  - api_base: https://api.voyageai.com/v1
    api_key: pa-***
    name: voyageai
    type: openai-compatible
  - api_base: https://openrouter.ai/api/v1
    api_key: sk-or-v1-***
    models:
      - max_input_tokens: 2000000
        max_output_tokens: 128000
        name: openrouter/sonoma-sky-alpha
      - max_input_tokens: 2000000
        max_output_tokens: 128000
        name: openrouter/sonoma-dusk-alpha
    name: openrouter
    type: openai-compatible
  - api_base: https://llm.chutes.ai/v1
    api_key: cpk_***
    models:
      - max_input_tokens: 128000
        max_output_tokens: 32767
        name: openai/gpt-oss-120b
      - max_input_tokens: 262144
        max_output_tokens: 32767
        name: Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8
    name: chutes
    type: openai-compatible
  - api_base: https://api.cerebras.ai/v1
    api_key: csk-***
    models:
      - max_input_tokens: 131072
        max_output_tokens: 40000
        name: qwen-3-coder-480b
    name: cerebras
    type: openai-compatible
cmd_prelude: null
# Compress session when token count reaches or exceeds this threshold
compress_threshold: 4000
document_loaders:
  docx: pandoc --to plain $1
  # You can add custom loaders using the following syntax:
  #   <file-extension>: <command-to-load-the-file>
  # Note: Use `$1` for input file and `$2` for output file. If `$2` is omitted, use stdout as output.
  pdf: pdftotext $1 -
editor: cursor
function_calling: true
# ---- apperence ----
highlight: true
# Custom REPL left/right prompts, see https://github.com/sigoden/aichat/wiki/Custom-REPL-Prompt for more details
left_prompt: '{color.green}{?session {?agent {agent}>}{session}{?role /}}{!session {?agent {agent}>}}{role}{?rag @{rag}}{color.cyan}{?session )}{!session >}{color.reset} '
light_theme: false
mapping_tools:
  fs: fs_cat,fs_ls,fs_mkdir,fs_rm,fs_write
model: cerebras:qwen-3-coder-480b
rag_chunk_overlap: null
rag_chunk_size: null
# ---- RAG ----
# See [RAG-Guide](https://github.com/sigoden/aichat/wiki/RAG-Guide) for more details.
rag_embedding_model: null
rag_reranker_model: null
# Defines the query structure using variables like __CONTEXT__ and __INPUT__ to tailor searches to specific needs
rag_template: |
  Answer the query based on the context while respecting the rules. (user query, some textual context and rules, all inside xml tags)

  <context>
  __CONTEXT__
  </context>

  <rules>
  - If you don't know, just say so.
  - If you are not sure, ask for clarification.
  - Answer in the same language as the user query.
  - If the context appears unreadable or of poor quality, tell the user then answer as best as you can.
  - If the answer is not in the context but you think you know the answer, explain that to the user then answer with your own knowledge.
  - Answer directly and without using xml tags.
  </rules>

  <user_query>
  __INPUT__
  </user_query>

# Define document loaders to control how RAG and `.file`/`--file` load files of specific formats.
rag_top_k: 5
# ---- prelude ----
repl_prelude: null
right_prompt: '{color.purple}{?session {?consume_tokens {consume_tokens}({consume_percent}%)}{!consume_tokens {consume_tokens}}}{color.reset}'
save: true
# ---- session ----
# Controls the persistence of the session. if true, auto save; if false, not save; if null, asking the user
save_session: null
save_shell_history: true
# ---- misc ----
serve_addr: 127.0.0.1:8010
# ---- behavior ----
stream: true
# Text prompt used for creating a concise summary of session message
summarize_prompt: TLDR the discussion briefly in 200 words or less to use as a prompt for future context.
# Text prompt used for including the summary of the entire session
summary_prompt: 'This is a TLDR of the chat history as a recap: '
use_tools: null
user_agent: null
wrap: 'no'
wrap_code: false
  # All clients have the following configuration:
  # - type: xxxx
  #   name: xxxx                                      # Only use it to distinguish clients with the same client type. Optional
  #   models:
  #     - name: xxxx                                  # Chat model
  #       max_input_tokens: 100000
  #       supports_vision: true
  #       supports_# ---- function-calling ----
# Visit https://github.com/sigoden/llm-functions for setup instructions
  #     - name: xxxx                                  # Embedding model
  #       type: embedding
  #       default_chunk_size: 1500
  #       max_batch_size: 100
  #     - name: xxxx                                  # Reranker model
  #       type: reranker
  #   patch:                                          # Patch api
  #     chat_completions:                             # Api type, possible values: chat_completions, embeddings, and rerank
  #       <regex>:                                    # The regex to match model names, e.g. '.*' 'gpt-4o' 'gpt-4o|gpt-4-.*'
  #         url: ''                                   # Patch request url
  #         body:                                     # Patch request body
  #           <json>
  #         headers:                                  # Patch request headers
  #           <key>: <value>
  #   extra:
  #     proxy: socks5://127.0.0.1:1080                # Set proxy
  #     connect_timeout: 10                           # Set timeout in seconds for connect to api
</document_content>
</document>

<document index="15">
<source>config/codex/config.toml</source>
<document_content>
approval_policy = "never"
disable_response_storage = false
file_opener = "cursor"
hide_agent_reasoning = true
model = "gpt-5"
model_provider = "openai"
model_reasoning_effort = "low"
model_reasoning_summary = "auto"
project_doc_max_bytes = 32768

[sandbox]
mode = "workspace-write"
network_access = true
writable_roots = [
  "/tmp",
]

[sandbox_workspace_write]
network_access = true

[shell_environment_policy]
exclude = [
  "AWS_*",
  "AZURE_*",
  "GOOGLE_*",
]
ignore_default_excludes = false
include_only = []
inherit = "core"

[tui]
disable_mouse_capture = false

[history]
persistence = "save-all"

# mcp_servers

[mcp_servers]
[mcp_servers.ask-pplx]
alwaysAllow = ["perplexity_ask"]
args = ["/usr/local/lib/node_modules/server-perplexity-ask/dist/index.js"]
command = "/usr/local/bin/node"
type = "stdio"

[mcp_servers.ask-pplx.env]
PERPLEXITY_API_KEY = "pplx-***"

[mcp_servers.codex]
alwaysAllow = ["codex"]
args = ["mcp"]
command = "/Users/adam/bin/codexx"
disabled = false
timeout = 600
active = true

[mcp_servers.control-the-browser]
alwaysAllow = [
  "browser_close",
  "browser_resize",
  "browser_console_messages",
  "browser_handle_dialog",
  "browser_evaluate",
  "browser_file_upload",
  "browser_install",
  "browser_press_key",
  "browser_navigate",
  "browser_type",
  "browser_wait_for",
  "browser_tab_close",
  "browser_tab_select",
  "browser_tab_new",
  "browser_hover",
  "browser_select_option",
  "browser_navigate_back",
  "browser_navigate_forward",
  "browser_tab_list",
  "browser_drag",
  "browser_network_requests",
  "browser_take_screenshot",
  "browser_snapshot",
  "browser_click",
]
args = ["/usr/local/lib/node_modules/@playwright/mcp/cli.js"]
command = "/usr/local/bin/node"
disabled = false
disabledTools = []
timeout = 600
type = "stdio"
active = false

# [mcp_servers.deep-research]
# command = "/usr/local/bin/node"
# args = [
#   "/usr/local/lib/node_modules/@pinkpixel/deep-research-mcp/dist/index.js",
# ]

# [mcp_servers.deep-research.env]
# TAVILY_API_KEY = "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"
# SEARCH_TIMEOUT = "120"
# CRAWL_TIMEOUT = "300"
# MAX_SEARCH_RESULTS = "10"
# CRAWL_MAX_DEPTH = "2"
# CRAWL_LIMIT = "15"
# FILE_WRITE_ENABLED = "true"
# FILE_WRITE_LINE_LIMIT = "300"

[mcp_servers.exa-search]
alwaysAllow = ["web_search_exa"]
args = ["/usr/local/lib/node_modules/exa-mcp-server/build/index.js"]
command = "/usr/local/bin/node"
disabled = false
disabledTools = []
type = "stdio"
active = true

[mcp_servers.exa-search.env]
EXA_API_KEY = "0da89771-a62d-435c-a243-0b49fa05b789"

[mcp_servers.gemini-cli]
alwaysAllow = []
args = ["/usr/local/lib/node_modules/gemini-mcp-tool/dist/index.js"]
command = "/usr/local/bin/node"
disabled = false
disabledTools = []
type = "stdio"
active = true

# [mcp_servers.hyper-mcp]
# alwaysAllow = [
#     "think",
#     "c7_resolve_library_id",
#     "c7_get_library_docs",
#     "fetch",
#     "time",
#     "read_file",
#     "read_multiple_files",
#     "search_files",
#     "get_file_info",
# ]
# args = []
# command = "/Users/adam/.cargo/bin/hyper-mcp"
# disabledTools = [
#     "hash",
#     "myip",
#     "gh-list-issues",
#     "gh-create-issue",
#     "gh-get-issue",
#     "gh-update-issue",
#     "gh-add-issue-comment",
#     "gh-get-file-contents",
#     "gh-create-or-update-file",
#     "gh-create-branch",
#     "gh-list-pull-requests",
#     "gh-create-pull-request",
#     "gh-get-repo-contributors",
#     "gh-get-repo-collaborators",
#     "gh-get-repo-details",
#     "gh-list-repos",
#     "gh-create-gist",
#     "gh-get-gist",
#     "gh-update-gist",
#     "gh-delete-gist",
#     "write_file",
#     "edit_file",
#     "create_dir",
#     "list_dir",
#     "move_file",
# ]
# type = "stdio"
# active = true

# model_providers

[model_providers]
[model_providers.aihorde]
base_url = "https://oai.aihorde.net/v1"
env_key = "AIHORDE_KEY"
name = "aihorde"
wire_api = "chat"

[model_providers.cerebras]
base_url = "https://api.cerebras.ai/v1"
env_key = "CEREBRAS_API_KEY"
name = "cerebras"
wire_api = "chat"

[model_providers.chutes]
base_url = "https://llm.chutes.ai/v1"
env_key = "CHUTES_API_KEY"
name = "chutes"
wire_api = "chat"

[model_providers.featherless]
base_url = "https://api.featherless.ai/v1"
env_key = "FEATHERLESS_API_KEY"
name = "featherless"
wire_api = "chat"

[model_providers.gemini]
base_url = "https://generativelanguage.googleapis.com/v1beta/openai"
env_key = "GOOGLE_API_KEY"
name = "gemini"
wire_api = "chat"

[model_providers.groq]
base_url = "https://api.groq.com/openai/v1"
env_key = "GROQ_API_KEY"
name = "groq"
wire_api = "chat"

[model_providers.infermatic]
base_url = "https://api.totalgpt.ai/v1"
env_key = "INFERMATIC_API_KEY"
name = "infermatic"
wire_api = "chat"

[model_providers.llm7]
base_url = "https://api.llm7.io/v1"
env_key = "LLM7_API_KEY"
name = "llm7"
wire_api = "chat"

[model_providers.lmstudio]
base_url = "http://othello.local:1234/v1"
env_key = "LMSTUDIO_API_KEY"
name = "lmstudio"
wire_api = "chat"

[model_providers.mancer]
base_url = "https://neuro.mancer.tech/oai/v1"
env_key = "MANCER_API_KEY"
name = "mancer"
wire_api = "chat"

[model_providers.openai]
base_url = "https://api.openai.com/v1"
env_key = "OPENAI_API_KEY"
name = "openai"
wire_api = "responses"

[model_providers.openai-chat-completions]
base_url = "https://api.openai.com/v1"
env_key = "OPENAI_API_KEY"
name = "openai-legacy"
wire_api = "chat"

[model_providers.openrouter]
base_url = "https://openrouter.ai/api/v1"
env_key = "OPENROUTER_API_KEY"
name = "openrouter"
wire_api = "chat"

[model_providers.poe]
base_url = "https://api.poe.com/v1"
env_key = "POE_API_KEY"
name = "poe"
wire_api = "chat"

[model_providers.pollinations]
base_url = "https://text.pollinations.ai/openai"
env_key = "POLLINATIONS_API_KEY"
name = "pollinations"
wire_api = "chat"

# profiles

[profiles]
# profiles.aih

[profiles.or_sonoma_dusk_alpha]
approval_policy = "never"
model = "openrouter/sonoma-dusk-alpha"
model_context_window = 2000000
model_max_output_tokens = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.or_sonoma_sky_alpha]
approval_policy = "never"
model = "openrouter/sonoma-sky-alpha"
model_context_window = 2000000
model_max_output_tokens = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.cerebras_code]
approval_policy = "never"
model = "qwen-3-coder-480b"
model_provider = "cerebras"
model_reasoning_effort = "high"

[profiles.aih_deepseek_ai_deepseek]
approval_policy = "never"
model = "deepseek-ai/DeepSeek-V3"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_anubis_70b]
approval_policy = "never"
model = "koboldcpp/Anubis-70B-v1.1"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_broken_tut]
approval_policy = "never"
model = "koboldcpp/Broken-Tutu-24B-Unslop-v2.0"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_darkidol_l]
approval_policy = "never"
model = "koboldcpp/DarkIdol-Llama-3.1-8B-Instruct-1.2-Uncensored.Q8_0"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_fimbulvetr]
approval_policy = "never"
model = "koboldcpp/Fimbulvetr-11B-v2"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_gaslit_tra]
approval_policy = "never"
model = "koboldcpp/Gaslit-Transgression-24B-v1.0.Q4_K_S"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_gemmasutra]
approval_policy = "never"
model = "koboldcpp/Gemmasutra-Mini-2B-v1"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_impish_mag]
approval_policy = "never"
model = "koboldcpp/Impish_Magic_24B"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_impish_nem]
approval_policy = "never"
model = "koboldcpp/Impish_Nemo_12B"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_judas_the_]
approval_policy = "never"
model = "koboldcpp/Judas-The-Uncensored-3.2-1B-Q8_0-GGUF"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_kobbletiny]
approval_policy = "never"
model = "koboldcpp/KobbleTiny-1.1B"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_l3_8b_sthe]
approval_policy = "never"
model = "koboldcpp/L3-8B-Stheno-v3.2.i1-Q5_K_M"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_l3_super_n]
approval_policy = "never"
model = "koboldcpp/L3-Super-Nova-RP-8B"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_llama2_13b]
approval_policy = "never"
model = "koboldcpp/LLaMA2-13B-Tiefighter"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_llama_3_lu]
approval_policy = "never"
model = "koboldcpp/Llama-3-Lumimaid-8B-v0.1"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_mawdistica]
approval_policy = "never"
model = "koboldcpp/Mawdistical-Squelching-Fantasies-8B"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_nemomixunl]
approval_policy = "never"
model = "koboldcpp/NemoMix Unleashed-12B_Q6_K.gguf"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_omega_dark]
approval_policy = "never"
model = "koboldcpp/Omega-Darker-Gaslight_The-Final-Forgoten-Fever-Dream"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_qwen3_0_6b]
approval_policy = "never"
model = "koboldcpp/Qwen3-0.6B.GGUF"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_qwen_qwen3]
approval_policy = "never"
model = "koboldcpp/Qwen_Qwen3-1.7B-Q4_K_M"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_steelskull]
approval_policy = "never"
model = "koboldcpp/Steelskull_L3.3-Electra-R1-70b-Q4_K_M"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_wizzgptv7]
approval_policy = "never"
model = "koboldcpp/WizzGPTv7"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_digo_prayu]
approval_policy = "never"
model = "koboldcpp/digo-prayudha/unsloth-llama-3.2-1b-gguf"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_gemma_3_27]
approval_policy = "never"
model = "koboldcpp/gemma-3-270m-it-Q8_0"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_mini_magnu]
approval_policy = "never"
model = "koboldcpp/mini-magnum-12b-v1.1"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_koboldcpp_mradermach]
approval_policy = "never"
model = "koboldcpp/mradermacher/pythia-70m-deduped.f16.gguf"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.aih_tabbyapi_behemoth_x_]
approval_policy = "never"
model = "tabbyAPI/Behemoth-X-123B-v2-exl2_5.0bpw"
model_provider = "aihorde"
model_reasoning_effort = "high"

[profiles.fast]
approval_policy = "never"
model = "codex-mini-latest"
model_provider = "openai"
model_reasoning_effort = "low"

[profiles.gpt41]
approval_policy = "never"
model = "gpt-4.1"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.gpt41m]
approval_policy = "never"
model = "gpt-4.1-mini"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.gpt41n]
approval_policy = "never"
model = "gpt-4.1-nano"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.gpt41nn]
approval_policy = "never"
model = "openai/gpt-4.1-nano:nitro"
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.gpt4o]
approval_policy = "never"
model = "gpt-4o"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.gpt5]
approval_policy = "never"
model = "gpt-5"
model_provider = "openai"
model_reasoning_effort = "low"

[profiles.gpt5m]
approval_policy = "never"
model = "gpt-5-mini"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.gpt5n]
approval_policy = "never"
model = "gpt-5-nano"
model_provider = "openai"
model_reasoning_effort = "low"

[profiles.mini]
approval_policy = "never"
model = "codex-mini-latest"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.o1p]
approval_policy = "never"
model = "o1-pro"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.o3]
approval_policy = "never"
model = "o3"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.o3deep]
approval_policy = "never"
model = "o3-deep-research"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.o3p]
approval_policy = "never"
model = "o3-pro"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.o4m]
approval_policy = "never"
model = "o4-mini"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.o4mdeep]
approval_policy = "never"
model = "o4-mini-deep-research"
model_provider = "openai"
model_reasoning_effort = "high"

[profiles.ope_agentica_org_deepcod]
approval_policy = "never"
model = "agentica-org/deepcoder-14b-preview:free"
model_context_window = 96000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_ai21_jamba_large_1_7]
approval_policy = "never"
model = "ai21/jamba-large-1.7"
model_context_window = 256000
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_ai21_jamba_mini_1_7]
approval_policy = "never"
model = "ai21/jamba-mini-1.7"
model_context_window = 256000
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_aion_labs_aion_1_0]
approval_policy = "never"
model = "aion-labs/aion-1.0"
model_context_window = 131072
model_max_output_tokens = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_aion_labs_aion_1_0_m]
approval_policy = "never"
model = "aion-labs/aion-1.0-mini"
model_context_window = 131072
model_max_output_tokens = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_aion_labs_aion_rp_ll]
approval_policy = "never"
model = "aion-labs/aion-rp-llama-3.1-8b"
model_context_window = 32768
model_max_output_tokens = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_alfredpros_codellama]
approval_policy = "never"
model = "alfredpros/codellama-7b-instruct-solidity"
model_context_window = 8192
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_alpindale_goliath_12]
approval_policy = "never"
model = "alpindale/goliath-120b"
model_context_window = 6144
model_max_output_tokens = 512
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_amazon_nova_lite_v1]
approval_policy = "never"
model = "amazon/nova-lite-v1"
model_context_window = 300000
model_max_output_tokens = 5120
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_amazon_nova_micro_v1]
approval_policy = "never"
model = "amazon/nova-micro-v1"
model_context_window = 128000
model_max_output_tokens = 5120
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_amazon_nova_pro_v1]
approval_policy = "never"
model = "amazon/nova-pro-v1"
model_context_window = 300000
model_max_output_tokens = 5120
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_anthracite_org_magnu]
approval_policy = "never"
model = "anthracite-org/magnum-v4-72b"
model_context_window = 16384
model_max_output_tokens = 1024
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_anthropic_claude_3_5]
approval_policy = "never"
model = "anthropic/claude-3.5-sonnet:beta"
model_context_window = 200000
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_anthropic_claude_3_7]
approval_policy = "never"
model = "anthropic/claude-3.7-sonnet:thinking"
model_context_window = 200000
model_max_output_tokens = 64000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_anthropic_claude_3_h]
approval_policy = "never"
model = "anthropic/claude-3-haiku:beta"
model_context_window = 200000
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_anthropic_claude_3_o]
approval_policy = "never"
model = "anthropic/claude-3-opus:beta"
model_context_window = 200000
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_anthropic_claude_opu]
approval_policy = "never"
model = "anthropic/claude-opus-4.1"
model_context_window = 200000
model_max_output_tokens = 32000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_anthropic_claude_son]
approval_policy = "never"
model = "anthropic/claude-sonnet-4"
model_context_window = 200000
model_max_output_tokens = 64000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_arcee_ai_coder_large]
approval_policy = "never"
model = "arcee-ai/coder-large"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_arcee_ai_maestro_rea]
approval_policy = "never"
model = "arcee-ai/maestro-reasoning"
model_context_window = 131072
model_max_output_tokens = 32000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_arcee_ai_spotlight]
approval_policy = "never"
model = "arcee-ai/spotlight"
model_context_window = 131072
model_max_output_tokens = 65537
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_arcee_ai_virtuoso_la]
approval_policy = "never"
model = "arcee-ai/virtuoso-large"
model_context_window = 131072
model_max_output_tokens = 64000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_arliai_qwq_32b_arlia]
approval_policy = "never"
model = "arliai/qwq-32b-arliai-rpr-v1:free"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_baidu_ernie_4_5_300b]
approval_policy = "never"
model = "baidu/ernie-4.5-300b-a47b"
model_context_window = 123000
model_max_output_tokens = 12000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_bytedance_ui_tars_1_]
approval_policy = "never"
model = "bytedance/ui-tars-1.5-7b"
model_context_window = 128000
model_max_output_tokens = 2048
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_cognitivecomputation]
approval_policy = "never"
model = "cognitivecomputations/dolphin3.0-r1-mistral-24b:free"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_cohere_command]
approval_policy = "never"
model = "cohere/command"
model_context_window = 4096
model_max_output_tokens = 4000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_cohere_command_a]
approval_policy = "never"
model = "cohere/command-a"
model_context_window = 256000
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_cohere_command_r]
approval_policy = "never"
model = "cohere/command-r"
model_context_window = 128000
model_max_output_tokens = 4000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_cohere_command_r7b_1]
approval_policy = "never"
model = "cohere/command-r7b-12-2024"
model_context_window = 128000
model_max_output_tokens = 4000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_cohere_command_r_03_]
approval_policy = "never"
model = "cohere/command-r-03-2024"
model_context_window = 128000
model_max_output_tokens = 4000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_cohere_command_r_08_]
approval_policy = "never"
model = "cohere/command-r-08-2024"
model_context_window = 128000
model_max_output_tokens = 4000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_cohere_command_r_plu]
approval_policy = "never"
model = "cohere/command-r-plus-08-2024"
model_context_window = 128000
model_max_output_tokens = 4000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_deepseek_deepseek_ch]
approval_policy = "never"
model = "deepseek/deepseek-chat-v3-0324:free"
model_context_window = 163840
model_max_output_tokens = 163840
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_deepseek_deepseek_pr]
approval_policy = "never"
model = "deepseek/deepseek-prover-v2"
model_context_window = 163840
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_deepseek_deepseek_r1]
approval_policy = "never"
model = "deepseek/deepseek-r1:free"
model_context_window = 163840
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_deepseek_deepseek_v3]
approval_policy = "never"
model = "deepseek/deepseek-v3-base"
model_context_window = 163840
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_eleutherai_llemma_7b]
approval_policy = "never"
model = "eleutherai/llemma_7b"
model_context_window = 4096
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_featherless_qwerky_7]
approval_policy = "never"
model = "featherless/qwerky-72b:free"
model_context_window = 32768
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_google_gemini_2_0_fl]
approval_policy = "never"
model = "google/gemini-2.0-flash-lite-001"
model_context_window = 1048576
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_google_gemini_2_5_fl]
approval_policy = "never"
model = "google/gemini-2.5-flash-lite-preview-06-17"
model_context_window = 1048576
model_max_output_tokens = 65535
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_google_gemini_2_5_pr]
approval_policy = "never"
model = "google/gemini-2.5-pro-preview-05-06"
model_context_window = 1048576
model_max_output_tokens = 65535
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_google_gemini_flash_]
approval_policy = "never"
model = "google/gemini-flash-1.5-8b"
model_context_window = 1000000
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_google_gemini_pro_1_]
approval_policy = "never"
model = "google/gemini-pro-1.5"
model_context_window = 2000000
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_google_gemma_2_27b_i]
approval_policy = "never"
model = "google/gemma-2-27b-it"
model_context_window = 8192
model_max_output_tokens = 2048
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_google_gemma_2_9b_it]
approval_policy = "never"
model = "google/gemma-2-9b-it:free"
model_context_window = 8192
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_google_gemma_3_12b_i]
approval_policy = "never"
model = "google/gemma-3-12b-it:free"
model_context_window = 96000
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_google_gemma_3_27b_i]
approval_policy = "never"
model = "google/gemma-3-27b-it:free"
model_context_window = 96000
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_google_gemma_3_4b_it]
approval_policy = "never"
model = "google/gemma-3-4b-it:free"
model_context_window = 32768
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_google_gemma_3n_e2b_]
approval_policy = "never"
model = "google/gemma-3n-e2b-it:free"
model_context_window = 8192
model_max_output_tokens = 2048
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_google_gemma_3n_e4b_]
approval_policy = "never"
model = "google/gemma-3n-e4b-it:free"
model_context_window = 8192
model_max_output_tokens = 2048
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_gryphe_mythomax_l2_1]
approval_policy = "never"
model = "gryphe/mythomax-l2-13b"
model_context_window = 4096
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_inception_mercury]
approval_policy = "never"
model = "inception/mercury"
model_context_window = 128000
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_inception_mercury_co]
approval_policy = "never"
model = "inception/mercury-coder"
model_context_window = 128000
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_infermatic_mn_infero]
approval_policy = "never"
model = "infermatic/mn-inferor-12b"
model_context_window = 8192
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_inflection_inflectio]
approval_policy = "never"
model = "inflection/inflection-3-productivity"
model_context_window = 8000
model_max_output_tokens = 1024
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_liquid_lfm_3b]
approval_policy = "never"
model = "liquid/lfm-3b"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_liquid_lfm_40b]
approval_policy = "never"
model = "liquid/lfm-40b"
model_context_window = 65536
model_max_output_tokens = 65536
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_liquid_lfm_7b]
approval_policy = "never"
model = "liquid/lfm-7b"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mancer_weaver]
approval_policy = "never"
model = "mancer/weaver"
model_context_window = 8000
model_max_output_tokens = 1000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_meta_llama_llama_3_1]
approval_policy = "never"
model = "meta-llama/llama-3.1-8b-instruct"
model_context_window = 131072
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_meta_llama_llama_3_2]
approval_policy = "never"
model = "meta-llama/llama-3.2-90b-vision-instruct"
model_context_window = 131072
model_max_output_tokens = 2048
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_meta_llama_llama_3_3]
approval_policy = "never"
model = "meta-llama/llama-3.3-70b-instruct:free"
model_context_window = 65536
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_meta_llama_llama_3_7]
approval_policy = "never"
model = "meta-llama/llama-3-70b-instruct"
model_context_window = 8192
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_meta_llama_llama_3_8]
approval_policy = "never"
model = "meta-llama/llama-3-8b-instruct"
model_context_window = 8192
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_meta_llama_llama_4_m]
approval_policy = "never"
model = "meta-llama/llama-4-maverick"
model_context_window = 1048576
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_meta_llama_llama_4_s]
approval_policy = "never"
model = "meta-llama/llama-4-scout"
model_context_window = 1048576
model_max_output_tokens = 1048576
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_meta_llama_llama_gua]
approval_policy = "never"
model = "meta-llama/llama-guard-4-12b"
model_context_window = 163840
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_microsoft_mai_ds_r1]
approval_policy = "never"
model = "microsoft/mai-ds-r1"
model_context_window = 163840
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_microsoft_mai_ds_r1f]
approval_policy = "never"
model = "microsoft/mai-ds-r1:free"
model_context_window = 163840
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_microsoft_phi_3_5_mi]
approval_policy = "never"
model = "microsoft/phi-3.5-mini-128k-instruct"
model_context_window = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_microsoft_phi_3_medi]
approval_policy = "never"
model = "microsoft/phi-3-medium-128k-instruct"
model_context_window = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_microsoft_phi_3_mini]
approval_policy = "never"
model = "microsoft/phi-3-mini-128k-instruct"
model_context_window = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_microsoft_phi_4]
approval_policy = "never"
model = "microsoft/phi-4"
model_context_window = 16384
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_microsoft_phi_4_mult]
approval_policy = "never"
model = "microsoft/phi-4-multimodal-instruct"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_microsoft_phi_4_reas]
approval_policy = "never"
model = "microsoft/phi-4-reasoning-plus"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_microsoft_wizardlm_2]
approval_policy = "never"
model = "microsoft/wizardlm-2-8x22b"
model_context_window = 65536
model_max_output_tokens = 65536
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_minimax_minimax_01]
approval_policy = "never"
model = "minimax/minimax-01"
model_context_window = 1000192
model_max_output_tokens = 1000192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_minimax_minimax_m1]
approval_policy = "never"
model = "minimax/minimax-m1"
model_context_window = 1000000
model_max_output_tokens = 40000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_codestral_]
approval_policy = "never"
model = "mistralai/codestral-2508"
model_context_window = 256000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_devstral_m]
approval_policy = "never"
model = "mistralai/devstral-medium"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_devstral_s]
approval_policy = "never"
model = "mistralai/devstral-small-2505:free"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_magistral_]
approval_policy = "never"
model = "mistralai/magistral-small-2506"
model_context_window = 40000
model_max_output_tokens = 40000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_ministral_]
approval_policy = "never"
model = "mistralai/ministral-8b"
model_context_window = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_mistral_7b]
approval_policy = "never"
model = "mistralai/mistral-7b-instruct:free"
model_context_window = 32768
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_mistral_la]
approval_policy = "never"
model = "mistralai/mistral-large-2411"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_mistral_me]
approval_policy = "never"
model = "mistralai/mistral-medium-3"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_mistral_ne]
approval_policy = "never"
model = "mistralai/mistral-nemo:free"
model_context_window = 131072
model_max_output_tokens = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_mistral_sa]
approval_policy = "never"
model = "mistralai/mistral-saba"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_mistral_sm]
approval_policy = "never"
model = "mistralai/mistral-small-3.2-24b-instruct:free"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_mistral_ti]
approval_policy = "never"
model = "mistralai/mistral-tiny"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_mixtral_8x]
approval_policy = "never"
model = "mistralai/mixtral-8x7b-instruct"
model_context_window = 32768
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_pixtral_12]
approval_policy = "never"
model = "mistralai/pixtral-12b"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_mistralai_pixtral_la]
approval_policy = "never"
model = "mistralai/pixtral-large-2411"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_moonshotai_kimi_dev_]
approval_policy = "never"
model = "moonshotai/kimi-dev-72b:free"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_moonshotai_kimi_k2]
approval_policy = "never"
model = "moonshotai/kimi-k2"
model_context_window = 63000
model_max_output_tokens = 63000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_moonshotai_kimi_k2fr]
approval_policy = "never"
model = "moonshotai/kimi-k2:free"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_moonshotai_kimi_vl_a]
approval_policy = "never"
model = "moonshotai/kimi-vl-a3b-thinking:free"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_morph_morph_v3_fast]
approval_policy = "never"
model = "morph/morph-v3-fast"
model_context_window = 81920
model_max_output_tokens = 38000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_morph_morph_v3_large]
approval_policy = "never"
model = "morph/morph-v3-large"
model_context_window = 81920
model_max_output_tokens = 38000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_neversleep_llama_3_1]
approval_policy = "never"
model = "neversleep/llama-3.1-lumimaid-8b"
model_context_window = 40000
model_max_output_tokens = 40000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_neversleep_llama_3_l]
approval_policy = "never"
model = "neversleep/llama-3-lumimaid-70b"
model_context_window = 8192
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_neversleep_noromaid_]
approval_policy = "never"
model = "neversleep/noromaid-20b"
model_context_window = 8192
model_max_output_tokens = 2048
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_nousresearch_deepher]
approval_policy = "never"
model = "nousresearch/deephermes-3-mistral-24b-preview"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_nousresearch_hermes_]
approval_policy = "never"
model = "nousresearch/hermes-3-llama-3.1-70b"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_nousresearch_nous_he]
approval_policy = "never"
model = "nousresearch/nous-hermes-2-mixtral-8x7b-dpo"
model_context_window = 32768
model_max_output_tokens = 2048
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_nvidia_llama_3_1_nem]
approval_policy = "never"
model = "nvidia/llama-3.1-nemotron-ultra-253b-v1:free"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_nvidia_llama_3_3_nem]
approval_policy = "never"
model = "nvidia/llama-3.3-nemotron-super-49b-v1"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_chatgpt_4o_la]
approval_policy = "never"
model = "openai/chatgpt-4o-latest"
model_context_window = 128000
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_codex_mini]
approval_policy = "never"
model = "openai/codex-mini"
model_context_window = 200000
model_max_output_tokens = 100000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_3_5_turbo]
approval_policy = "never"
model = "openai/gpt-3.5-turbo-instruct"
model_context_window = 4095
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4]
approval_policy = "never"
model = "openai/gpt-4"
model_context_window = 8191
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4_0314]
approval_policy = "never"
model = "openai/gpt-4-0314"
model_context_window = 8191
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4_1]
approval_policy = "never"
model = "openai/gpt-4.1"
model_context_window = 1047576
model_max_output_tokens = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4_1106_pr]
approval_policy = "never"
model = "openai/gpt-4-1106-preview"
model_context_window = 128000
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4_1_mini]
approval_policy = "never"
model = "openai/gpt-4.1-mini"
model_context_window = 1047576
model_max_output_tokens = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4_1_nano]
approval_policy = "never"
model = "openai/gpt-4.1-nano"
model_context_window = 1047576
model_max_output_tokens = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4_turbo]
approval_policy = "never"
model = "openai/gpt-4-turbo"
model_context_window = 128000
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4_turbo_p]
approval_policy = "never"
model = "openai/gpt-4-turbo-preview"
model_context_window = 128000
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4o]
approval_policy = "never"
model = "openai/gpt-4o"
model_context_window = 128000
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4o_2024_0]
approval_policy = "never"
model = "openai/gpt-4o-2024-08-06"
model_context_window = 128000
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4o_2024_1]
approval_policy = "never"
model = "openai/gpt-4o-2024-11-20"
model_context_window = 128000
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4o_mini]
approval_policy = "never"
model = "openai/gpt-4o-mini"
model_context_window = 128000
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4o_mini_2]
approval_policy = "never"
model = "openai/gpt-4o-mini-2024-07-18"
model_context_window = 128000
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4o_mini_s]
approval_policy = "never"
model = "openai/gpt-4o-mini-search-preview"
model_context_window = 128000
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4o_search]
approval_policy = "never"
model = "openai/gpt-4o-search-preview"
model_context_window = 128000
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_4oextende]
approval_policy = "never"
model = "openai/gpt-4o:extended"
model_context_window = 128000
model_max_output_tokens = 64000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_5]
approval_policy = "never"
model = "openai/gpt-5"
model_context_window = 400000
model_max_output_tokens = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_5_chat]
approval_policy = "never"
model = "openai/gpt-5-chat"
model_context_window = 400000
model_max_output_tokens = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_5_mini]
approval_policy = "never"
model = "openai/gpt-5-mini"
model_context_window = 400000
model_max_output_tokens = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_5_nano]
approval_policy = "never"
model = "openai/gpt-5-nano"
model_context_window = 400000
model_max_output_tokens = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_oss_120b]
approval_policy = "never"
model = "openai/gpt-oss-120b"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_oss_20b]
approval_policy = "never"
model = "openai/gpt-oss-20b"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_gpt_oss_20bfr]
approval_policy = "never"
model = "openai/gpt-oss-20b:free"
model_context_window = 131072
model_max_output_tokens = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_o1]
approval_policy = "never"
model = "openai/o1"
model_context_window = 200000
model_max_output_tokens = 100000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_o1_mini]
approval_policy = "never"
model = "openai/o1-mini"
model_context_window = 128000
model_max_output_tokens = 65536
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_o1_mini_2024_]
approval_policy = "never"
model = "openai/o1-mini-2024-09-12"
model_context_window = 128000
model_max_output_tokens = 65536
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_o1_pro]
approval_policy = "never"
model = "openai/o1-pro"
model_context_window = 200000
model_max_output_tokens = 100000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_o3]
approval_policy = "never"
model = "openai/o3"
model_context_window = 200000
model_max_output_tokens = 100000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_o3_mini]
approval_policy = "never"
model = "openai/o3-mini"
model_context_window = 200000
model_max_output_tokens = 100000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_o3_mini_high]
approval_policy = "never"
model = "openai/o3-mini-high"
model_context_window = 200000
model_max_output_tokens = 100000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_o3_pro]
approval_policy = "never"
model = "openai/o3-pro"
model_context_window = 200000
model_max_output_tokens = 100000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_o4_mini]
approval_policy = "never"
model = "openai/o4-mini"
model_context_window = 200000
model_max_output_tokens = 100000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openai_o4_mini_high]
approval_policy = "never"
model = "openai/o4-mini-high"
model_context_window = 200000
model_max_output_tokens = 100000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_opengvlab_internvl3_]
approval_policy = "never"
model = "opengvlab/internvl3-14b"
model_context_window = 12288
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_openrouter_auto]
approval_policy = "never"
model = "openrouter/auto"
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_perplexity_r1_1776]
approval_policy = "never"
model = "perplexity/r1-1776"
model_context_window = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_perplexity_sonar]
approval_policy = "never"
model = "perplexity/sonar"
model_context_window = 127072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_perplexity_sonar_dee]
approval_policy = "never"
model = "perplexity/sonar-deep-research"
model_context_window = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_perplexity_sonar_pro]
approval_policy = "never"
model = "perplexity/sonar-pro"
model_context_window = 200000
model_max_output_tokens = 8000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_perplexity_sonar_rea]
approval_policy = "never"
model = "perplexity/sonar-reasoning-pro"
model_context_window = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_pygmalionai_mythalio]
approval_policy = "never"
model = "pygmalionai/mythalion-13b"
model_context_window = 4096
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen2_5_vl_32b_]
approval_policy = "never"
model = "qwen/qwen2.5-vl-32b-instruct:free"
model_context_window = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen2_5_vl_72b_]
approval_policy = "never"
model = "qwen/qwen2.5-vl-72b-instruct:free"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen3_14b]
approval_policy = "never"
model = "qwen/qwen3-14b"
model_context_window = 40960
model_max_output_tokens = 40960
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen3_14bfree]
approval_policy = "never"
model = "qwen/qwen3-14b:free"
model_context_window = 40960
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen3_235b_a22b]
approval_policy = "never"
model = "qwen/qwen3-235b-a22b:free"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen3_30b_a3b]
approval_policy = "never"
model = "qwen/qwen3-30b-a3b"
model_context_window = 40960
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen3_30b_a3b_i]
approval_policy = "never"
model = "qwen/qwen3-30b-a3b-instruct-2507"
model_context_window = 131072
model_max_output_tokens = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen3_30b_a3bfr]
approval_policy = "never"
model = "qwen/qwen3-30b-a3b:free"
model_context_window = 40960
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen3_32b]
approval_policy = "never"
model = "qwen/qwen3-32b"
model_context_window = 40960
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen3_4bfree]
approval_policy = "never"
model = "qwen/qwen3-4b:free"
model_context_window = 40960
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen3_8b]
approval_policy = "never"
model = "qwen/qwen3-8b"
model_context_window = 128000
model_max_output_tokens = 20000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen3_8bfree]
approval_policy = "never"
model = "qwen/qwen3-8b:free"
model_context_window = 40960
model_max_output_tokens = 40960
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen3_coder]
approval_policy = "never"
model = "qwen/qwen3-coder:nitro"
model_context_window = 262144
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen3_coderfree]
approval_policy = "never"
model = "qwen/qwen3-coder:free"
model_context_window = 262144
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen_2_5_72b_in]
approval_policy = "never"
model = "qwen/qwen-2.5-72b-instruct:free"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen_2_5_7b_ins]
approval_policy = "never"
model = "qwen/qwen-2.5-7b-instruct"
model_context_window = 32768
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen_2_5_coder_]
approval_policy = "never"
model = "qwen/qwen-2.5-coder-32b-instruct:free"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen_2_5_vl_7b_]
approval_policy = "never"
model = "qwen/qwen-2.5-vl-7b-instruct"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen_2_72b_inst]
approval_policy = "never"
model = "qwen/qwen-2-72b-instruct"
model_context_window = 32768
model_max_output_tokens = 4096
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen_max]
approval_policy = "never"
model = "qwen/qwen-max"
model_context_window = 32768
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen_plus]
approval_policy = "never"
model = "qwen/qwen-plus"
model_context_window = 131072
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen_turbo]
approval_policy = "never"
model = "qwen/qwen-turbo"
model_context_window = 1000000
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen_vl_max]
approval_policy = "never"
model = "qwen/qwen-vl-max"
model_context_window = 7500
model_max_output_tokens = 1500
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwen_vl_plus]
approval_policy = "never"
model = "qwen/qwen-vl-plus"
model_context_window = 7500
model_max_output_tokens = 1500
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwq_32b]
approval_policy = "never"
model = "qwen/qwq-32b"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwq_32b_preview]
approval_policy = "never"
model = "qwen/qwq-32b-preview"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_qwen_qwq_32bfree]
approval_policy = "never"
model = "qwen/qwq-32b:free"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_raifle_sorcererlm_8x]
approval_policy = "never"
model = "raifle/sorcererlm-8x22b"
model_context_window = 16000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_rekaai_reka_flash_3f]
approval_policy = "never"
model = "rekaai/reka-flash-3:free"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_sao10k_l3_1_euryale_]
approval_policy = "never"
model = "sao10k/l3.1-euryale-70b"
model_context_window = 131072
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_sao10k_l3_3_euryale_]
approval_policy = "never"
model = "sao10k/l3.3-euryale-70b"
model_context_window = 131072
model_max_output_tokens = 16384
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_sao10k_l3_euryale_70]
approval_policy = "never"
model = "sao10k/l3-euryale-70b"
model_context_window = 8192
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_sao10k_l3_lunaris_8b]
approval_policy = "never"
model = "sao10k/l3-lunaris-8b"
model_context_window = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_sarvamai_sarvam_mfre]
approval_policy = "never"
model = "sarvamai/sarvam-m:free"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_scb10x_llama3_1_typh]
approval_policy = "never"
model = "scb10x/llama3.1-typhoon2-70b-instruct"
model_context_window = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_shisa_ai_shisa_v2_ll]
approval_policy = "never"
model = "shisa-ai/shisa-v2-llama3.3-70b:free"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_sophosympatheia_midn]
approval_policy = "never"
model = "sophosympatheia/midnight-rose-70b"
model_context_window = 4096
model_max_output_tokens = 2048
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_switchpoint_router]
approval_policy = "never"
model = "switchpoint/router"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_tencent_hunyuan_a13b]
approval_policy = "never"
model = "tencent/hunyuan-a13b-instruct:free"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_thedrummer_anubis_70]
approval_policy = "never"
model = "thedrummer/anubis-70b-v1.1"
model_context_window = 131072
model_max_output_tokens = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_thedrummer_anubis_pr]
approval_policy = "never"
model = "thedrummer/anubis-pro-105b-v1"
model_context_window = 131072
model_max_output_tokens = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_thedrummer_rocinante]
approval_policy = "never"
model = "thedrummer/rocinante-12b"
model_context_window = 8192
model_max_output_tokens = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_thedrummer_skyfall_3]
approval_policy = "never"
model = "thedrummer/skyfall-36b-v2"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_thedrummer_unslopnem]
approval_policy = "never"
model = "thedrummer/unslopnemo-12b"
model_context_window = 32000
model_max_output_tokens = 32000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_thedrummer_valkyrie_]
approval_policy = "never"
model = "thedrummer/valkyrie-49b-v1"
model_context_window = 131072
model_max_output_tokens = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_thudm_glm_4_1v_9b_th]
approval_policy = "never"
model = "thudm/glm-4.1v-9b-thinking"
model_context_window = 65536
model_max_output_tokens = 8000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_thudm_glm_4_32b]
approval_policy = "never"
model = "thudm/glm-4-32b"
model_context_window = 32000
model_max_output_tokens = 32000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_thudm_glm_z1_32bfree]
approval_policy = "never"
model = "thudm/glm-z1-32b:free"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_tngtech_deepseek_r1t]
approval_policy = "never"
model = "tngtech/deepseek-r1t2-chimera:free"
model_context_window = 163840
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_undi95_remm_slerp_l2]
approval_policy = "never"
model = "undi95/remm-slerp-l2-13b"
model_context_window = 6144
model_max_output_tokens = 1024
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_x_ai_grok_2_1212]
approval_policy = "never"
model = "x-ai/grok-2-1212"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_x_ai_grok_2_vision_1]
approval_policy = "never"
model = "x-ai/grok-2-vision-1212"
model_context_window = 32768
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_x_ai_grok_3]
approval_policy = "never"
model = "x-ai/grok-3"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_x_ai_grok_3_beta]
approval_policy = "never"
model = "x-ai/grok-3-beta"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_x_ai_grok_3_mini]
approval_policy = "never"
model = "x-ai/grok-3-mini"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_x_ai_grok_3_mini_bet]
approval_policy = "never"
model = "x-ai/grok-3-mini-beta"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_x_ai_grok_4]
approval_policy = "never"
model = "x-ai/grok-4"
model_context_window = 256000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_x_ai_grok_vision_bet]
approval_policy = "never"
model = "x-ai/grok-vision-beta"
model_context_window = 8192
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_z_ai_glm_4_32b]
approval_policy = "never"
model = "z-ai/glm-4-32b"
model_context_window = 128000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_z_ai_glm_4_5]
approval_policy = "never"
model = "z-ai/glm-4.5"
model_context_window = 98304
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_z_ai_glm_4_5_air]
approval_policy = "never"
model = "z-ai/glm-4.5-air"
model_context_window = 131072
model_max_output_tokens = 96000
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.ope_z_ai_glm_4_5_airfree]
approval_policy = "never"
model = "z-ai/glm-4.5-air:free"
model_context_window = 131072
model_provider = "openrouter"
model_reasoning_effort = "high"

[profiles.chu_arliai_qwq_32b_arlia]
approval_policy = "never"
model = "ArliAI/QwQ-32B-ArliAI-RpR-v1"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 32768

[profiles.chu_nousresearch_deepher]
approval_policy = "never"
model = "NousResearch/DeepHermes-3-Mistral-24B-Preview"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 32768

[profiles.chu_qwen_qwen2_5_72b_ins]
approval_policy = "never"
model = "Qwen/Qwen2.5-72B-Instruct"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 32768

[profiles.chu_qwen_qwen2_5_coder_3]
approval_policy = "never"
model = "Qwen/Qwen2.5-Coder-32B-Instruct"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 32768

[profiles.chu_qwen_qwen2_5_vl_32b_]
approval_policy = "never"
model = "Qwen/Qwen2.5-VL-32B-Instruct"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 16384

[profiles.chu_qwen_qwen2_5_vl_72b_]
approval_policy = "never"
model = "Qwen/Qwen2.5-VL-72B-Instruct"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 32768

[profiles.chu_qwen_qwen3_14b]
approval_policy = "never"
model = "Qwen/Qwen3-14B"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 40960

[profiles.chu_qwen_qwen3_235b_a22b]
approval_policy = "never"
model = "Qwen/Qwen3-235B-A22B-Thinking-2507"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 262144

[profiles.chu_qwen_qwen3_30b_a3b]
approval_policy = "never"
model = "Qwen/Qwen3-30B-A3B"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 40960

[profiles.chu_qwen_qwen3_32b]
approval_policy = "never"
model = "Qwen/Qwen3-32B"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 40960

[profiles.chu_qwen_qwen3_8b]
approval_policy = "never"
model = "Qwen/Qwen3-8B"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 40960

[profiles.chu_qwen_qwen3_coder_30b]
approval_policy = "never"
model = "Qwen/Qwen3-Coder-30B-A3B-Instruct"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 262144

[profiles.chu_qwen_qwen3_coder_480]
approval_policy = "never"
model = "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 262144

[profiles.chu_salesforce_xgen_smal]
approval_policy = "never"
model = "Salesforce/xgen-small-9B-instruct-r"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 262144

[profiles.chu_tesslate_uigen_x_32b]
approval_policy = "never"
model = "Tesslate/UIGEN-X-32B-0727"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 40960

[profiles.chu_thedrummer_cydonia_2]
approval_policy = "never"
model = "TheDrummer/Cydonia-24B-v2.1"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 32768

[profiles.chu_thedrummer_gemmasutr]
approval_policy = "never"
model = "TheDrummer/Gemmasutra-Pro-27B-v1.1"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 8192

[profiles.chu_thedrummer_skyfall_3]
approval_policy = "never"
model = "TheDrummer/Skyfall-36B-v2"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 32768

[profiles.chu_thedrummer_tunguska_]
approval_policy = "never"
model = "TheDrummer/Tunguska-39B-v1"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 131072

[profiles.chu_agentica_org_deepcod]
approval_policy = "never"
model = "agentica-org/DeepCoder-14B-Preview"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 96000

[profiles.chu_all_hands_openhands_]
approval_policy = "never"
model = "all-hands/openhands-lm-32b-v0.1-ep3"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 16384

[profiles.chu_chutesai_devstral_sm]
approval_policy = "never"
model = "chutesai/Devstral-Small-2505"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 131072

[profiles.chu_chutesai_llama_4_mav]
approval_policy = "never"
model = "chutesai/Llama-4-Maverick-17B-128E-Instruct-FP8"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 128000

[profiles.chu_chutesai_mistral_sma]
approval_policy = "never"
model = "chutesai/Mistral-Small-3.2-24B-Instruct-2506"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 131072

[profiles.chu_cognitivecomputation]
approval_policy = "never"
model = "cognitivecomputations/Dolphin3.0-R1-Mistral-24B"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 32768

[profiles.chu_deepseek_ai_deepseek]
approval_policy = "never"
model = "deepseek-ai/DeepSeek-V3-Base"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 163840

[profiles.chu_internlm_intern_s1]
approval_policy = "never"
model = "internlm/Intern-S1"
model_provider = "chutes"
model_reasoning_effort = "high"

[profiles.chu_microsoft_mai_ds_r1_]
approval_policy = "never"
model = "microsoft/MAI-DS-R1-FP8"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 163840

[profiles.chu_moonshotai_kimi_dev_]
approval_policy = "never"
model = "moonshotai/Kimi-Dev-72B"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 131072

[profiles.chu_moonshotai_kimi_k2_i]
approval_policy = "never"
model = "moonshotai/Kimi-K2-Instruct"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 75000

[profiles.chu_moonshotai_kimi_vl_a]
approval_policy = "never"
model = "moonshotai/Kimi-VL-A3B-Thinking"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 131072

[profiles.chu_nvidia_llama_3_3_nem]
approval_policy = "never"
model = "nvidia/Llama-3_3-Nemotron-Super-49B-v1_5"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 131072

[profiles.chu_openai_gpt_oss_120b]
approval_policy = "never"
model = "openai/gpt-oss-120b"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 131072

[profiles.chu_shisa_ai_shisa_v2_ll]
approval_policy = "never"
model = "shisa-ai/shisa-v2-llama3.3-70b"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 32768

[profiles.chu_stepfun_ai_step3]
approval_policy = "never"
model = "stepfun-ai/step3"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 65536

[profiles.chu_tencent_hunyuan_a13b]
approval_policy = "never"
model = "tencent/Hunyuan-A13B-Instruct"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 32768

[profiles.chu_tngtech_deepseek_r1t]
approval_policy = "never"
model = "tngtech/DeepSeek-R1T-Chimera"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 163840

[profiles.chu_tngtech_deepseek_tng]
approval_policy = "never"
model = "tngtech/DeepSeek-TNG-R1T2-Chimera"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 163840

[profiles.chu_tplr_templar_i]
approval_policy = "never"
model = "tplr/TEMPLAR-I"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 2048

[profiles.chu_unsloth_llama_3_2_1b]
approval_policy = "never"
model = "unsloth/Llama-3.2-1B-Instruct"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 16384

[profiles.chu_unsloth_llama_3_2_3b]
approval_policy = "never"
model = "unsloth/Llama-3.2-3B-Instruct"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 16384

[profiles.chu_unsloth_mistral_nemo]
approval_policy = "never"
model = "unsloth/Mistral-Nemo-Instruct-2407"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 131072

[profiles.chu_unsloth_mistral_smal]
approval_policy = "never"
model = "unsloth/Mistral-Small-24B-Instruct-2501"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 32768

[profiles.chu_unsloth_gemma_2_9b_i]
approval_policy = "never"
model = "unsloth/gemma-2-9b-it"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 8192

[profiles.chu_unsloth_gemma_3_12b_]
approval_policy = "never"
model = "unsloth/gemma-3-12b-it"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 96000

[profiles.chu_unsloth_gemma_3_27b_]
approval_policy = "never"
model = "unsloth/gemma-3-27b-it"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 96000

[profiles.chu_unsloth_gemma_3_4b_i]
approval_policy = "never"
model = "unsloth/gemma-3-4b-it"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 96000

[profiles.chu_zai_org_glm_4_32b_04]
approval_policy = "never"
model = "zai-org/GLM-4-32B-0414"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 32768

[profiles.chu_zai_org_glm_4_5_air]
approval_policy = "never"
model = "zai-org/GLM-4.5-Air"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 131072

[profiles.chu_zai_org_glm_4_5_fp8]
approval_policy = "never"
model = "zai-org/GLM-4.5-FP8"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 98304

[profiles.chu_zai_org_glm_4_5v_fp8]
approval_policy = "never"
model = "zai-org/GLM-4.5V-FP8"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 65536

[profiles.chu_zai_org_glm_z1_32b_0]
approval_policy = "never"
model = "zai-org/GLM-Z1-32B-0414"
model_provider = "chutes"
model_reasoning_effort = "high"
model_context_window = 32768

[profiles.cer_gpt_oss_120b]
approval_policy = "never"
model = "gpt-oss-120b"
model_provider = "cerebras"
model_reasoning_effort = "high"

[profiles.cer_llama_3_3_70b]
approval_policy = "never"
model = "llama-3.3-70b"
model_provider = "cerebras"
model_reasoning_effort = "high"

[profiles.cer_llama_4_maverick_17b]
approval_policy = "never"
model = "llama-4-maverick-17b-128e-instruct"
model_provider = "cerebras"
model_reasoning_effort = "high"

[profiles.cer_llama_4_scout_17b_16]
approval_policy = "never"
model = "llama-4-scout-17b-16e-instruct"
model_provider = "cerebras"
model_reasoning_effort = "high"

[profiles.cer_llama3_1_8b]
approval_policy = "never"
model = "llama3.1-8b"
model_provider = "cerebras"
model_reasoning_effort = "high"

[profiles.cer_qwen_3_235b_a22b_ins]
approval_policy = "never"
model = "qwen-3-235b-a22b-instruct-2507"
model_provider = "cerebras"
model_reasoning_effort = "high"

[profiles.cer_qwen_3_235b_a22b_thi]
approval_policy = "never"
model = "qwen-3-235b-a22b-thinking-2507"
model_provider = "cerebras"
model_reasoning_effort = "high"

[profiles.cer_qwen_3_32b]
approval_policy = "never"
model = "qwen-3-32b"
model_provider = "cerebras"
model_reasoning_effort = "high"

[profiles.cer_qwen_3_coder_480b]
approval_policy = "never"
model = "qwen-3-coder-480b"
model_provider = "cerebras"
model_reasoning_effort = "high"

[profiles.gem_models_aqa]
approval_policy = "never"
model = "models/aqa"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_embedding_001]
approval_policy = "never"
model = "models/embedding-001"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_embedding_gec]
approval_policy = "never"
model = "models/embedding-gecko-001"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemini_1_5_fl]
approval_policy = "never"
model = "models/gemini-1.5-flash-latest"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemini_1_5_pr]
approval_policy = "never"
model = "models/gemini-1.5-pro-latest"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemini_2_0_fl]
approval_policy = "never"
model = "models/gemini-2.0-flash-thinking-exp-1219"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemini_2_0_pr]
approval_policy = "never"
model = "models/gemini-2.0-pro-exp-02-05"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemini_2_5_fl]
approval_policy = "never"
model = "models/gemini-2.5-flash-preview-tts"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemini_2_5_pr]
approval_policy = "never"
model = "models/gemini-2.5-pro-preview-tts"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemini_embedd]
approval_policy = "never"
model = "models/gemini-embedding-exp-03-07"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemini_exp_12]
approval_policy = "never"
model = "models/gemini-exp-1206"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemini_live_2]
approval_policy = "never"
model = "models/gemini-live-2.5-flash-preview"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemma_3_12b_i]
approval_policy = "never"
model = "models/gemma-3-12b-it"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemma_3_1b_it]
approval_policy = "never"
model = "models/gemma-3-1b-it"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemma_3_27b_i]
approval_policy = "never"
model = "models/gemma-3-27b-it"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemma_3_4b_it]
approval_policy = "never"
model = "models/gemma-3-4b-it"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemma_3n_e2b_]
approval_policy = "never"
model = "models/gemma-3n-e2b-it"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_gemma_3n_e4b_]
approval_policy = "never"
model = "models/gemma-3n-e4b-it"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_imagen_3_0_ge]
approval_policy = "never"
model = "models/imagen-3.0-generate-002"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_imagen_4_0_fa]
approval_policy = "never"
model = "models/imagen-4.0-fast-generate-001"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_imagen_4_0_ge]
approval_policy = "never"
model = "models/imagen-4.0-generate-preview-06-06"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_imagen_4_0_ul]
approval_policy = "never"
model = "models/imagen-4.0-ultra-generate-preview-06-06"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_learnlm_2_0_f]
approval_policy = "never"
model = "models/learnlm-2.0-flash-experimental"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_text_embeddin]
approval_policy = "never"
model = "models/text-embedding-004"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_veo_2_0_gener]
approval_policy = "never"
model = "models/veo-2.0-generate-001"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_veo_3_0_fast_]
approval_policy = "never"
model = "models/veo-3.0-fast-generate-preview"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.gem_models_veo_3_0_gener]
approval_policy = "never"
model = "models/veo-3.0-generate-preview"
model_provider = "gemini"
model_reasoning_effort = "high"

[profiles.llm_bidara]
approval_policy = "never"
model = "bidara"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_codestral_2405]
approval_policy = "never"
model = "codestral-2405"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_codestral_2501]
approval_policy = "never"
model = "codestral-2501"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_elixposearch]
approval_policy = "never"
model = "elixposearch"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_gpt_4_1_nano_2025_04]
approval_policy = "never"
model = "gpt-4.1-nano-2025-04-14"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_gpt_4o_mini_2024_07_]
approval_policy = "never"
model = "gpt-4o-mini-2024-07-18"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_gpt_5_nano_2025_08_0]
approval_policy = "never"
model = "gpt-5-nano-2025-08-07"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_llama_3_1_8b_instruc]
approval_policy = "never"
model = "llama-3.1-8b-instruct-fp8"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_llama_4_scout_17b_16]
approval_policy = "never"
model = "llama-4-scout-17b-16e-instruct"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_llama_fast_roblox]
approval_policy = "never"
model = "llama-fast-roblox"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_ministral_3b_2410]
approval_policy = "never"
model = "ministral-3b-2410"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_ministral_8b_2410]
approval_policy = "never"
model = "ministral-8b-2410"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_mirexa]
approval_policy = "never"
model = "mirexa"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_mistral_large_2402]
approval_policy = "never"
model = "mistral-large-2402"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_mistral_large_2407]
approval_policy = "never"
model = "mistral-large-2407"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_mistral_large_2411]
approval_policy = "never"
model = "mistral-large-2411"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_mistral_medium]
approval_policy = "never"
model = "mistral-medium"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_mistral_nemo_roblox]
approval_policy = "never"
model = "mistral-nemo-roblox"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_mistral_saba_2502]
approval_policy = "never"
model = "mistral-saba-2502"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_mistral_small_2402]
approval_policy = "never"
model = "mistral-small-2402"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_mistral_small_2409]
approval_policy = "never"
model = "mistral-small-2409"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_mistral_small_2501]
approval_policy = "never"
model = "mistral-small-2501"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_mistral_small_2503]
approval_policy = "never"
model = "mistral-small-2503"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_mistral_small_3_1_24]
approval_policy = "never"
model = "mistral-small-3.1-24b-instruct-2503"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_nova_fast]
approval_policy = "never"
model = "nova-fast"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_open_mistral_7b]
approval_policy = "never"
model = "open-mistral-7b"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_open_mistral_nemo]
approval_policy = "never"
model = "open-mistral-nemo"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_open_mixtral_8x22b]
approval_policy = "never"
model = "open-mixtral-8x22b"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_open_mixtral_8x7b]
approval_policy = "never"
model = "open-mixtral-8x7b"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_pixtral_12b_2409]
approval_policy = "never"
model = "pixtral-12b-2409"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_pixtral_large_2411]
approval_policy = "never"
model = "pixtral-large-2411"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_qwen2_5_coder_32b_in]
approval_policy = "never"
model = "qwen2.5-coder-32b-instruct"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_roblox_rp]
approval_policy = "never"
model = "roblox-rp"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.llm_rtist]
approval_policy = "never"
model = "rtist"
model_provider = "llm7"
model_reasoning_effort = "high"

[profiles.poe_app_creator]
approval_policy = "never"
model = "App-Creator"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_aya_expanse_32b]
approval_policy = "never"
model = "Aya-Expanse-32B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_aya_vision]
approval_policy = "never"
model = "Aya-Vision"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_bagoodex_web_search]
approval_policy = "never"
model = "Bagoodex-Web-Search"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_bria_eraser]
approval_policy = "never"
model = "Bria-Eraser"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_cartesia]
approval_policy = "never"
model = "Cartesia"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_chatgpt_4o_latest]
approval_policy = "never"
model = "ChatGPT-4o-Latest"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_clarity_upscaler]
approval_policy = "never"
model = "Clarity-Upscaler"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_haiku_3]
approval_policy = "never"
model = "Claude-Haiku-3"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_haiku_3_5]
approval_policy = "never"
model = "Claude-Haiku-3.5"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_haiku_3_5_sea]
approval_policy = "never"
model = "Claude-Haiku-3.5-Search"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_opus_3]
approval_policy = "never"
model = "Claude-Opus-3"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_opus_4]
approval_policy = "never"
model = "Claude-Opus-4"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_opus_4_reason]
approval_policy = "never"
model = "Claude-Opus-4-Reasoning"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_opus_4_search]
approval_policy = "never"
model = "Claude-Opus-4-Search"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_opus_4_1]
approval_policy = "never"
model = "Claude-Opus-4.1"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_sonnet_3_5]
approval_policy = "never"
model = "Claude-Sonnet-3.5"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_sonnet_3_5_ju]
approval_policy = "never"
model = "Claude-Sonnet-3.5-June"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_sonnet_3_5_se]
approval_policy = "never"
model = "Claude-Sonnet-3.5-Search"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_sonnet_3_7]
approval_policy = "never"
model = "Claude-Sonnet-3.7"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_sonnet_3_7_re]
approval_policy = "never"
model = "Claude-Sonnet-3.7-Reasoning"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_sonnet_3_7_se]
approval_policy = "never"
model = "Claude-Sonnet-3.7-Search"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_sonnet_4]
approval_policy = "never"
model = "Claude-Sonnet-4"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_sonnet_4_reas]
approval_policy = "never"
model = "Claude-Sonnet-4-Reasoning"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_claude_sonnet_4_sear]
approval_policy = "never"
model = "Claude-Sonnet-4-Search"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_command_r]
approval_policy = "never"
model = "Command-R"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_command_r_plus]
approval_policy = "never"
model = "Command-R-Plus"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_dall_e_3]
approval_policy = "never"
model = "DALL-E-3"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_deepclaude]
approval_policy = "never"
model = "DeepClaude"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_deepseek_r1]
approval_policy = "never"
model = "DeepSeek-R1"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_deepseek_r1_di]
approval_policy = "never"
model = "DeepSeek-R1-DI"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_deepseek_r1_distill]
approval_policy = "never"
model = "DeepSeek-R1-Distill"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_deepseek_r1_fw]
approval_policy = "never"
model = "DeepSeek-R1-FW"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_deepseek_r1_turbo_di]
approval_policy = "never"
model = "DeepSeek-R1-Turbo-DI"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_deepseek_v3]
approval_policy = "never"
model = "DeepSeek-V3"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_deepseek_v3_di]
approval_policy = "never"
model = "DeepSeek-V3-DI"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_deepseek_v3_turbo_di]
approval_policy = "never"
model = "DeepSeek-V3-Turbo-DI"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_deepseek_v3_fw]
approval_policy = "never"
model = "Deepseek-V3-FW"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_dream_machine]
approval_policy = "never"
model = "Dream-Machine"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_dreamina_3_1]
approval_policy = "never"
model = "Dreamina-3.1"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_elevenlabs]
approval_policy = "never"
model = "ElevenLabs"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_fill]
approval_policy = "never"
model = "FLUX-Fill"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_inpaint]
approval_policy = "never"
model = "FLUX-Inpaint"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_krea]
approval_policy = "never"
model = "FLUX-Krea"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_dev]
approval_policy = "never"
model = "FLUX-dev"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_dev_di]
approval_policy = "never"
model = "FLUX-dev-DI"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_dev_finetuner]
approval_policy = "never"
model = "FLUX-dev-finetuner"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_pro]
approval_policy = "never"
model = "FLUX-pro"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_pro_1_t]
approval_policy = "never"
model = "FLUX-pro-1-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_pro_1_1]
approval_policy = "never"
model = "FLUX-pro-1.1"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_pro_1_1_t]
approval_policy = "never"
model = "FLUX-pro-1.1-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_pro_1_1_ultra]
approval_policy = "never"
model = "FLUX-pro-1.1-ultra"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_schnell]
approval_policy = "never"
model = "FLUX-schnell"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_schnell_di]
approval_policy = "never"
model = "FLUX-schnell-DI"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_1_dev_fw]
approval_policy = "never"
model = "Flux-1-Dev-FW"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_1_schnell_fw]
approval_policy = "never"
model = "Flux-1-Schnell-FW"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_kontext_max]
approval_policy = "never"
model = "Flux-Kontext-Max"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_kontext_pro]
approval_policy = "never"
model = "Flux-Kontext-Pro"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_flux_schnell_t]
approval_policy = "never"
model = "Flux-Schnell-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_glm_4_5]
approval_policy = "never"
model = "GLM-4.5"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_3_5_turbo]
approval_policy = "never"
model = "GPT-3.5-Turbo"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_3_5_turbo_instru]
approval_policy = "never"
model = "GPT-3.5-Turbo-Instruct"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_3_5_turbo_raw]
approval_policy = "never"
model = "GPT-3.5-Turbo-Raw"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_4_classic]
approval_policy = "never"
model = "GPT-4-Classic"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_4_classic_0314]
approval_policy = "never"
model = "GPT-4-Classic-0314"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_4_turbo]
approval_policy = "never"
model = "GPT-4-Turbo"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_4_1]
approval_policy = "never"
model = "GPT-4.1"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_4_1_mini]
approval_policy = "never"
model = "GPT-4.1-mini"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_4_1_nano]
approval_policy = "never"
model = "GPT-4.1-nano"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_4o]
approval_policy = "never"
model = "GPT-4o"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_4o_aug]
approval_policy = "never"
model = "GPT-4o-Aug"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_4o_search]
approval_policy = "never"
model = "GPT-4o-Search"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_4o_mini]
approval_policy = "never"
model = "GPT-4o-mini"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_4o_mini_search]
approval_policy = "never"
model = "GPT-4o-mini-Search"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_5]
approval_policy = "never"
model = "GPT-5"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_5_chat]
approval_policy = "never"
model = "GPT-5-Chat"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_5_mini]
approval_policy = "never"
model = "GPT-5-mini"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_5_nano]
approval_policy = "never"
model = "GPT-5-nano"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_image_1]
approval_policy = "never"
model = "GPT-Image-1"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_oss_120b]
approval_policy = "never"
model = "GPT-OSS-120B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_oss_120b_t]
approval_policy = "never"
model = "GPT-OSS-120B-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_oss_20b]
approval_policy = "never"
model = "GPT-OSS-20B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_oss_20b_t]
approval_policy = "never"
model = "GPT-OSS-20B-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gpt_researcher]
approval_policy = "never"
model = "GPT-Researcher"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gemini_1_5_flash]
approval_policy = "never"
model = "Gemini-1.5-Flash"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gemini_1_5_flash_sea]
approval_policy = "never"
model = "Gemini-1.5-Flash-Search"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gemini_1_5_pro]
approval_policy = "never"
model = "Gemini-1.5-Pro"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gemini_1_5_pro_searc]
approval_policy = "never"
model = "Gemini-1.5-Pro-Search"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gemini_2_0_flash]
approval_policy = "never"
model = "Gemini-2.0-Flash"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gemini_2_0_flash_lit]
approval_policy = "never"
model = "Gemini-2.0-Flash-Lite"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gemini_2_0_flash_pre]
approval_policy = "never"
model = "Gemini-2.0-Flash-Preview"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gemini_2_5_flash]
approval_policy = "never"
model = "Gemini-2.5-Flash"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gemini_2_5_flash_lit]
approval_policy = "never"
model = "Gemini-2.5-Flash-Lite-Preview"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gemini_2_5_pro]
approval_policy = "never"
model = "Gemini-2.5-Pro"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gemma_2_27b_t]
approval_policy = "never"
model = "Gemma-2-27b-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_gemma_3_27b]
approval_policy = "never"
model = "Gemma-3-27B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_grok_2]
approval_policy = "never"
model = "Grok-2"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_grok_3]
approval_policy = "never"
model = "Grok-3"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_grok_3_mini]
approval_policy = "never"
model = "Grok-3-Mini"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_grok_4]
approval_policy = "never"
model = "Grok-4"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_hailuo_02]
approval_policy = "never"
model = "Hailuo-02"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_hailuo_02_pro]
approval_policy = "never"
model = "Hailuo-02-Pro"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_hailuo_02_standard]
approval_policy = "never"
model = "Hailuo-02-Standard"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_hailuo_ai]
approval_policy = "never"
model = "Hailuo-AI"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_hailuo_director_01]
approval_policy = "never"
model = "Hailuo-Director-01"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_hailuo_live]
approval_policy = "never"
model = "Hailuo-Live"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_hailuo_speech_02]
approval_policy = "never"
model = "Hailuo-Speech-02"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_hermes_3_70b]
approval_policy = "never"
model = "Hermes-3-70B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_hidream_i1_full]
approval_policy = "never"
model = "Hidream-I1-full"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_ideogram]
approval_policy = "never"
model = "Ideogram"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_ideogram_v2]
approval_policy = "never"
model = "Ideogram-v2"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_ideogram_v2a]
approval_policy = "never"
model = "Ideogram-v2a"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_ideogram_v2a_turbo]
approval_policy = "never"
model = "Ideogram-v2a-Turbo"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_ideogram_v3]
approval_policy = "never"
model = "Ideogram-v3"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_imagen_3]
approval_policy = "never"
model = "Imagen-3"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_imagen_3_fast]
approval_policy = "never"
model = "Imagen-3-Fast"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_imagen_4]
approval_policy = "never"
model = "Imagen-4"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_imagen_4_fast]
approval_policy = "never"
model = "Imagen-4-Fast"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_imagen_4_ultra_exp]
approval_policy = "never"
model = "Imagen-4-Ultra-Exp"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_inception_mercury]
approval_policy = "never"
model = "Inception-Mercury"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_inception_mercury_co]
approval_policy = "never"
model = "Inception-Mercury-Coder"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_kimi_k2]
approval_policy = "never"
model = "Kimi-K2"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_kimi_k2_instruct_n]
approval_policy = "never"
model = "Kimi-K2-Instruct-N"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_kimi_k2_t]
approval_policy = "never"
model = "Kimi-K2-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_kling_1_5_pro]
approval_policy = "never"
model = "Kling-1.5-Pro"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_kling_1_6_pro]
approval_policy = "never"
model = "Kling-1.6-Pro"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_kling_2_0_master]
approval_policy = "never"
model = "Kling-2.0-Master"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_kling_2_1_master]
approval_policy = "never"
model = "Kling-2.1-Master"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_kling_2_1_pro]
approval_policy = "never"
model = "Kling-2.1-Pro"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_kling_2_1_std]
approval_policy = "never"
model = "Kling-2.1-Std"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_kling_pro_effects]
approval_policy = "never"
model = "Kling-Pro-Effects"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_liveportrait]
approval_policy = "never"
model = "LivePortrait"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_70b_fp16]
approval_policy = "never"
model = "Llama-3-70B-FP16"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_70b_t]
approval_policy = "never"
model = "Llama-3-70B-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_70b_groq]
approval_policy = "never"
model = "Llama-3-70b-Groq"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_8b_t]
approval_policy = "never"
model = "Llama-3-8B-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_8b_groq]
approval_policy = "never"
model = "Llama-3-8b-Groq"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_405b]
approval_policy = "never"
model = "Llama-3.1-405B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_405b_fp16]
approval_policy = "never"
model = "Llama-3.1-405B-FP16"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_405b_fw]
approval_policy = "never"
model = "Llama-3.1-405B-FW"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_405b_t]
approval_policy = "never"
model = "Llama-3.1-405B-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_70b]
approval_policy = "never"
model = "Llama-3.1-70B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_70b_fp16]
approval_policy = "never"
model = "Llama-3.1-70B-FP16"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_70b_fw]
approval_policy = "never"
model = "Llama-3.1-70B-FW"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_70b_t]
approval_policy = "never"
model = "Llama-3.1-70B-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_8b]
approval_policy = "never"
model = "Llama-3.1-8B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_8b_di]
approval_policy = "never"
model = "Llama-3.1-8B-DI"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_8b_fp16]
approval_policy = "never"
model = "Llama-3.1-8B-FP16"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_8b_fw]
approval_policy = "never"
model = "Llama-3.1-8B-FW"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_8b_t_128k]
approval_policy = "never"
model = "Llama-3.1-8B-T-128k"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_1_nemotron]
approval_policy = "never"
model = "Llama-3.1-Nemotron"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_3_70b]
approval_policy = "never"
model = "Llama-3.3-70B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_3_70b_di]
approval_policy = "never"
model = "Llama-3.3-70B-DI"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_3_70b_fw]
approval_policy = "never"
model = "Llama-3.3-70B-FW"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_3_3_70b_vers]
approval_policy = "never"
model = "Llama-3.3-70B-Vers"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_4_maverick]
approval_policy = "never"
model = "Llama-4-Maverick"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_4_maverick_b10]
approval_policy = "never"
model = "Llama-4-Maverick-B10"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_4_maverick_t]
approval_policy = "never"
model = "Llama-4-Maverick-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_4_scout]
approval_policy = "never"
model = "Llama-4-Scout"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_4_scout_b10]
approval_policy = "never"
model = "Llama-4-Scout-B10"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_4_scout_cs]
approval_policy = "never"
model = "Llama-4-Scout-CS"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_llama_4_scout_t]
approval_policy = "never"
model = "Llama-4-Scout-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_luma_photon]
approval_policy = "never"
model = "Luma-Photon"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_luma_photon_flash]
approval_policy = "never"
model = "Luma-Photon-Flash"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_lyria]
approval_policy = "never"
model = "Lyria"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_markitdown]
approval_policy = "never"
model = "MarkItDown"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_minimax_m1]
approval_policy = "never"
model = "MiniMax-M1"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_mistral_7b_v0_3_di]
approval_policy = "never"
model = "Mistral-7B-v0.3-DI"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_mistral_7b_v0_3_t]
approval_policy = "never"
model = "Mistral-7B-v0.3-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_mistral_large_2]
approval_policy = "never"
model = "Mistral-Large-2"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_mistral_medium]
approval_policy = "never"
model = "Mistral-Medium"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_mistral_nemo]
approval_policy = "never"
model = "Mistral-NeMo"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_mistral_small_3]
approval_policy = "never"
model = "Mistral-Small-3"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_mistral_small_3_1]
approval_policy = "never"
model = "Mistral-Small-3.1"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_mistral_small_3_2]
approval_policy = "never"
model = "Mistral-Small-3.2"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_mixtral8x22b_inst_fw]
approval_policy = "never"
model = "Mixtral8x22b-Inst-FW"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_mochi_preview]
approval_policy = "never"
model = "Mochi-preview"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_omnihuman]
approval_policy = "never"
model = "OmniHuman"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_openai_gpt_oss_120b]
approval_policy = "never"
model = "OpenAI-GPT-OSS-120B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_openai_gpt_oss_20b]
approval_policy = "never"
model = "OpenAI-GPT-OSS-20B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_orpheus_tts]
approval_policy = "never"
model = "Orpheus-TTS"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_perplexity_deep_rese]
approval_policy = "never"
model = "Perplexity-Deep-Research"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_perplexity_r1_1776]
approval_policy = "never"
model = "Perplexity-R1-1776"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_perplexity_sonar]
approval_policy = "never"
model = "Perplexity-Sonar"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_perplexity_sonar_pro]
approval_policy = "never"
model = "Perplexity-Sonar-Pro"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_perplexity_sonar_rsn]
approval_policy = "never"
model = "Perplexity-Sonar-Rsn-Pro"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_phi_4_di]
approval_policy = "never"
model = "Phi-4-DI"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_phoenix_1_0]
approval_policy = "never"
model = "Phoenix-1.0"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_pika]
approval_policy = "never"
model = "Pika"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_pixverse_v4_5]
approval_policy = "never"
model = "Pixverse-v4.5"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_playai_dialog]
approval_policy = "never"
model = "PlayAI-Dialog"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_playai_tts]
approval_policy = "never"
model = "PlayAI-TTS"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_poe_system_bot]
approval_policy = "never"
model = "Poe-System-Bot"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_python]
approval_policy = "never"
model = "Python"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwq_32b_b10]
approval_policy = "never"
model = "QwQ-32B-B10"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwq_32b_preview_t]
approval_policy = "never"
model = "QwQ-32B-Preview-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwq_32b_t]
approval_policy = "never"
model = "QwQ-32B-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen_2_5_72b_t]
approval_policy = "never"
model = "Qwen-2.5-72B-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen_2_5_7b_t]
approval_policy = "never"
model = "Qwen-2.5-7B-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen_2_5_coder_32b_t]
approval_policy = "never"
model = "Qwen-2.5-Coder-32B-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen_2_5_vl_32b]
approval_policy = "never"
model = "Qwen-2.5-VL-32b"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen_3_235b_2507_t]
approval_policy = "never"
model = "Qwen-3-235B-2507-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen_72b_t]
approval_policy = "never"
model = "Qwen-72B-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen2_72b_instruct_t]
approval_policy = "never"
model = "Qwen2-72B-Instruct-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen2_5_coder_32b]
approval_policy = "never"
model = "Qwen2.5-Coder-32B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen2_5_vl_72b_t]
approval_policy = "never"
model = "Qwen2.5-VL-72B-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen3_235b_2507_cs]
approval_policy = "never"
model = "Qwen3-235B-2507-CS"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen3_235b_2507_fw]
approval_policy = "never"
model = "Qwen3-235B-2507-FW"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen3_235b_a22b]
approval_policy = "never"
model = "Qwen3-235B-A22B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen3_235b_a22b_di]
approval_policy = "never"
model = "Qwen3-235B-A22B-DI"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen3_32b_cs]
approval_policy = "never"
model = "Qwen3-32B-CS"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen3_coder_480b_fw]
approval_policy = "never"
model = "Qwen3-Coder-480B-FW"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_qwen3_coder_480b_t]
approval_policy = "never"
model = "Qwen3-Coder-480B-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_ray2]
approval_policy = "never"
model = "Ray2"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_recraft_v3]
approval_policy = "never"
model = "Recraft-V3"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_reka_core]
approval_policy = "never"
model = "Reka-Core"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_reka_flash]
approval_policy = "never"
model = "Reka-Flash"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_reka_research]
approval_policy = "never"
model = "Reka-Research"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_restyler]
approval_policy = "never"
model = "Restyler"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_retro_diffusion_core]
approval_policy = "never"
model = "Retro-Diffusion-Core"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_runway]
approval_policy = "never"
model = "Runway"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_runway_gen_4_turbo]
approval_policy = "never"
model = "Runway-Gen-4-Turbo"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_sana_t2i]
approval_policy = "never"
model = "Sana-T2I"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_seedance_1_0_lite]
approval_policy = "never"
model = "Seedance-1.0-Lite"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_seedance_1_0_pro]
approval_policy = "never"
model = "Seedance-1.0-Pro"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_seedream_3_0]
approval_policy = "never"
model = "Seedream-3.0"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_sketch_to_image]
approval_policy = "never"
model = "Sketch-to-Image"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_solar_pro_2]
approval_policy = "never"
model = "Solar-Pro-2"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_sora]
approval_policy = "never"
model = "Sora"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_stablediffusion3_2b]
approval_policy = "never"
model = "StableDiffusion3-2B"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_stablediffusion3_5_l]
approval_policy = "never"
model = "StableDiffusion3.5-L"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_stablediffusion3_5_t]
approval_policy = "never"
model = "StableDiffusion3.5-T"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_stablediffusionxl]
approval_policy = "never"
model = "StableDiffusionXL"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_tako]
approval_policy = "never"
model = "Tako"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_topazlabs]
approval_policy = "never"
model = "TopazLabs"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_trellis_3d]
approval_policy = "never"
model = "Trellis-3D"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_twelvelabs]
approval_policy = "never"
model = "TwelveLabs"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_unreal_speech_tts]
approval_policy = "never"
model = "Unreal-Speech-TTS"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_veo_2]
approval_policy = "never"
model = "Veo-2"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_veo_2_video]
approval_policy = "never"
model = "Veo-2-Video"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_veo_3]
approval_policy = "never"
model = "Veo-3"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_veo_3_fast]
approval_policy = "never"
model = "Veo-3-Fast"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_wan_2_1]
approval_policy = "never"
model = "Wan-2.1"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_wan_2_2]
approval_policy = "never"
model = "Wan-2.2"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_web_search]
approval_policy = "never"
model = "Web-Search"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_o1]
approval_policy = "never"
model = "o1"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_o1_mini]
approval_policy = "never"
model = "o1-mini"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_o1_pro]
approval_policy = "never"
model = "o1-pro"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_o3]
approval_policy = "never"
model = "o3"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_o3_deep_research]
approval_policy = "never"
model = "o3-deep-research"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_o3_mini]
approval_policy = "never"
model = "o3-mini"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_o3_mini_high]
approval_policy = "never"
model = "o3-mini-high"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_o3_pro]
approval_policy = "never"
model = "o3-pro"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_o4_mini]
approval_policy = "never"
model = "o4-mini"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_o4_mini_deep_researc]
approval_policy = "never"
model = "o4-mini-deep-research"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.poe_remove_background]
approval_policy = "never"
model = "remove-background"
model_provider = "poe"
model_reasoning_effort = "high"

[profiles.pol_bidara]
approval_policy = "never"
model = "bidara"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_elixposearch]
approval_policy = "never"
model = "elixposearch"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_evil]
approval_policy = "never"
model = "evil"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_gpt_5_nano]
approval_policy = "never"
model = "gpt-5-nano"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_llama_fast_roblox]
approval_policy = "never"
model = "llama-fast-roblox"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_llama_roblox]
approval_policy = "never"
model = "llama-roblox"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_llamascout]
approval_policy = "never"
model = "llamascout"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_midijourney]
approval_policy = "never"
model = "midijourney"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_mirexa]
approval_policy = "never"
model = "mirexa"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_mistral]
approval_policy = "never"
model = "mistral"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_mistral_nemo_roblox]
approval_policy = "never"
model = "mistral-nemo-roblox"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_mistral_roblox]
approval_policy = "never"
model = "mistral-roblox"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_nova_fast]
approval_policy = "never"
model = "nova-fast"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_openai]
approval_policy = "never"
model = "openai"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_openai_audio]
approval_policy = "never"
model = "openai-audio"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_openai_fast]
approval_policy = "never"
model = "openai-fast"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_openai_large]
approval_policy = "never"
model = "openai-large"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_openai_roblox]
approval_policy = "never"
model = "openai-roblox"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_qwen_coder]
approval_policy = "never"
model = "qwen-coder"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_roblox_rp]
approval_policy = "never"
model = "roblox-rp"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_rtist]
approval_policy = "never"
model = "rtist"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_sur]
approval_policy = "never"
model = "sur"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.pol_unity]
approval_policy = "never"
model = "unity"
model_provider = "pollinations"
model_reasoning_effort = "high"

[profiles.lms_darkidol_longwriter_]
approval_policy = "never"
model = "darkidol-longwriter-v13-8b-uncensored-1048k"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_deepseek_r1_distill_]
approval_policy = "never"
model = "deepseek-r1-distill-qwen-14b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_dragon_mistral_7b_v0]
approval_policy = "never"
model = "dragon-mistral-7b-v0"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_fix_json]
approval_policy = "never"
model = "fix-json"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_florence_2_base_ft]
approval_policy = "never"
model = "florence-2-base-ft"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_florence_2_large_ft]
approval_policy = "never"
model = "florence-2-large-ft"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_gemma_3_270m_it_qat]
approval_policy = "never"
model = "gemma-3-270m-it-qat"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_google_gemma_3_1b]
approval_policy = "never"
model = "google/gemma-3-1b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_google_gemma_3_4b]
approval_policy = "never"
model = "google/gemma-3-4b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_granite_4_0_tiny_pre]
approval_policy = "never"
model = "granite-4.0-tiny-preview"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_huihui_qwen3_14b_abl]
approval_policy = "never"
model = "huihui-qwen3-14b-abliterated-v2"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_llama_3_1_8b_sarcasm]
approval_policy = "never"
model = "llama-3.1-8b-sarcasm"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_llama_3_2_8x3b_moe_d]
approval_policy = "never"
model = "llama-3.2-8x3b-moe-dark-champion-instruct-uncensored-abliterated-18.4b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_llama3_2_entity]
approval_policy = "never"
model = "llama3.2-entity"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_llama3_2_entity_1b]
approval_policy = "never"
model = "llama3.2-entity-1b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_longwriter_v_7b_dpo_]
approval_policy = "never"
model = "longwriter-v-7b-dpo-i1"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_magistral_small_2507]
approval_policy = "never"
model = "magistral-small-2507-rebased-vision-i1"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_meta_llama_3_1_8b_in]
approval_policy = "never"
model = "meta-llama-3.1-8b-instruct-summarizer"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_minicpm4_0_5b]
approval_policy = "never"
model = "minicpm4-0.5b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_minicpm4_8b]
approval_policy = "never"
model = "minicpm4-8b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_mistral_7b_instruct_]
approval_policy = "never"
model = "mistral-7b-instruct-v0.3-196k-dq68-mlx"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_mistral_7b_pii_entit]
approval_policy = "never"
model = "mistral-7b-pii-entity-extractor"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_mistral_7b_sarcasm_s]
approval_policy = "never"
model = "mistral-7b-sarcasm-scrolls-v2"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_mistral_small_24b_in]
approval_policy = "never"
model = "mistral-small-24b-instruct-2501-writer-i1"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_mistralai_voxtral_mi]
approval_policy = "never"
model = "mistralai_voxtral-mini-3b-2507"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_mungert_text_embeddi]
approval_policy = "never"
model = "mungert/text-embedding-qwen3-embedding-0.6b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_numarkdown_8b_thinki]
approval_policy = "never"
model = "numarkdown-8b-thinking-i1"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_ocr_vlm_qwen2_5vl_3b]
approval_policy = "never"
model = "ocr_vlm-qwen2.5vl-3b-final"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_ocrflux_3b]
approval_policy = "never"
model = "ocrflux-3b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_olmocr_7b_0725]
approval_policy = "never"
model = "olmocr-7b-0725"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_openai_gpt_oss_20b]
approval_policy = "never"
model = "openai/gpt-oss-20b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_openai_gpt_oss_20b_n]
approval_policy = "never"
model = "openai_gpt-oss-20b-neo"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_osmosis_structure_0_]
approval_policy = "never"
model = "osmosis-structure-0.6b@q8_0"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_pixtral_12b_i1]
approval_policy = "never"
model = "pixtral-12b-i1"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_qwen_qwen2_5_vl_7b]
approval_policy = "never"
model = "qwen/qwen2.5-vl-7b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_qwen_text_embedding_]
approval_policy = "never"
model = "qwen/text-embedding-qwen3-embedding-0.6b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_qwen2_audio_7b_instr]
approval_policy = "never"
model = "qwen2-audio-7b-instruct-i1"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_qwen2_5_omni_3b]
approval_policy = "never"
model = "qwen2.5-omni-3b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_qwen2_5_omni_7b]
approval_policy = "never"
model = "qwen2.5-omni-7b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_qwen3_30b_a3b_instru]
approval_policy = "never"
model = "qwen3-30b-a3b-instruct-2507"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_qwen3_30b_a3b_thinki]
approval_policy = "never"
model = "qwen3-30b-a3b-thinking-2507"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_qwen3_4b_instruct_25]
approval_policy = "never"
model = "qwen3-4b-instruct-2507"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_qwen3_4b_thinking_25]
approval_policy = "never"
model = "qwen3-4b-thinking-2507"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_qwen3_8b_320k_contex]
approval_policy = "never"
model = "qwen3-8b-320k-context-10x-massive"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_qwen3_coder_30b_a3b_]
approval_policy = "never"
model = "qwen3-coder-30b-a3b-instruct-1m"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_r3_qwen3_14b_skywork]
approval_policy = "never"
model = "r3-qwen3-14b-skywork-i1"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_skycaptioner_v1]
approval_policy = "never"
model = "skycaptioner-v1"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_category_tool]
approval_policy = "never"
model = "slim-category-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_emotions_tool]
approval_policy = "never"
model = "slim-emotions-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_extract_qwen_1_]
approval_policy = "never"
model = "slim-extract-qwen-1.5b"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_extract_tiny_to]
approval_policy = "never"
model = "slim-extract-tiny-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_extract_tool]
approval_policy = "never"
model = "slim-extract-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_intent_tool]
approval_policy = "never"
model = "slim-intent-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_ner]
approval_policy = "never"
model = "slim-ner"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_ner_tool]
approval_policy = "never"
model = "slim-ner-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_nli_tool]
approval_policy = "never"
model = "slim-nli-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_q_gen_tiny_tool]
approval_policy = "never"
model = "slim-q-gen-tiny-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_qa_gen_tiny_too]
approval_policy = "never"
model = "slim-qa-gen-tiny-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_sa_ner_tool]
approval_policy = "never"
model = "slim-sa-ner-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_sentiment_tool]
approval_policy = "never"
model = "slim-sentiment-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_sql_tool]
approval_policy = "never"
model = "slim-sql-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_summary_tool]
approval_policy = "never"
model = "slim-summary-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_tags_tool]
approval_policy = "never"
model = "slim-tags-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_topics_tool]
approval_policy = "never"
model = "slim-topics-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_slim_xsum_tool]
approval_policy = "never"
model = "slim-xsum-tool"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_text_embedding_bge_m]
approval_policy = "never"
model = "text-embedding-bge-m3"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_text_embedding_jina_]
approval_policy = "never"
model = "text-embedding-jina-embeddings-v4-text-retrieval@q8_0"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_text_embedding_mxbai]
approval_policy = "never"
model = "text-embedding-mxbai-embed-xsmall-v1-i1"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_text_embedding_nomic]
approval_policy = "never"
model = "text-embedding-nomic-embed-text-v2"
model_provider = "lmstudio"
model_reasoning_effort = "high"

[profiles.lms_text_embedding_qwen3]
approval_policy = "never"
model = "text-embedding-qwen3-embedding-8b"
model_provider = "lmstudio"
model_reasoning_effort = "high"


</document_content>
</document>

<document index="16">
<source>config/mods/mods.yml</source>
<document_content>
---
apis:
  anthropic:
    api-key: null
    api-key-env: ANTHROPIC_API_KEY
    base-url: https://api.anthropic.com/v1
    models:
      claude-3-5-sonnet-20240620:
        max-input-chars: 680000
      claude-3-5-sonnet-20241022:
        max-input-chars: 680000
      claude-3-5-sonnet-latest:
        aliases: ['4o-mini']
        max-input-chars: 680000
      claude-3-7-sonnet-20250219:
        max-input-chars: 680000
      claude-3-7-sonnet-latest:
        aliases: ['claude3.7-sonnet', 'claude-3-7-sonnet', 'sonnet-3.7']
        max-input-chars: 680000
      claude-3-opus-20240229:
        aliases: ['claude3-opus', 'opus']
        max-input-chars: 680000
      claude-sonnet-4-20250514:
        aliases: ['claude-sonnet-4', 'sonnet-4']
        max-input-chars: 680000
  azure:
    api-key: null
    api-key-env: AZURE_OPENAI_KEY
    # Set to 'azure-ad' to use Active Directory
    # Azure OpenAI setup: https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource
    base-url: https://YOUR_RESOURCE_NAME.openai.azure.com
    models:
      gpt-35:
        aliases: ['35t']
        fallback: null
        max-input-chars: 12250
      gpt-35-turbo:
        aliases: ['az35t']
        fallback: gpt-35
        max-input-chars: 12250
      gpt-4:
        aliases: ['az4']
        fallback: gpt-35-turbo
        max-input-chars: 24500
      o1-mini:
        aliases: ['o1-mini']
        max-input-chars: 128000
      o1-preview:
        aliases: ['o1-preview']
        max-input-chars: 128000
  cerebras:
    api-key: null
    api-key-env: CEREBRAS_API_KEY
    base-url: https://api.cerebras.ai/v1
    models:
      qwen-3-coder-480b:
        aliases: ['qwen-3-coder']
        max-completion-tokens: 40000
        max-input-chars: 131072
  chutes:
    api-key: null
    api-key-env: CHUTES_API_KEY
    base-url: https://llm.chutes.ai/v1
    models:
      Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8:
        max-completion-tokens: 32767
        max-input-chars: 262144
      openai/gpt-oss-120b:
        max-completion-tokens: 32767
        max-input-chars: 128000
  cohere:
    base-url: https://api.cohere.com/v1
    models:
      command-r:
        max-input-chars: 128000
      command-r-plus:
        max-input-chars: 128000
  copilot:
    base-url: https://api.githubcopilot.com
    models:
      claude-3.5-sonnet:
        aliases: ['gpt-4.5', 'gpt4.5']
        max-input-chars: 680000
      gemini-2.0-flash-001:
        aliases: ['gm2f', 'flash-2', 'gemini-2-flash']
        max-input-chars: 4194304
      gpt-3.5-turbo:
        aliases: ['35t']
        max-input-chars: 12250
      gpt-4:
        aliases: ['4']
        max-input-chars: 24500
      gpt-4o-2024-05-13:
        aliases: ['4o-2024', '4o', 'gpt-4o']
        max-input-chars: 392000
      o1-mini:
        aliases: ['o1-mini', 'o1m', 'o1-mini-2024-09-12']
        max-input-chars: 128000
      o1-preview:
        aliases: ['o1-preview']
        max-input-chars: 128000
      o1-preview-2024-09-12:
        aliases: ['o1-preview', 'o1p']
        max-input-chars: 128000
      o3-mini:
        aliases: ['o3m', 'o3-mini']
        max-input-chars: 128000
  # DeepSeek
  # https://api-docs.deepseek.com
  deepseek:
    api-key: null
    api-key-env: DEEPSEEK_API_KEY
    base-url: https://api.deepseek.com/
    models:
      deepseek-chat:
        aliases: ['35t-1106']
        max-input-chars: 384000
      deepseek-reasoner:
        aliases: ['r1']
        max-input-chars: 384000
  featherless:
    api-key-env: FEATHERLESS_API_KEY
    base-url: https://api.featherless.ai/v1
    models:
      openai/gpt-oss-120b:
        max-completion-tokens: 4096
        max-input-chars: 16384
  # GitHub Models
  # https://github.com/marketplace/models
  github-models:
    api-key: null
    api-key-env: GITHUB_PERSONAL_ACCESS_TOKEN
    base-url: https://models.github.ai/inference
    models:
      ai21-labs/AI21-Jamba-1.5-Large:
        max-input-chars: 392000
      ai21-labs/AI21-Jamba-1.5-Mini:
        max-input-chars: 392000
      cohere/Cohere-command-r:
        max-input-chars: 392000
      cohere/Cohere-command-r-08-2024:
        max-input-chars: 392000
      cohere/Cohere-command-r-plus:
        max-input-chars: 392000
      cohere/Cohere-command-r-plus-08-2024:
        max-input-chars: 392000
      cohere/Cohere-embed-v3-english:
        max-input-chars: 392000
      cohere/Cohere-embed-v3-multilingual:
        max-input-chars: 392000
      cohere/cohere-command-a:
        max-input-chars: 392000
      core42/jais-30b-chat:
        max-input-chars: 392000
      deepseek/DeepSeek-R1:
        max-input-chars: 392000
      deepseek/DeepSeek-V3-0324:
        max-input-chars: 392000
      meta/Llama-3.2-11B-Vision-Instruct:
        max-input-chars: 392000
      meta/Llama-3.2-90B-Vision-Instruct:
        max-input-chars: 392000
      meta/Llama-3.3-70B-Instruct:
        max-input-chars: 392000
      meta/Llama-4-Maverick-17B-128E-Instruct-FP8:
        max-input-chars: 392000
      meta/Llama-4-Scout-17B-16E-Instruct:
        max-input-chars: 392000
      meta/Meta-Llama-3-70B-Instruct:
        max-input-chars: 392000
      meta/Meta-Llama-3-8B-Instruct:
        max-input-chars: 392000
      meta/Meta-Llama-3.1-405B-Instruct:
        max-input-chars: 392000
      meta/Meta-Llama-3.1-70B-Instruct:
        max-input-chars: 392000
      meta/Meta-Llama-3.1-8B-Instruct:
        max-input-chars: 392000
      microsoft/MAI-DS-R1:
        max-input-chars: 392000
      microsoft/Phi-3-medium-128k-instruct:
        max-input-chars: 392000
      microsoft/Phi-3-medium-4k-instruct:
        max-input-chars: 392000
      microsoft/Phi-3-mini-128k-instruct:
        max-input-chars: 392000
      microsoft/Phi-3-mini-4k-instruct:
        max-input-chars: 392000
      microsoft/Phi-3-small-128k-instruct:
        max-input-chars: 392000
      microsoft/Phi-3-small-8k-instruct:
        max-input-chars: 392000
      microsoft/Phi-3.5-MoE-instruct:
        max-input-chars: 392000
      microsoft/Phi-3.5-mini-instruct:
        max-input-chars: 392000
      microsoft/Phi-3.5-vision-instruct:
        max-input-chars: 392000
      microsoft/Phi-4:
        max-input-chars: 392000
      microsoft/Phi-4-mini-instruct:
        max-input-chars: 392000
      microsoft/Phi-4-mini-reasoning:
        max-input-chars: 392000
      microsoft/Phi-4-multimodal-instruct:
        max-input-chars: 392000
      microsoft/Phi-4-reasoning:
        max-input-chars: 392000
      mistral-ai/Codestral-2501:
        max-input-chars: 392000
      mistral-ai/Ministral-3B:
        max-input-chars: 392000
      mistral-ai/Mistral-Large-2411:
        max-input-chars: 392000
      mistral-ai/Mistral-Nemo:
        max-input-chars: 392000
      mistral-ai/mistral-medium-2505:
        max-input-chars: 392000
      mistral-ai/mistral-small-2503:
        max-input-chars: 392000
      openai/gpt-4.1:
        max-input-chars: 392000
      openai/o3-mini:
        max-input-chars: 392000
      openai/o4-mini:
        max-input-chars: 392000
      openai/text-embedding-3-large:
        max-input-chars: 392000
      openai/text-embedding-3-small:
        max-input-chars: 392000
      xai/grok-3:
        max-input-chars: 392000
      xai/grok-3-mini:
        max-input-chars: 392000
  google:
    models:
      gemini-1.5-flash-latest:
        aliases: ['gmf', 'flash', 'gemini-1.5-flash']
        max-input-chars: 392000
      gemini-1.5-pro-latest:
        aliases: ['gmp', 'gemini', 'gemini-1.5-pro']
        max-input-chars: 392000
      gemini-2.0-flash-001:
        aliases: ['4o']
        max-input-chars: 4194304
      gemini-2.0-flash-lite:
        aliases: ['gm2fl', 'flash-2-lite', 'gemini-2-flash-lite']
        max-input-chars: 4194304
  groq:
    api-key: null
    api-key-env: GROQ_API_KEY
    base-url: https://api.groq.com/openai/v1
    models:
      deepseek-r1-distill-llama-70b:
        aliases: ['deepseek-r1-llama', 'r1-llama']
        max-input-chars: 392000
      deepseek-r1-distill-llama-70b-specdec:
        aliases: ['deepseek-r1-specdec', 'r1-llama-specdec']
        max-completion-tokens: 49152
        max-input-chars: 392000
      deepseek-r1-distill-qwen-32b:
        aliases: ['4']
        max-completion-tokens: 49152
        max-input-chars: 392000
      # Production models
      gemma2-9b-it:
        aliases: ['gemma2', 'gemma']
        max-input-chars: 24500
      llama-3.1-8b-instant:
        aliases: ['llama3.1-8b', 'llama3.1-instant']
        max-completion-tokens: 24500
        max-input-chars: 392000
      llama-3.2-11b-vision-preview:
        aliases: ['llama3.2-vision', 'llama3.2-11b-vision']
        max-completion-tokens: 24500
        max-input-chars: 392000
      llama-3.2-1b-preview:
        aliases: ['llama3.2-1b']
        max-completion-tokens: 24500
        max-input-chars: 392000
      llama-3.2-3b-preview:
        aliases: ['llama3.2-3b']
        max-completion-tokens: 24500
        max-input-chars: 392000
      llama-3.2-90b-vision-preview:
        aliases: ['llama3.2-90b-vision']
        max-completion-tokens: 24500
        max-input-chars: 392000
      llama-3.3-70b-specdec:
        aliases: ['llama3.3-specdec']
        max-input-chars: 24500
      llama-3.3-70b-versatile:
        aliases: ['llama3.3', 'llama3.3-70b', 'llama3.3-versatile']
        max-completion-tokens: 98000
        max-input-chars: 392000
      llama-guard-3-8b:
        aliases: ['llama-guard']
        max-input-chars: 24500
      llama3-70b-8192:
        aliases: ['llama3', 'llama3-70b']
        fallback: llama3-8b-8192
        max-input-chars: 24500
      llama3-8b-8192:
        aliases: ['llama3-8b']
        max-input-chars: 24500
      meta-llama/llama-4-maverick-17b-128e-instruct:
        aliases: ['llama4', 'llama4-maverick']
        max-input-chars: 392000
      meta-llama/llama-4-scout-17b-16e-instruct:
        aliases: ['llama4-scout']
        max-input-chars: 392000
      # Preview models
      mistral-saba-24b:
        aliases: ['saba', 'mistral-saba', 'saba-24b']
        max-input-chars: 98000
      mixtral-8x7b-32768:
        aliases: ['mixtral']
        max-input-chars: 98000
      qwen-2.5-coder-32b:
        aliases: ['qwen-coder', 'qwen2.5-coder', 'qwen-2.5-coder']
        max-input-chars: 392000
  localai:
    # LocalAI setup instructions: https://github.com/go-skynet/LocalAI#example-use-gpt4all-j-model
    base-url: http://localhost:8080
    models:
      ggml-gpt4all-j:
        aliases: ['local', '4all']
        fallback: null
        max-input-chars: 12250
  mistral:
    api-key: null
    api-key-env: MISTRAL_API_KEY
    base-url: https://api.mistral.ai/v1
    models:
      mistral-large-latest:
        aliases: ['mistral-large']
        max-input-chars: 384000
      open-mistral-nemo:
        aliases: ['mistral-nemo']
        max-input-chars: 384000
  ollama:
    base-url: http://localhost:11434
    models:
      llama3.2:1b:
        aliases: ['llama3.2_1b']
        max-input-chars: 650000
      llama3.2:3b:
        aliases: ['llama3.2']
        max-input-chars: 650000
      llama3:70b:
        aliases: ['llama3']
        max-input-chars: 650000
  openai:
    api-key: null
    api-key-env: OPENAI_API_KEY
    base-url: https://api.openai.com/v1
    models:
      gpt-3.5:
        aliases: ['35']
        fallback: null
        max-input-chars: 12250
      gpt-3.5-turbo:
        fallback: gpt-3.5
        max-input-chars: 12250
      gpt-3.5-turbo-1106:
        fallback: gpt-3.5-turbo
        max-input-chars: 12250
      gpt-3.5-turbo-16k:
        aliases: ['35t16k']
        fallback: gpt-3.5
        max-input-chars: 44500
      gpt-4:
        fallback: gpt-3.5-turbo
        max-input-chars: 24500
      gpt-4-1106-preview:
        fallback: gpt-4
        max-input-chars: 392000
      gpt-4-32k:
        aliases: ['32k']
        fallback: gpt-4
        max-input-chars: 98000
      gpt-4.5-preview:
        fallback: gpt-4
        max-input-chars: 392000
      gpt-4.5-preview-2025-02-27:
        fallback: gpt-4
        max-input-chars: 392000
      gpt-4o:
        fallback: gpt-4
        max-input-chars: 392000
      gpt-4o-mini:
        fallback: gpt-4o
        max-input-chars: 392000
      o1:
        aliases: ['o1']
        max-input-chars: 200000
      o1-mini:
        aliases: ['o1-mini']
        max-input-chars: 128000
      o1-preview:
        aliases: ['o1-preview']
        max-input-chars: 128000
      o3-mini:
        aliases: ['o3m', 'o3-mini']
        max-input-chars: 200000
  openrouter:
    api-key: null
    api-key-env: OPENROUTER_API_KEY
    base-url: https://openrouter.ai/api/v1
    models:
      openrouter/sonoma-dusk-alpha:
        aliases: ['sonoma-dusk-alpha']
        max-completion-tokens: 128000
        max-tokens: 2000000
      openrouter/sonoma-sky-alpha:
        aliases: ['sonoma-sky-alpha']
        max-completion-tokens: 128000
        max-tokens: 2000000
  perplexity:
    api-key: null
    api-key-env: PERPLEXITY_API_KEY
    base-url: https://api.perplexity.ai
    models:
      llama-3.1-sonar-huge-128k-online:
        aliases: ['llam31-huge']
        max-input-chars: 127072
      llama-3.1-sonar-large-128k-online:
        aliases: ['llam31-large']
        max-input-chars: 127072
      llama-3.1-sonar-small-128k-online:
        aliases: ['llam31-small']
        max-input-chars: 127072
  runpod:
    api-key: null
    api-key-env: RUNPOD_API_KEY
    # https://docs.runpod.io/serverless/workers/vllm/openai-compatibility
    base-url: https://api.runpod.ai/v2/${YOUR_ENDPOINT}/openai/v1
    models:
      openchat/openchat-3.5-1210:
        aliases: ['openchat']
        max-input-chars: 8192
  sambanova:
    api-key: null
    api-key-env: SAMBANOVA_API_KEY
    base-url: https://api.sambanova.ai/v1
    # api-key-cmd: rbw get -f OPENAI_API_KEY chat.openai.com
    models:
      # Preview models
      DeepSeek-R1:
        aliases: ['deepseek-r1-sambanova', 'deepseek-r1-preview']
        max-input-chars: 24500
      # Production models
      DeepSeek-R1-Distill-Llama-70B:
        aliases: ['deepseek-r1-llama-sambanova', 'deepseek-r1-distill']
        max-input-chars: 98000
      Llama-3.1-Tulu-3-405B:
        aliases: ['llama3.1-tulu', 'tulu-405b']
        max-input-chars: 49000
      Llama-3.2-11B-Vision-Instruct:
        aliases: ['llama3.2-vision-11b', 'llama3.2-11b-vision-sambanova']
        max-input-chars: 12250
      Llama-3.2-90B-Vision-Instruct:
        aliases: ['llama3.2-vision-90b', 'llama3.2-90b-vision-sambanova']
        max-input-chars: 12250
      Meta-Llama-3.1-405B-Instruct:
        aliases: ['llama3.1-405b-sambanova']
        max-input-chars: 49000
      Meta-Llama-3.1-70B-Instruct:
        aliases: ['llama3.1-70b-sambanova']
        max-input-chars: 392000
      Meta-Llama-3.1-8B-Instruct:
        aliases: ['llama3.1-8b-sambanova']
        max-input-chars: 49000
      Meta-Llama-3.2-1B-Instruct:
        aliases: ['llama3.2-1b-sambanova']
        max-input-chars: 49000
      Meta-Llama-3.2-3B-Instruct:
        aliases: ['llama3.2-3b-sambanova']
        max-input-chars: 24500
      Meta-Llama-3.3-70B-Instruct:
        aliases: ['llama3.3-sambanova', 'llama3.3-70b-sambanova']
        max-input-chars: 392000
      Meta-Llama-Guard-3-8B:
        aliases: ['llama-guard-sambanova']
        max-input-chars: 24500
      QwQ-32B-Preview:
        aliases: ['qwq-sambanova', 'qwq-32b']
        max-input-chars: 49000
      Qwen2.5-72B-Instruct:
        aliases: ['qwen2.5-sambanova', 'qwen2.5-72b']
        max-input-chars: 49000
      Qwen2.5-Coder-32B-Instruct:
        aliases: ['qwen2.5-coder-sambanova', 'qwen-coder-sambanova']
        max-input-chars: 49000
# OpenAI compatible REST API (openai, localai, anthropic, ...)
default-api: cerebras
# Default model (gpt-3.5-turbo, gpt-4, ggml-gpt4all-j...)
default-model: qwen-3-coder-480b
# Your desired level of fanciness
fanciness: 10
  # Example, a role called `shell`:
  # shell:
  #   - you are a shell expert
  #   - you do not explain anything
  #   - you simply output one liners to solve the problems you're asked
  #   - you do not provide any explanation whatsoever, ONLY the command
# Ask for the response to be formatted as markdown unless otherwise set
format: false
# Text to append when using the -f flag
format-text:
  json: Format the response as json without enclosing backticks.
  markdown: Format the response as markdown without enclosing backticks.
# Include the prompt from the arguments and stdin, truncate stdin to specified number of lines
include-prompt: 0
# Include the prompt from the arguments in the response
include-prompt-args: false
#
max-completion-tokens: 65535
# Default character limit on input to model
max-input-chars: 100000000
# Maximum number of times to retry API calls
max-retries: 5
# MCP Servers configurations
mcp-servers:
  # Example: GitHub MCP via Docker:
  # github:
  #   command: docker
  #   env:
  #     - GITHUB_PERSONAL_ACCESS_TOKEN=xxxyyy
  #   args:
  #     - run
  #     - "-i"
  #     - "--rm"
  #     - "-e"
  #     - GITHUB_PERSONAL_ACCESS_TOKEN
  #     - "ghcr.io/github/github-mcp-server"
  # Timeout for MCP server calls, defaults to 15 seconds
  ask-pplx:
    alwaysAllow:
      - perplexity_ask
    args:
      - /usr/local/lib/node_modules/server-perplexity-ask/dist/index.js
    command: /usr/local/bin/node
    disabled: true
    disabledTools: []
    env:
      - PERPLEXITY_API_KEY=pplx-***
    type: stdio
  codex:
    args:
      - mcp
    command: /Users/adam/bin/codexx
    timeout: 600
  control-the-browser:
    active: false
    alwaysAllow:
      - browser_close
      - browser_resize
      - browser_console_messages
      - browser_handle_dialog
      - browser_evaluate
      - browser_file_upload
      - browser_install
      - browser_press_key
      - browser_navigate
      - browser_type
      - browser_wait_for
      - browser_tab_close
      - browser_tab_select
      - browser_tab_new
      - browser_hover
      - browser_select_option
      - browser_navigate_back
      - browser_navigate_forward
      - browser_tab_list
      - browser_drag
      - browser_network_requests
      - browser_take_screenshot
      - browser_snapshot
      - browser_click
    args:
      - /usr/local/lib/node_modules/@playwright/mcp/cli.js
    command: /usr/local/bin/node
    disabled: false
    disabledTools: []
    env: []
    timeout: 600
    type: stdio
  deep-research:
    alwaysAllow: []
    args:
      - /usr/local/lib/node_modules/@pinkpixel/deep-research-mcp/dist/index.js
    command: /usr/local/bin/node
    disabled: true
    env:
      - CRAWL_LIMIT='15'
      - CRAWL_MAX_DEPTH='2'
      - CRAWL_TIMEOUT='300'
      - FILE_WRITE_ENABLED='true'
      - FILE_WRITE_LINE_LIMIT='300'
      - MAX_SEARCH_RESULTS='10'
      - SEARCH_TIMEOUT='120'
      - TAVILY_API_KEY=tvly-***
  exa-search:
    active: true
    alwaysAllow:
      - web_search_exa
    args:
      - /usr/local/lib/node_modules/exa-mcp-server/build/index.js
    command: /usr/local/bin/node
    disabled: true
    disabledTools: []
    env:
      - EXA_API_KEY=***
    type: stdio
  gemini-cli:
    active: true
    alwaysAllow: []
    args:
      - /usr/local/lib/node_modules/gemini-mcp-tool/dist/index.js
    command: /usr/local/bin/node
    disabled: false
    disabledTools: []
    env: []
    type: stdio
  hyper-mcp:
    active: true
    alwaysAllow:
      - think
      - c7_resolve_library_id
      - c7_get_library_docs
      - fetch
      - time
      - read_file
      - read_multiple_files
      - search_files
      - get_file_info
    args: []
    command: /Users/adam/.cargo/bin/hyper-mcp
    disabledTools:
      - hash
      - myip
      - gh-list-issues
      - gh-create-issue
      - gh-get-issue
      - gh-update-issue
      - gh-add-issue-comment
      - gh-get-file-contents
      - gh-create-or-update-file
      - gh-create-branch
      - gh-list-pull-requests
      - gh-create-pull-request
      - gh-get-repo-contributors
      - gh-get-repo-collaborators
      - gh-get-repo-details
      - gh-list-repos
      - gh-create-gist
      - gh-get-gist
      - gh-update-gist
      - gh-delete-gist
      - write_file
      - edit_file
      - create_dir
      - list_dir
      - move_file
    env: []
    type: stdio
  screenshot-fast:
    alwaysAllow:
      - capture_console
      - take_screenshot
    args:
      - /usr/local/lib/node_modules/@just-every/mcp-screenshot-website-fast/bin/mcp-screenshot-website.js
      - serve
    command: /usr/local/bin/node
    disabled: true
    disabledTools:
      - take_screencast
    env: []
    timeout: 300
    type: stdio
mcp-timeout: 15s
# Turn off the client-side limit on the size of the input into the model
no-limit: false
# Quiet mode (hide the spinner while loading and stderr messages for success)
quiet: false
# Render output as raw text when connected to a TTY
raw: false
role: default
# System role to use
# List of predefined system messages that can be used as roles
roles:
  default: []
# Text to show while generating
status-text: Generating
temp: 1
# Theme to use in the forms; valid choices are charm, catppuccin, dracula, and base16
theme: charm
# TopK, only sample from the top K options for each subsequent token, -1 to disable
topk: 50
# TopP, an alternative to # Temperature (randomness) of results, from 0.0 to 2.0, -1.0 to disable
topp: 1
# Maximum number of tokens in response
# max-tokens: 100
# Wrap formatted output at specific width (default is 80)
word-wrap: 80
</document_content>
</document>

<document index="17">
<source>external/chutes_chutes.json</source>
<document_content>
{
  "total": 340,
  "page": 0,
  "limit": 25,
  "items": [
... (file content truncated to first 5 lines)
</document_content>
</document>

# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/external/dump_models.py
# Language: python

import asyncio
import json
import os
import sys
from dataclasses import dataclass
from enum import Enum
from datetime import datetime
from pathlib import Path
import fire
import httpx
import tomli_w
from loguru import logger
from rich.console import Console
import re

class ProviderKind(E, n, u, m):

class Provider:
    def build_models_url((self)) -> str:

class ModelDumper:
    """Dumps model lists from various AI API providers."""
    def __init__((self)):
        """Initialize with provider configurations."""
    def _setup_url_environment((self)):
        """Set up environment variables for provider URLs from PROVIDER_URL_CONFIG."""
    def _parse_provider_config((self)) -> list[Provider]:
        """Parse the simple text-based provider configuration."""
    def _init_failed_models((self)):
        """Initialize failed models tracking."""
    def _load_failed_models((self)):
        """Load failed models configuration from JSON file."""
    def _save_failed_models((self)):
        """Save failed models configuration to JSON file."""
    def _files_exist((self, api_name: str)) -> bool:
        """Check if output files already exist for an API."""
    def _build_curl((self, provider: Provider, models_url: str)) -> str:
    def _sort_json_data((self, data)):
        """Sort JSON data based on its format."""
    def _extract_model_ids((self, data)):
        """Extract model IDs from JSON data."""
    def _get_provider_base_url((self, provider: Provider)) -> str:
        """Get the base URL for a provider."""
    def _generate_profile_name((self, provider_name: str, model_id: str)) -> str:
        """Generate a profile name from provider and model ID."""
    def _extract_model_details((self, data, model_id: str)):
        """Extract model details from JSON data for a specific model."""
    def _generate_toml_config((self, provider: Provider, sorted_data)) -> dict:
        """Generate TOML configuration for a provider and its models."""
    def _process_provider((self, provider: Provider, force: bool = False)) -> bool:
        """Process a provider into JSON and TXT files."""
    def _mark_api_failed((self, api_name: str, error: str)):
        """Mark an API as failed in the configuration."""
    def _fetch_chutes_data((self, client: httpx.AsyncClient)) -> dict | None:
        """ Fetch chutes data from the Chutes API...."""
    def _merge_chutes_data((self, openai_data: dict, chutes_data: dict | None)) -> dict:
        """ Merge chutes API data with OpenAI API model data...."""
    def dump_models((
        self,
        force: bool = False,
        failed_rescan: bool = False,
        verbose: bool = False,
    )):
        """ Dump model lists from all configured APIs...."""

class AsyncModelDumper(M, o, d, e, l, D, u, m, p, e, r):
    """Synchronous wrapper for the async ModelDumper."""
    def dump_models((
        self,
        force: bool = False,
        failed_rescan: bool = False,
        verbose: bool = False,
    )):
        """Synchronous wrapper for the async dump_models method."""

def build_models_url((self)) -> str:

def __init__((self)):
    """Initialize with provider configurations."""

def _setup_url_environment((self)):
    """Set up environment variables for provider URLs from PROVIDER_URL_CONFIG."""

def _parse_provider_config((self)) -> list[Provider]:
    """Parse the simple text-based provider configuration."""

def _init_failed_models((self)):
    """Initialize failed models tracking."""

def _load_failed_models((self)):
    """Load failed models configuration from JSON file."""

def _save_failed_models((self)):
    """Save failed models configuration to JSON file."""

def _files_exist((self, api_name: str)) -> bool:
    """Check if output files already exist for an API."""

def _build_curl((self, provider: Provider, models_url: str)) -> str:

def _sort_json_data((self, data)):
    """Sort JSON data based on its format."""

def _extract_model_ids((self, data)):
    """Extract model IDs from JSON data."""

def _get_provider_base_url((self, provider: Provider)) -> str:
    """Get the base URL for a provider."""

def _generate_profile_name((self, provider_name: str, model_id: str)) -> str:
    """Generate a profile name from provider and model ID."""

def _extract_model_details((self, data, model_id: str)):
    """Extract model details from JSON data for a specific model."""

def _generate_toml_config((self, provider: Provider, sorted_data)) -> dict:
    """Generate TOML configuration for a provider and its models."""

def _process_provider((self, provider: Provider, force: bool = False)) -> bool:
    """Process a provider into JSON and TXT files."""

def _mark_api_failed((self, api_name: str, error: str)):
    """Mark an API as failed in the configuration."""

def _fetch_chutes_data((self, client: httpx.AsyncClient)) -> dict | None:
    """ Fetch chutes data from the Chutes API...."""

def _merge_chutes_data((self, openai_data: dict, chutes_data: dict | None)) -> dict:
    """ Merge chutes API data with OpenAI API model data...."""

def dump_models((
        self,
        force: bool = False,
        failed_rescan: bool = False,
        verbose: bool = False,
    )):
    """ Dump model lists from all configured APIs...."""

def dump_models((
        self,
        force: bool = False,
        failed_rescan: bool = False,
        verbose: bool = False,
    )):
    """Synchronous wrapper for the async dump_models method."""

def main(()):
    """Main entry point for the model dumper."""


<document index="18">
<source>external/failed_models.json</source>
<document_content>
{
  "aihorde": {
    "failed": false,
    "error": "404 Client Error: Not Found for url: https://oai.aihorde.net/models",
    "last_attempt": "2025-08-07T21:28:19.043446",
... (file content truncated to first 5 lines)
</document_content>
</document>

<document index="19">
<source>package.toml</source>
<document_content>
# Package configuration
[package]
include_cli = true        # Include CLI boilerplate
include_logging = true    # Include logging setup
use_pydantic = true      # Use Pydantic for data validation
use_rich = true          # Use Rich for terminal output

[features]
mkdocs = false           # Enable MkDocs documentation
vcs = true              # Initialize Git repository
github_actions = true   # Add GitHub Actions workflows 
</document_content>
</document>

<document index="20">
<source>pyproject.toml</source>
<document_content>
# this_file: pyproject.toml
#==============================================================================
# VEXY-CO-MODEL-CATALOG PACKAGE CONFIGURATION
# This pyproject.toml defines the package metadata, dependencies, build system,
# and development environment for the vexy-co-model-catalog package.
#==============================================================================

#------------------------------------------------------------------------------
# PROJECT METADATA
# Core package information used by PyPI and package managers.
#------------------------------------------------------------------------------
[project]
name = 'vexy-co-model-catalog' # Package name on PyPI
description = '' # Short description
readme = 'README.md' # Path to README file
requires-python = '>=3.10' # Minimum Python version
keywords = [
] # Keywords for PyPI search
dynamic = ["version"] # Fields set dynamically at build time

# PyPI classifiers for package categorization
classifiers = [
    'Development Status :: 4 - Beta', # Package maturity level
    'Programming Language :: Python',
    'Programming Language :: Python :: 3.10',
    'Programming Language :: Python :: 3.11',
    'Programming Language :: Python :: 3.12',
    'Programming Language :: Python :: Implementation :: CPython',
    'Programming Language :: Python :: Implementation :: PyPy',
    'Operating System :: OS Independent',
    'License :: OSI Approved :: MIT License',
    'Intended Audience :: Developers',
]

dependencies = [
]

# Author information
[[project.authors]]
name = 'Fontlab Ltd.'
email = 'opensource@vexy.art'

# License information
[project.license]
text = 'MIT'

# Project URLs
[project.urls]
Documentation = 'https://github.com/vexyart/vexy-co-model-catalog#readme'
Issues = 'https://github.com/vexyart/vexy-co-model-catalog/issues'
Source = 'https://github.com/vexyart/vexy-co-model-catalog'

#------------------------------------------------------------------------------
# OPTIONAL DEPENDENCIES
# Additional dependencies for optional features, development, and testing.
#------------------------------------------------------------------------------
[project.optional-dependencies]

# Development tools
dev = [
    'pre-commit>=4.1.0', # Pre-commit hook manager - Keep pre-commit as is, update if newer pre-commit version is required
    'ruff>=0.9.7', # Linting and formatting - Keep ruff as is, update if newer ruff version is required
    'mypy>=1.15.0', # Type checking - Keep mypy as is, update if newer mypy version is required
    'absolufy-imports>=0.3.1', # Convert relative imports to absolute - Keep absolufy-imports as is, update if newer absolufy-imports version is required
    'pyupgrade>=3.19.1', # Upgrade Python syntax - Keep pyupgrade as is, update if newer pyupgrade version is required
    'isort>=6.0.1', # Sort imports - Keep isort as is, update if newer isort version is required
]

# Testing tools and frameworks
test = [
    'pytest>=8.3.4', # Testing framework - Keep pytest as is, update if newer pytest version is required
    'pytest-cov>=6.0.0', # Coverage plugin for pytest - Keep pytest-cov as is, update if newer pytest-cov version is required
    'pytest-xdist>=3.6.1', # Parallel test execution - Keep pytest-xdist as is, update if newer pytest-xdist version is required
    'pytest-benchmark[histogram]>=5.1.0', # Benchmarking plugin - Keep pytest-benchmark as is, update if newer pytest-benchmark version is required
    'pytest-asyncio>=0.25.3', # Async test support - Keep pytest-asyncio as is, update if newer pytest-asyncio version is required
    'coverage[toml]>=7.6.12',
]

docs = [
    "sphinx>=7.2.6",
    "sphinx-rtd-theme>=2.0.0",
    "sphinx-autodoc-typehints>=2.0.0",
    "myst-parser>=3.0.0", # Markdown support in Sphinx
]

# All optional dependencies combined
all = [
]

#------------------------------------------------------------------------------
# COMMAND-LINE SCRIPTS
# Entry points for command-line executables installed with the package.
#------------------------------------------------------------------------------
[project.scripts]
# CLINAME = "vexy_co_model_catalog.__main__:main"

#------------------------------------------------------------------------------
# BUILD SYSTEM CONFIGURATION
# Defines the tools required to build the package and the build backend.
#------------------------------------------------------------------------------
[build-system]
# Hatchling is a modern build backend for Python packaging
# hatch-vcs integrates with version control systems for versioning
requires = [
    'hatchling>=1.27.0', # Keep hatchling as is, update if newer hatchling version is required
    'hatch-vcs>=0.4.0', # Keep hatch-vcs as is, update if newer hatch-vcs version is required
]
build-backend = 'hatchling.build' # Specifies Hatchling as the build backend


#------------------------------------------------------------------------------
# HATCH BUILD CONFIGURATION
# Configures the build process, specifying which packages to include and
# how to handle versioning.
#------------------------------------------------------------------------------
[tool.hatch.build]
# Include package data files
include = [
    "src/vexy_co_model_catalog/py.typed", # For better type checking support
    "src/vexy_co_model_catalog/data/**/*", # Include data files if any

]
exclude = ["**/__pycache__", "**/.pytest_cache", "**/.mypy_cache"]

[tool.hatch.build.targets.wheel]
packages = ["src/vexy_co_model_catalog"]
reproducible = true


# Version control system hook configuration
# Automatically updates the version file from git tags
[tool.hatch.build.hooks.vcs]
version-file = "src/vexy_co_model_catalog/__version__.py"

# Version source configuration
[tool.hatch.version]
source = 'vcs' # Get version from git tags or other VCS info

# Metadata handling configuration
[tool.hatch.metadata]
allow-direct-references = true # Allow direct references in metadata (useful for local dependencies)


#------------------------------------------------------------------------------
# DEVELOPMENT ENVIRONMENTS

[tool.hatch.envs.default]
features = ['dev', 'test', 'all']
dependencies = [
]

# Commands available in the default environment
[tool.hatch.envs.default.scripts]
# Run tests with optional arguments
test = 'pytest {args:tests}'
# Run tests with coverage reporting
test-cov = "pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/vexy_co_model_catalog --cov=tests {args:tests}"
# Run type checking
type-check = "mypy src/vexy_co_model_catalog tests"
# Run linting and formatting
lint = ["ruff check src/vexy_co_model_catalog tests", "ruff format --respect-gitignore src/vexy_co_model_catalog tests"]
# Format and fix style issues
fmt = ["ruff format --respect-gitignore src/vexy_co_model_catalog tests", "ruff check --fix src/vexy_co_model_catalog tests"]
fix = ["ruff check --fix --unsafe-fixes src/vexy_co_model_catalog tests", "ruff format --respect-gitignore src/vexy_co_model_catalog tests"]

# Matrix configuration to test across multiple Python versions

[[tool.hatch.envs.all.matrix]]
python = ["3.10", "3.11", "3.12"]

#------------------------------------------------------------------------------
# SPECIALIZED ENVIRONMENTS
# Additional environments for specific development tasks.
#------------------------------------------------------------------------------

# Dedicated environment for linting and code quality checks
[tool.hatch.envs.lint]
detached = true # Create a separate, isolated environment
features = ['dev'] # Use dev extras  dependencies 

# Linting environment commands
[tool.hatch.envs.lint.scripts]
# Type checking with automatic type installation
typing = "mypy --install-types --non-interactive {args:src/vexy_co_model_catalog tests}"
# Check style and format code
style = ["ruff check {args:.}", "ruff format --respect-gitignore {args:.}"]
# Format and fix style issues
fmt = ["ruff format --respect-gitignore {args:.}", "ruff check --fix {args:.}"]
fix = ["ruff check --fix --unsafe-fixes {args:.}", "ruff format --respect-gitignore {args:.}"]
# Run all ops
all = ["style", "typing", "fix"]

# Dedicated environment for testing
[tool.hatch.envs.test]
features = ['test'] # Use test extras as dependencies

# Testing environment commands
[tool.hatch.envs.test.scripts]
# Run tests in parallel
test = "python -m pytest -n auto {args:tests}"
# Run tests with coverage in parallel
test-cov = "python -m pytest -n auto --cov-report=term-missing --cov-config=pyproject.toml --cov=src/vexy_co_model_catalog --cov=tests {args:tests}"
# Run benchmarks
bench = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only"
# Run benchmarks and save results
bench-save = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json"

# Documentation environment
[tool.hatch.envs.docs]
features = ['docs']

# Documentation environment commands
[tool.hatch.envs.docs.scripts]
build = "sphinx-build -b html docs/source docs/build"

# GitHub Actions workflow configuration
[tool.hatch.envs.ci]
features = ['test']


[tool.hatch.envs.ci.scripts]
test = "pytest --cov=src/vexy_co_model_catalog --cov-report=xml"


#------------------------------------------------------------------------------
# CODE QUALITY TOOLS
# Configuration for linting, formatting, and code quality enforcement.
#------------------------------------------------------------------------------

#------------------------------------------------------------------------------
# COVERAGE CONFIGURATION
# Settings for test coverage measurement and reporting.
#------------------------------------------------------------------------------

# Path mapping for coverage in different environments
[tool.coverage.paths]
vexy_co_model_catalog = ["src/vexy_co_model_catalog", "*/vexy-co-model-catalog/src/vexy_co_model_catalog"]
tests = ["tests", "*/vexy-co-model-catalog/tests"]

# Coverage report configuration
[tool.coverage.report]
# Lines to exclude from coverage reporting
exclude_lines = [
    'no cov', # Custom marker to skip coverage
    'if __name__ == .__main__.:', # Script execution guard
    'if TYPE_CHECKING:', # Type checking imports and code
    'pass', # Empty pass statements
    'raise NotImplementedError', # Unimplemented method placeholders
    'raise ImportError', # Import error handling
    'except ImportError', # Import error handling
    'except KeyError', # Common error handling
    'except AttributeError', # Common error handling
    'except NotImplementedError', # Common error handling
]

[tool.coverage.run]
source_pkgs = ["vexy_co_model_catalog", "tests"]
branch = true # Measure branch coverage (if/else statements)
parallel = true # Support parallel test execution
omit = [
    "src/vexy_co_model_catalog/__about__.py",
]

#------------------------------------------------------------------------------
# MYPY CONFIGURATION
# Configuration for type checking with mypy.
#------------------------------------------------------------------------------

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[[tool.mypy.overrides]]
module = ["tests.*"]
disallow_untyped_defs = false
disallow_incomplete_defs = false

#------------------------------------------------------------------------------
# PYTEST CONFIGURATION
# Configuration for pytest, including markers, options, and benchmark settings.
#------------------------------------------------------------------------------

[tool.pytest.ini_options]
addopts = "-v --durations=10 -p no:briefcase"
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
console_output_style = "progress"
filterwarnings = ["ignore::DeprecationWarning", "ignore::UserWarning"]
log_cli = true
log_cli_level = "INFO"
markers = [
    "benchmark: marks tests as benchmarks (select with '-m benchmark')",
    "unit: mark a test as a unit test",
    "integration: mark a test as an integration test",
    "permutation: tests for permutation functionality", 
    "parameter: tests for parameter parsing",
    "prompt: tests for prompt parsing",
]
norecursedirs = [
    ".*",
    "build",
    "dist", 
    "venv",
    "__pycache__",
    "*.egg-info",
    "_private",
]
python_classes = ["Test*"]
python_files = ["test_*.py"]
python_functions = ["test_*"]
testpaths = ["tests"]

[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = "file"
save-data = true
compare = [
    "min",    # Minimum time
    "max",    # Maximum time
    "mean",   # Mean time
    "stddev", # Standard deviation
    "median", # Median time
    "iqr",    # Inter-quartile range
    "ops",    # Operations per second
    "rounds", # Number of rounds
]

#------------------------------------------------------------------------------
# RUFF CONFIGURATION
# Configuration for Ruff, including linter and formatter settings.
#------------------------------------------------------------------------------ 

# Ruff linter and formatter configuration
[tool.ruff]
target-version = "py310"
line-length = 120

# Linting rules configuration
[tool.ruff.lint]
# Rule sets to enable, organized by category
select = [
    # flake8 plugins and extensions
    'A', # flake8-builtins: checks for shadowed builtins
    'ARG', # flake8-unused-arguments: checks for unused function arguments
    'ASYNC', # flake8-async: checks for async/await issues
    'B', # flake8-bugbear: finds likely bugs and design problems
    'C', # flake8-comprehensions: helps write better list/dict comprehensions
    'DTZ', # flake8-datetimez: checks for datetime timezone issues
    'E', # pycodestyle errors: PEP 8 style guide errors
    'EM', # flake8-errmsg: checks for better error messages
    'F', # pyflakes: detects various errors
    'FBT', # flake8-boolean-trap: checks for boolean traps in function signatures
    'I', # isort: sorts imports
    'ICN', # flake8-import-conventions: checks for import conventions
    'ISC', # flake8-implicit-str-concat: checks for implicit string concatenation
    'LOG', # flake8-logging: checks for logging issues
    'N', # pep8-naming: checks naming conventions
    'PLC', # pylint convention: checks for convention issues
    'PLE', # pylint error: checks for errors
    'PLR', # pylint refactor: suggests refactors
    'PLW', # pylint warning: checks for suspicious code
    'PT', # flake8-pytest-style: checks pytest-specific style
    'PTH', # flake8-use-pathlib: checks for stdlib path usage vs pathlib
    'PYI', # flake8-pyi: checks stub files
    'RET', # flake8-return: checks return statement consistency
    'RSE', # flake8-raise: checks raise statements
    'RUF', # Ruff-specific rules
    'S', # flake8-bandit: checks for security issues
    'SIM', # flake8-simplify: checks for code simplification opportunities
    'T', # flake8-print: checks for print statements
    'TCH', # flake8-type-checking: helps with type-checking
    'TID', # flake8-tidy-imports: checks for tidy import statements
    'UP', # pyupgrade: checks for opportunities to use newer Python features
    'W', # pycodestyle warnings: PEP 8 style guide warnings
    'YTT', # flake8-2020: checks for misuse of sys.version or sys.version_info

]
# Rules to ignore (with reasons)
ignore = [
    'B027', # Empty method in abstract base class - sometimes needed for interfaces
    'C901', # Function is too complex - sometimes complexity is necessary
    'FBT003', # Boolean positional argument in function definition - sometimes unavoidable
    'PLR0911', # Too many return statements - sometimes needed for readability
    'PLR0912', # Too many branches - sometimes needed for complex logic
    'PLR0913', # Too many arguments - sometimes needed in APIs
    'PLR0915', # Too many statements - sometimes needed for comprehensive functions
    'PLR1714', # Consider merging multiple comparisons - sometimes less readable
    'PLW0603', # Using the global statement - sometimes necessary
    'PT013', # Pytest explicit test parameter - sometimes clearer
    'PTH123', # Path traversal - sometimes needed
    'PYI056', # Calling open() in pyi file - sometimes needed in type stubs
    'S105', # Possible hardcoded password - often false positives
    'S106', # Possible hardcoded password - often false positives
    'S107', # Possible hardcoded password - often false positives
    'S110', # try-except-pass - sometimes valid for suppressing exceptions
    'SIM102'
    # Nested if statements - sometimes more readable than combined conditions
]
# Rules that should not be automatically fixed
unfixable = [
    'F401', # Don't automatically remove unused imports - may be needed later

]
# Configure extend-exclude to ignore specific directories
extend-exclude = [".git", ".venv", "venv", "dist", "build"]

# isort configuration within Ruff
[tool.ruff.lint.isort]
known-first-party = ['vexy_co_model_catalog'] # Treat as first-party imports for sorting

# flake8-tidy-imports configuration within Ruff
[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = 'all' # Ban all relative imports for consistency

# Per-file rule exceptions
[tool.ruff.lint.per-file-ignores]
# Tests can use magic values, assertions, and relative imports
'tests/**/*' = [
    'PLR2004', # Allow magic values in tests for readability
    'S101', # Allow assertions in tests
    'TID252'
    # Allow relative imports in tests for convenience
]

</document_content>
</document>

# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/src/vexy_co_model_catalog/__init__.py
# Language: python

from ._version import __version__


# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/src/vexy_co_model_catalog/__main__.py
# Language: python

from .cli import main


# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/src/vexy_co_model_catalog/_version.py
# Language: python

from typing import Tuple
from typing import Union


# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/src/vexy_co_model_catalog/cli.py
# Language: python

import sys
from typing import Iterable
import fire
from rich.console import Console
from rich.table import Table
from . import __version__

class CLI:
    """Model Catalog Manager CLI (minimal)."""
    def version((self)) -> str:
        """Print and return the package version."""
    def list_providers((self)) -> list[str]:
        """List registered providers (placeholder)."""

def version((self)) -> str:
    """Print and return the package version."""

def list_providers((self)) -> list[str]:
    """List registered providers (placeholder)."""

def main(()) -> None:
    """Main CLI entry point."""


# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/src/vexy_co_model_catalog/core/__init__.py
# Language: python

from .storage import StorageManager


# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/src/vexy_co_model_catalog/core/catalog.py
# Language: python

from dataclasses import dataclass
from typing import Any
from loguru import logger
from .storage import StorageManager

class ProviderRegistry:
    def __init__((self)) -> None:
    def register((self, name: str, provider: Any)) -> None:
    def get((self, name: str)) -> Any:
    def list_names((self)) -> list[str]:
    def remove((self, name: str)) -> bool:

class ModelCatalog:
    def __post_init__((self)) -> None:
    def get_failed_providers((self)) -> dict[str, str]:

def __init__((self)) -> None:

def register((self, name: str, provider: Any)) -> None:

def get((self, name: str)) -> Any:

def list_names((self)) -> list[str]:

def remove((self, name: str)) -> bool:

def __post_init__((self)) -> None:

def get_failed_providers((self)) -> dict[str, str]:


# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/src/vexy_co_model_catalog/core/fetcher.py
# Language: python

import asyncio
from typing import Any
import httpx
from ..utils.exceptions import AuthenticationError, FetchError, RateLimitError

class ModelFetcher:
    def __init__((self, max_concurrency: int = 8, timeout: float = 15.0)) -> None:
    def close((self)) -> None:
    def __aenter__((self)) -> "ModelFetcher":
    def __aexit__((self, exc_type, exc, tb)) -> None:
    def get_json((
        self,
        url: str,
        *,
        headers: dict[str, str] | None = None,
        max_attempts: int = 3,
    )) -> dict[str, Any] | list[Any]:
    def stats((self)) -> dict[str, float | int | None]:

def __init__((self, max_concurrency: int = 8, timeout: float = 15.0)) -> None:

def close((self)) -> None:

def __aenter__((self)) -> "ModelFetcher":

def __aexit__((self, exc_type, exc, tb)) -> None:

def get_json((
        self,
        url: str,
        *,
        headers: dict[str, str] | None = None,
        max_attempts: int = 3,
    )) -> dict[str, Any] | list[Any]:

def stats((self)) -> dict[str, float | int | None]:


# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/src/vexy_co_model_catalog/core/provider.py
# Language: python

import os
from dataclasses import dataclass
from enum import Enum
from typing import Any

class ProviderKind(s, t, r, ,,  , E, n, u, m):

class Model:

class ProviderConfig:
    def get_base_url((self)) -> str | None:

def get_base_url((self)) -> str | None:


# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/src/vexy_co_model_catalog/core/storage.py
# Language: python

import json
import os
import tempfile
from pathlib import Path
from typing import Any
import tomli_w
from loguru import logger

class StorageError(E, x, c, e, p, t, i, o, n):

class StorageManager:
    """Manages file storage with atomic writes and a simple layout."""
    def __init__((self, root_path: str | os.PathLike | None = None)) -> None:
    def _ensure_directories((self)) -> None:
    def _atomic_write((self, target_path: Path, data: bytes)) -> None:
    def write_json((
        self,
        filename: str,
        data: Any,
        *,
        indent: int | None = 2,
        sort_keys: bool = False,
        ensure_ascii: bool = False,
    )) -> None:
    def write_text((self, filename: str, lines: list[str])) -> None:
    def write_toml((self, filename: str, data: dict[str, Any])) -> None:
    def write_extra((self, filename: str, data: Any)) -> None:
    def read_json((self, filename: str, *, directory: str)) -> Any | None:
    def list_files((self, directory: str, pattern: str = "*")) -> list[Path]:
    def cleanup_temp_files((self)) -> None:
    def get_file_stats((self)) -> dict[str, int]:
    def __repr__((self)) -> str:

def __init__((self, root_path: str | os.PathLike | None = None)) -> None:

def _ensure_directories((self)) -> None:

def _atomic_write((self, target_path: Path, data: bytes)) -> None:

def write_json((
        self,
        filename: str,
        data: Any,
        *,
        indent: int | None = 2,
        sort_keys: bool = False,
        ensure_ascii: bool = False,
    )) -> None:

def write_text((self, filename: str, lines: list[str])) -> None:

def write_toml((self, filename: str, data: dict[str, Any])) -> None:

def write_extra((self, filename: str, data: Any)) -> None:

def read_json((self, filename: str, *, directory: str)) -> Any | None:

def list_files((self, directory: str, pattern: str = "*")) -> list[Path]:

def cleanup_temp_files((self)) -> None:

def get_file_stats((self)) -> dict[str, int]:

def __repr__((self)) -> str:


# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/src/vexy_co_model_catalog/utils/__init__.py
# Language: python

from .exceptions import (
    ModelCatalogError,
    ProviderError,
    FetchError,
    AuthenticationError,
    RateLimitError,
    ConfigurationError,
    GeneratorError,
    ValidationError,
)


# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/src/vexy_co_model_catalog/utils/exceptions.py
# Language: python

class ModelCatalogError(E, x, c, e, p, t, i, o, n):
    """Base error for the package."""

class ProviderError(M, o, d, e, l, C, a, t, a, l, o, g, E, r, r, o, r):
    """Provider-specific error."""

class FetchError(P, r, o, v, i, d, e, r, E, r, r, o, r):
    """Generic fetch error (HTTP or parsing)."""

class AuthenticationError(P, r, o, v, i, d, e, r, E, r, r, o, r):
    """Authentication or authorization failure."""

class RateLimitError(P, r, o, v, i, d, e, r, E, r, r, o, r):
    """Rate limiting encountered."""

class ConfigurationError(M, o, d, e, l, C, a, t, a, l, o, g, E, r, r, o, r):
    """Invalid or missing configuration."""

class GeneratorError(M, o, d, e, l, C, a, t, a, l, o, g, E, r, r, o, r):
    """Generation/conversion failure."""

class ValidationError(M, o, d, e, l, C, a, t, a, l, o, g, E, r, r, o, r):
    """Input/output validation error."""

class StorageError(M, o, d, e, l, C, a, t, a, l, o, g, E, r, r, o, r):
    """Storage-layer failure."""


# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/src/vexy_co_model_catalog/vexy_co_model_catalog.py
# Language: python

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
import logging

class Config:
    """Configuration settings for vexy_co_model_catalog."""

def process_data((
    data: List[Any],
    config: Optional[Config] = None,
    *,
    debug: bool = False
)) -> Dict[str, Any]:
    """Process the input data according to configuration."""

def main(()) -> None:
    """Main entry point for vexy_co_model_catalog."""


# File: /Users/adam/Developer/vcs/github.vexyart/vexy-co-model-catalog/tests/test_package.py
# Language: python

import vexy_co_model_catalog

def test_version(()):
    """Verify package exposes version."""


</documents>